Nuclear engineering is the engineering discipline concerned with the design and application of systems that make use of the energy released by nuclear processes.[1][2] The most prominent application of nuclear engineering is the generation of electricity. Worldwide, some 440 nuclear reactors in 32 countries generate 10 percent of the world's energy through nuclear fission.[3] In the future, it is expected that nuclear fusion will add another nuclear means of generating energy.[4] Both reactions make use of the nuclear binding energy released when atomic nucleons are either separated (fission) or brought together (fusion). The energy available is given by the binding energy curve, and the amount generated is much greater than that generated through chemical reactions. Fission of 1 gram of uranium yields as much energy as burning 3 tons of coal or 600 gallons of fuel oil,[5] without adding carbon dioxide to the atmosphere.[6]

What nuclear engineers do
Nuclear engineers work in such areas as the following:[7][8][9]

Nuclear reactor design, which has evolved from the Generation I, proof-of concept, reactors of the 1950s and 1960s,[10] to Generation II, Generation III, and Generation IV concepts
Thermal hydraulics and heat transfer. In a typical nuclear power plant, heat generates steam that drives a steam turbine and a generator that produces electricity
Materials science as it relates to nuclear power applications
Managing the nuclear fuel cycle, in which fissile material is obtained, formed into fuel, removed when depleted, and safely stored or reprocessed
Nuclear propulsion, mainly for military naval vessels, but there have been concepts for aircraft and missiles. Nuclear power has been used in space since the 1960s
Plasma physics, which is integral to the development of fusion power
Weapons development and management
Generation of radionuclides, which have applications in industry, medicine, and many other areas
Nuclear waste management
Health physics
Nuclear medicine and Medical Physics
Health and safety
Instrumentation and control engineering
Process engineering
Project Management
Quality engineering
Reactor operations[11]
Nuclear security (detection of clandestine nuclear materials)[12]
Nuclear engineering even has a role in criminal investigation,[13] and agriculture.[14]
Many chemical, electrical and mechanical and other types of engineers also work in the nuclear industry, as do many scientists and support staff. In the U.S., nearly 100,000 people directly work in the nuclear industry. Including secondary sector jobs, the number of people supported by the U.S. nuclear industry is 475,000.[15]

History of nuclear engineering
An argument can be made that nuclear engineering was born in 1938, with the discovery of nuclear fission.[16] However, from the engineering perspective of applying science to create something new, a more fitting beginning might be 1942, when Chicago Pile-1 (CP-1) began operating at the University of Chicago as a part of the Manhattan Project. The first artificial nuclear reactor, CP-1 was designed by a team of physicists who were concerned that Nazi Germany might also be seeking to build a bomb based on nuclear fission. (The earliest known nuclear reaction on Earth occurred naturally, 1.7 billion years ago, in Oklo, Gabon, Africa.) The second artificial nuclear reactor, the X-10 Graphite Reactor was also a part of the Manhattan Project, as were the plutonium-producing reactors of the Hanford Engineer Works.

The first nuclear reactor to generate electricity was Experimental Breeder Reactor I (EBR-I), which did so near Arco, Idaho, in 1951.[17] EBR-I was a standalone facility, not connected to a grid, but a later Idaho research reactor in the BORAX series did briefly supply power to the town of Arco in 1955.

The first commercial nuclear power plant, built to be connected to an electrical grid, is the Obninsk Nuclear Power Plant, which began operation in 1954. The second appears to be the Shippingport Atomic Power Station, which produced electricity in 1957.

For a brief chronology, from the discovery of uranium to the current era, see Outline History of Nuclear Energy or History of Nuclear Power.

See List of Commercial Nuclear Reactors for a comprehensive listing of nuclear power reactors and IAEA Power Reactor Information System (PRIS) for worldwide and country-level statistics on nuclear power generation.

Where nuclear engineers work
In the United States, nuclear engineers are employed as follows:[18]

Electric power generation 25%
Federal government 18%
Scientific research and development 15%
Engineering services 5%
Manufacturing 10%
Other areas 27%
Worldwide, job prospects for nuclear engineers are likely best in those countries that are active in or exploring nuclear technologies
This article is about the study of atomic nuclei. For other uses, see Nuclear physics (disambiguation).
Nuclear physics

NucleusNucleons pnNuclear matterNuclear forceNuclear structureNuclear reaction
Models of the nucleus
Nuclides' classification
Nuclear stability
Radioactive decay
Nuclear fission
Capturing processes
High-energy processes
Nucleosynthesis and
nuclear astrophysics
High-energy nuclear physics
Scientists
icon Physics portal Category
vte
Nuclear physics is the field of physics that studies atomic nuclei and their constituents and interactions, in addition to the study of other forms of nuclear matter.

Nuclear physics should not be confused with atomic physics, which studies the atom as a whole, including its electrons.

Discoveries in nuclear physics have led to applications in many fields. This includes nuclear power, nuclear weapons, nuclear medicine and magnetic resonance imaging, industrial and agricultural isotopes, ion implantation in materials engineering, and radiocarbon dating in geology and archaeology. Such applications are studied in the field of nuclear engineering.

Particle physics evolved out of nuclear physics and the two fields are typically taught in close association. Nuclear astrophysics, the application of nuclear physics to astrophysics, is crucial in explaining the inner workings of stars and the origin of the chemical elements.

History

Henri Becquerel

Since the 1920s, cloud chambers played an important role of particle detectors and eventually lead to the discovery of positron, muon and kaon.
The history of nuclear physics as a discipline distinct from atomic physics, starts with the discovery of radioactivity by Henri Becquerel in 1896,[1] made while investigating phosphorescence in uranium salts.[2] The discovery of the electron by J. J. Thomson[3] a year later was an indication that the atom had internal structure. At the beginning of the 20th century the accepted model of the atom was J. J. Thomson's "plum pudding" model in which the atom was a positively charged ball with smaller negatively charged electrons embedded inside it.

In the years that followed, radioactivity was extensively investigated, notably by Marie Curie, a Polish physicist whose maiden name was Sklodowska, Pierre Curie, Ernest Rutherford and others. By the turn of the century, physicists had also discovered three types of radiation emanating from atoms, which they named alpha, beta, and gamma radiation. Experiments by Otto Hahn in 1911 and by James Chadwick in 1914 discovered that the beta decay spectrum was continuous rather than discrete. That is, electrons were ejected from the atom with a continuous range of energies, rather than the discrete amounts of energy that were observed in gamma and alpha decays. This was a problem for nuclear physics at the time, because it seemed to indicate that energy was not conserved in these decays.

The 1903 Nobel Prize in Physics was awarded jointly to Becquerel, for his discovery and to Marie and Pierre Curie for their subsequent research into radioactivity. Rutherford was awarded the Nobel Prize in Chemistry in 1908 for his "investigations into the disintegration of the elements and the chemistry of radioactive substances".

In 1905, Albert Einstein formulated the idea of mass–energy equivalence. While the work on radioactivity by Becquerel and Marie Curie predates this, an explanation of the source of the energy of radioactivity would have to wait for the discovery that the nucleus itself was composed of smaller constituents, the nucleons.

Rutherford discovers the nucleus
In 1906, Ernest Rutherford published "Retardation of the α Particle from Radium in passing through matter."[4] Hans Geiger expanded on this work in a communication to the Royal Society[5] with experiments he and Rutherford had done, passing alpha particles through air, aluminum foil and gold leaf. More work was published in 1909 by Geiger and Ernest Marsden,[6] and further greatly expanded work was published in 1910 by Geiger.[7] In 1911–1912 Rutherford went before the Royal Society to explain the experiments and propound the new theory of the atomic nucleus as we now understand it.

Published in 1909,[8] with the eventual classical analysis by Rutherford published May 1911,[9][10][11][12] the key preemptive experiment was performed during 1909,[9][13][14][15] at the University of Manchester. Ernest Rutherford's assistant, Professor [15] Johannes [14] "Hans" Geiger, and an undergraduate, Marsden,[15] performed an experiment in which Geiger and Marsden under Rutherford's supervision fired alpha particles (helium 4 nuclei[16]) at a thin film of gold foil. The plum pudding model had predicted that the alpha particles should come out of the foil with their trajectories being at most slightly bent. But Rutherford instructed his team to look for something that shocked him to observe: a few particles were scattered through large angles, even completely backwards in some cases. He likened it to firing a bullet at tissue paper and having it bounce off. The discovery, with Rutherford's analysis of the data in 1911, led to the Rutherford model of the atom, in which the atom had a very small, very dense nucleus containing most of its mass, and consisting of heavy positively charged particles with embedded electrons in order to balance out the charge (since the neutron was unknown). As an example, in this model (which is not the modern one) nitrogen-14 consisted of a nucleus with 14 protons and 7 electrons (21 total particles) and the nucleus was surrounded by 7 more orbiting electrons.

Eddington and stellar nuclear fusion
Around 1920, Arthur Eddington anticipated the discovery and mechanism of nuclear fusion processes in stars, in his paper The Internal Constitution of the Stars.[17][18] At that time, the source of stellar energy was a complete mystery; Eddington correctly speculated that the source was fusion of hydrogen into helium, liberating enormous energy according to Einstein's equation E = mc2. This was a particularly remarkable development since at that time fusion and thermonuclear energy, and even that stars are largely composed of hydrogen (see metallicity), had not yet been discovered.

Studies of nuclear spin
The Rutherford model worked quite well until studies of nuclear spin were carried out by Franco Rasetti at the California Institute of Technology in 1929. By 1925 it was known that protons[citation needed] and electrons each had a spin of ±+1⁄2. In the Rutherford model of nitrogen-14, 20 of the total 21 nuclear particles should have paired up to cancel each other's spin, and the final odd particle should have left the nucleus with a net spin of 1⁄2. Rasetti discovered, however, that nitrogen-14 had a spin of 1.

James Chadwick discovers the neutron
Main article: Discovery of the neutron
In 1932 Chadwick realized that radiation that had been observed by Walther Bothe, Herbert Becker, Irène and Frédéric Joliot-Curie was actually due to a neutral particle of about the same mass as the proton, that he called the neutron (following a suggestion from Rutherford about the need for such a particle).[19] In the same year Dmitri Ivanenko suggested that there were no electrons in the nucleus — only protons and neutrons — and that neutrons were spin 1⁄2 particles, which explained the mass not due to protons. The neutron spin immediately solved the problem of the spin of nitrogen-14, as the one unpaired proton and one unpaired neutron in this model each contributed a spin of 1⁄2 in the same direction, giving a final total spin of 1.

With the discovery of the neutron, scientists could at last calculate what fraction of binding energy each nucleus had, by comparing the nuclear mass with that of the protons and neutrons which composed it. Differences between nuclear masses were calculated in this way. When nuclear reactions were measured, these were found to agree with Einstein's calculation of the equivalence of mass and energy to within 1% as of 1934.

Proca's equations of the massive vector boson field
Alexandru Proca was the first to develop and report the massive vector boson field equations and a theory of the mesonic field of nuclear forces. Proca's equations were known to Wolfgang Pauli[20] who mentioned the equations in his Nobel address, and they were also known to Yukawa, Wentzel, Taketani, Sakata, Kemmer, Heitler, and Fröhlich who appreciated the content of Proca's equations for developing a theory of the atomic nuclei in Nuclear Physics.[21][22][23][24][25]

Yukawa's meson postulated to bind nuclei
In 1935 Hideki Yukawa[26] proposed the first significant theory of the strong force to explain how the nucleus holds together. In the Yukawa interaction a virtual particle, later called a meson, mediated a force between all nucleons, including protons and neutrons. This force explained why nuclei did not disintegrate under the influence of proton repulsion, and it also gave an explanation of why the attractive strong force had a more limited range than the electromagnetic repulsion between protons. Later, the discovery of the pi meson showed it to have the properties of Yukawa's particle.

With Yukawa's papers, the modern model of the atom was complete. The center of the atom contains a tight ball of neutrons and protons, which is held together by the strong nuclear force, unless it is too large. Unstable nuclei may undergo alpha decay, in which they emit an energetic helium nucleus, or beta decay, in which they eject an electron (or positron). After one of these decays the resultant nucleus may be left in an excited state, and in this case it decays to its ground state by emitting high-energy photons (gamma decay).

The study of the strong and weak nuclear forces (the latter explained by Enrico Fermi via Fermi's interaction in 1934) led physicists to collide nuclei and electrons at ever higher energies. This research became the science of particle physics, the crown jewel of which is the standard model of particle physics, which describes the strong, weak, and electromagnetic forces.

Modern nuclear physics
Main articles: Liquid-drop model, Nuclear shell model, and Nuclear structure
A heavy nucleus can contain hundreds of nucleons. This means that with some approximation it can be treated as a classical system, rather than a quantum-mechanical one. In the resulting liquid-drop model,[27] the nucleus has an energy that arises partly from surface tension and partly from electrical repulsion of the protons. The liquid-drop model is able to reproduce many features of nuclei, including the general trend of binding energy with respect to mass number, as well as the phenomenon of nuclear fission.

Superimposed on this classical picture, however, are quantum-mechanical effects, which can be described using the nuclear shell model, developed in large part by Maria Goeppert Mayer[28] and J. Hans D. Jensen.[29] Nuclei with certain "magic" numbers of neutrons and protons are particularly stable, because their shells are filled.

Other more complicated models for the nucleus have also been proposed, such as the interacting boson model, in which pairs of neutrons and protons interact as bosons.

Ab initio methods try to solve the nuclear many-body problem from the ground up, starting from the nucleons and their interactions.[30]

Much of current research in nuclear physics relates to the study of nuclei under extreme conditions such as high spin and excitation energy. Nuclei may also have extreme shapes (similar to that of Rugby balls or even pears) or extreme neutron-to-proton ratios. Experimenters can create such nuclei using artificially induced fusion or nucleon transfer reactions, employing ion beams from an accelerator. Beams with even higher energies can be used to create nuclei at very high temperatures, and there are signs that these experiments have produced a phase transition from normal nuclear matter to a new state, the quark–gluon plasma, in which the quarks mingle with one another, rather than being segregated in triplets as they are in neutrons and protons.

Nuclear decay
Main articles: Radioactivity and Valley of stability
Eighty elements have at least one stable isotope which is never observed to decay, amounting to a total of about 251 stable nuclides. However, thousands of isotopes have been characterized as unstable. These "radioisotopes" decay over time scales ranging from fractions of a second to trillions of years. Plotted on a chart as a function of atomic and neutron numbers, the binding energy of the nuclides forms what is known as the valley of stability. Stable nuclides lie along the bottom of this energy valley, while increasingly unstable nuclides lie up the valley walls, that is, have weaker binding energy.

The most stable nuclei fall within certain ranges or balances of composition of neutrons and protons: too few or too many neutrons (in relation to the number of protons) will cause it to decay. For example, in beta decay, a nitrogen-16 atom (7 protons, 9 neutrons) is converted to an oxygen-16 atom (8 protons, 8 neutrons)[31] within a few seconds of being created. In this decay a neutron in the nitrogen nucleus is converted by the weak interaction into a proton, an electron and an antineutrino. The element is transmuted to another element, with a different number of protons.

In alpha decay, which typically occurs in the heaviest nuclei, the radioactive element decays by emitting a helium nucleus (2 protons and 2 neutrons), giving another element, plus helium-4. In many cases this process continues through several steps of this kind, including other types of decays (usually beta decay) until a stable element is formed.

In gamma decay, a nucleus decays from an excited state into a lower energy state, by emitting a gamma ray. The element is not changed to another element in the process (no nuclear transmutation is involved).

Other more exotic decays are possible (see the first main article). For example, in internal conversion decay, the energy from an excited nucleus may eject one of the inner orbital electrons from the atom, in a process which produces high speed electrons but is not beta decay and (unlike beta decay) does not transmute one element to another.

Nuclear fusion
In nuclear fusion, two low-mass nuclei come into very close contact with each other so that the strong force fuses them. It requires a large amount of energy for the strong or nuclear forces to overcome the electrical repulsion between the nuclei in order to fuse them; therefore nuclear fusion can only take place at very high temperatures or high pressures. When nuclei fuse, a very large amount of energy is released and the combined nucleus assumes a lower energy level. The binding energy per nucleon increases with mass number up to nickel-62. Stars like the Sun are powered by the fusion of four protons into a helium nucleus, two positrons, and two neutrinos. The uncontrolled fusion of hydrogen into helium is known as thermonuclear runaway. A frontier in current research at various institutions, for example the Joint European Torus (JET) and ITER, is the development of an economically viable method of using energy from a controlled fusion reaction. Nuclear fusion is the origin of the energy (including in the form of light and other electromagnetic radiation) produced by the core of all stars including our own Sun.

Nuclear fission
Nuclear fission is the reverse process to fusion. For nuclei heavier than nickel-62 the binding energy per nucleon decreases with the mass number. It is therefore possible for energy to be released if a heavy nucleus breaks apart into two lighter ones.

The process of alpha decay is in essence a special type of spontaneous nuclear fission. It is a highly asymmetrical fission because the four particles which make up the alpha particle are especially tightly bound to each other, making production of this nucleus in fission particularly likely.

From several of the heaviest nuclei whose fission produces free neutrons, and which also easily absorb neutrons to initiate fission, a self-igniting type of neutron-initiated fission can be obtained, in a chain reaction. Chain reactions were known in chemistry before physics, and in fact many familiar processes like fires and chemical explosions are chemical chain reactions. The fission or "nuclear" chain-reaction, using fission-produced neutrons, is the source of energy for nuclear power plants and fission-type nuclear bombs, such as those detonated in Hiroshima and Nagasaki, Japan, at the end of World War II. Heavy nuclei such as uranium and thorium may also undergo spontaneous fission, but they are much more likely to undergo decay by alpha decay.

For a neutron-initiated chain reaction to occur, there must be a critical mass of the relevant isotope present in a certain space under certain conditions. The conditions for the smallest critical mass require the conservation of the emitted neutrons and also their slowing or moderation so that there is a greater cross-section or probability of them initiating another fission. In two regions of Oklo, Gabon, Africa, natural nuclear fission reactors were active over 1.5 billion years ago.[32] Measurements of natural neutrino emission have demonstrated that around half of the heat emanating from the Earth's core results from radioactive decay. However, it is not known if any of this results from fission chain reactions.[33]

Production of "heavy" elements
Main article: nucleosynthesis
According to the theory, as the Universe cooled after the Big Bang it eventually became possible for common subatomic particles as we know them (neutrons, protons and electrons) to exist. The most common particles created in the Big Bang which are still easily observable to us today were protons and electrons (in equal numbers). The protons would eventually form hydrogen atoms. Almost all the neutrons created in the Big Bang were absorbed into helium-4 in the first three minutes after the Big Bang, and this helium accounts for most of the helium in the universe today (see Big Bang nucleosynthesis).

Some relatively small quantities of elements beyond helium (lithium, beryllium, and perhaps some boron) were created in the Big Bang, as the protons and neutrons collided with each other, but all of the "heavier elements" (carbon, element number 6, and elements of greater atomic number) that we see today, were created inside stars during a series of fusion stages, such as the proton–proton chain, the CNO cycle and the triple-alpha process. Progressively heavier elements are created during the evolution of a star.

Energy is only released in fusion processes involving smaller atoms than iron because the binding energy per nucleon peaks around iron (56 nucleons). Since the creation of heavier nuclei by fusion requires energy, nature resorts to the process of neutron capture. Neutrons (due to their lack of charge) are readily absorbed by a nucleus. The heavy elements are created by either a slow neutron capture process (the so-called s-process) or the rapid, or r-process. The s process occurs in thermally pulsing stars (called AGB, or asymptotic giant branch stars) and takes hundreds to thousands of years to reach the heaviest elements of lead and bismuth. The r-process is thought to occur in supernova explosions, which provide the necessary conditions of high temperature, high neutron flux and ejected matter. These stellar conditions make the successive neutron captures very fast, involving very neutron-rich species which then beta-decay to heavier elements, especially at the so-called waiting points that correspond to more stable nuclides with closed neutron shells (magic numbers).
Nuclear power is the use of nuclear reactions to produce electricity. Nuclear power can be obtained from nuclear fission, nuclear decay and nuclear fusion reactions. Presently, the vast majority of electricity from nuclear power is produced by nuclear fission of uranium and plutonium in nuclear power plants. Nuclear decay processes are used in niche applications such as radioisotope thermoelectric generators in some space probes such as Voyager 2. Generating electricity from fusion power remains the focus of international research.

Most nuclear power plants use thermal reactors with enriched uranium in a once-through fuel cycle. Fuel is removed when the percentage of neutron absorbing atoms becomes so large that a chain reaction can no longer be sustained, typically three years. It is then cooled for several years in on-site spent fuel pools before being transferred to long term storage. The spent fuel, though low in volume, is high-level radioactive waste. While its radioactivity decreases exponentially it must be isolated from the biosphere for hundreds of thousands of years, though newer technologies (like fast reactors) have the potential to reduce this significantly. Because the spent fuel is still mostly fissionable material, some countries (e.g. France and Russia) reprocess their spent fuel by extracting fissile and fertile elements for fabrication in new fuel, although this process is more expensive than producing new fuel from mined uranium. All reactors breed some plutonium-239, which is found in the spent fuel, and because Pu-239 is the preferred material for nuclear weapons, reprocessing is seen as a weapon proliferation risk.

The first nuclear power plant was built in the 1950s. The global installed nuclear capacity grew to 100 GW in the late 1970s, and then expanded rapidly during the 1980s, reaching 300 GW by 1990. The 1979 Three Mile Island accident in the United States and the 1986 Chernobyl disaster in the Soviet Union resulted in increased regulation and public opposition to nuclear plants. These factors, along with high cost of construction, resulted in the global installed capacity only increasing to 390 GW by 2022. These plants supplied 2,586 terawatt hours (TWh) of electricity in 2019, equivalent to about 10% of global electricity generation, and were the second-largest low-carbon power source after hydroelectricity. As of August 2023, there are 410 civilian fission reactors in the world, with overall capacity of 369 GW,[1] 57 under construction and 102 planned, with a combined capacity of 59 GW and 96 GW, respectively. The United States has the largest fleet of nuclear reactors, generating almost 800 TWh of low-carbon electricity per year with an average capacity factor of 92%. Average global capacity factor is 89%.[1] Most new reactors under construction are generation III reactors in Asia.

Proponents contend that nuclear power is a safe, sustainable energy source that reduces carbon emissions. This is because nuclear power generation causes one of the lowest levels of fatalities per unit of energy generated compared to other energy sources. Coal, petroleum, natural gas and hydroelectricity each have caused more fatalities per unit of energy due to air pollution and accidents. Nuclear power plants also emit no greenhouse gases and result in less life-cycle carbon emissions than common "renewables". The novel radiological hazards associated with nuclear power are the primary motivations of the anti-nuclear movement, which contends that nuclear power poses many threats to people and the environment, citing the potential for accidents like the Fukushima nuclear disaster in Japan in 2011, and is too expensive/slow to deploy when compared to alternative sustainable energy sources.
History
Main article: History of nuclear power
Origins

The first light bulbs ever lit by electricity generated by nuclear power at EBR-1 at Argonne National Laboratory-West, December 20, 1951.[2]
The discovery of nuclear fission occurred in 1938 following over four decades of work on the science of radioactivity and the elaboration of new nuclear physics that described the components of atoms. Soon after the discovery of the fission process, it was realized that a fissioning nucleus can induce further nucleus fissions, thus inducing a self-sustaining chain reaction.[3] Once this was experimentally confirmed in 1939, scientists in many countries petitioned their governments for support of nuclear fission research, just on the cusp of World War II, for the development of a nuclear weapon.[4]

In the United States, these research efforts led to the creation of the first man-made nuclear reactor, the Chicago Pile-1, which achieved criticality on December 2, 1942. The reactor's development was part of the Manhattan Project, the Allied effort to create atomic bombs during World War II. It led to the building of larger single-purpose production reactors for the production of weapons-grade plutonium for use in the first nuclear weapons. The United States tested the first nuclear weapon in July 1945, the Trinity test, with the atomic bombings of Hiroshima and Nagasaki taking place one month later.


The launching ceremony of the USS Nautilus January 1954. In 1958 it would become the first vessel to reach the North Pole.[5]

The Calder Hall nuclear power station in the United Kingdom, the world's first commercial nuclear power station.
Despite the military nature of the first nuclear devices, the 1940s and 1950s were characterized by strong optimism for the potential of nuclear power to provide cheap and endless energy.[6] Electricity was generated for the first time by a nuclear reactor on December 20, 1951, at the EBR-I experimental station near Arco, Idaho, which initially produced about 100 kW.[7][8] In 1953, American President Dwight Eisenhower gave his "Atoms for Peace" speech at the United Nations, emphasizing the need to develop "peaceful" uses of nuclear power quickly. This was followed by the Atomic Energy Act of 1954 which allowed rapid declassification of U.S. reactor technology and encouraged development by the private sector.

First power generation
The first organization to develop practical nuclear power was the U.S. Navy, with the S1W reactor for the purpose of propelling submarines and aircraft carriers. The first nuclear-powered submarine, USS Nautilus, was put to sea in January 1954.[9][10] The S1W reactor was a pressurized water reactor. This design was chosen because it was simpler, more compact, and easier to operate compared to alternative designs, thus more suitable to be used in submarines. This decision would result in the PWR being the reactor of choice also for power generation, thus having a lasting impact on the civilian electricity market in the years to come.[11]

On June 27, 1954, the Obninsk Nuclear Power Plant in the USSR became the world's first nuclear power plant to generate electricity for a power grid, producing around 5 megawatts of electric power.[12] The world's first commercial nuclear power station, Calder Hall at Windscale, England was connected to the national power grid on 27 August 1956. In common with a number of other generation I reactors, the plant had the dual purpose of producing electricity and plutonium-239, the latter for the nascent nuclear weapons program in Britain.[13]

Expansion and first opposition
The total global installed nuclear capacity initially rose relatively quickly, rising from less than 1 gigawatt (GW) in 1960 to 100 GW in the late 1970s.[9] During the 1970s and 1980s rising economic costs (related to extended construction times largely due to regulatory changes and pressure-group litigation)[14] and falling fossil fuel prices made nuclear power plants then under construction less attractive. In the 1980s in the U.S. and 1990s in Europe, the flat electric grid growth and electricity liberalization also made the addition of large new baseload energy generators economically unattractive.

The 1973 oil crisis had a significant effect on countries, such as France and Japan, which had relied more heavily on oil for electric generation to invest in nuclear power.[15] France would construct 25 nuclear power plants over the next 15 years,[16][17] and as of 2019, 71% of French electricity was generated by nuclear power, the highest percentage by any nation in the world.[18]

Some local opposition to nuclear power emerged in the United States in the early 1960s.[19] In the late 1960s some members of the scientific community began to express pointed concerns.[20] These anti-nuclear concerns related to nuclear accidents, nuclear proliferation, nuclear terrorism and radioactive waste disposal.[21] In the early 1970s, there were large protests about a proposed nuclear power plant in Wyhl, Germany. The project was cancelled in 1975. The anti-nuclear success at Wyhl inspired opposition to nuclear power in other parts of Europe and North America.[22][23]

By the mid-1970s anti-nuclear activism gained a wider appeal and influence, and nuclear power began to become an issue of major public protest.[24][25] In some countries, the nuclear power conflict "reached an intensity unprecedented in the history of technology controversies".[26][27] The increased public hostility to nuclear power led to a longer license procurement process, regulations and increased requirements for safety equipment, which made new construction much more expensive.[28][29] In the United States, over 120 LWR reactor proposals were ultimately cancelled[30] and the construction of new reactors ground to a halt.[31] The 1979 accident at Three Mile Island with no fatalities, played a major part in the reduction in the number of new plant constructions in many countries.[20]

Chernobyl and renaissance

The town of Pripyat abandoned since 1986, with the Chernobyl plant and the Chernobyl New Safe Confinement arch in the distance

Olkiluoto 3 under construction in 2009. It was the first EPR, a modernized PWR design, to start construction.
During the 1980s one new nuclear reactor started up every 17 days on average.[32] By the end of the decade, global installed nuclear capacity reached 300 GW. Since the late 1980s, new capacity additions slowed down significantly, with the installed nuclear capacity reaching 366 GW in 2005.

The 1986 Chernobyl disaster in the USSR, involving an RBMK reactor, altered the development of nuclear power and led to a greater focus on meeting international safety and regulatory standards.[33] It is considered the worst nuclear disaster in history both in total casualties, with 56 direct deaths, and financially, with the cleanup and the cost estimated at 18 billion Rbls (US$68 billion in 2019, adjusted for inflation).[34][35] The international organization to promote safety awareness and the professional development of operators in nuclear facilities, the World Association of Nuclear Operators (WANO), was created as a direct outcome of the 1986 Chernobyl accident. The Chernobyl disaster played a major part in the reduction in the number of new plant constructions in the following years.[20] Influenced by these events, Italy voted against nuclear power in a 1987 referendum, becoming the first country to completely phase out nuclear power in 1990.

In the early 2000s, nuclear energy was expecting a nuclear renaissance, an increase in the construction of new reactors, due to concerns about carbon dioxide emissions.[36] During this period, newer generation III reactors, such as the EPR began construction.
Prospects of a nuclear renaissance were delayed by another nuclear accident.[36][38] The 2011 Fukushima Daiichi nuclear accident was caused by the Tōhoku earthquake and tsunami, one of the largest earthquakes ever recorded. The Fukushima Daiichi Nuclear Power Plant suffered three core meltdowns due to failure of the emergency cooling system for lack of electricity supply. This resulted in the most serious nuclear accident since the Chernobyl disaster.

The accident prompted a re-examination of nuclear safety and nuclear energy policy in many countries.[39] Germany approved plans to close all its reactors by 2022, and many other countries reviewed their nuclear power programs.[40][41][42][43] Following the disaster, Japan shut down all of its nuclear power reactors, some of them permanently, and in 2015 began a gradual process to restart the remaining 40 reactors, following safety checks and based on revised criteria for operations and public approval.[44]

In 2022, the Japanese government, under the leadership of Prime Minister Fumio Kishida, has declared that 10 more nuclear power plants be reopened since the 2011 disaster.[45] Kishida is also pushing for research and construction of new safer nuclear plants to safeguard Japanese consumers from the fluctuating fossil fuel market and reduce Japan's greenhouse gas emissions.[46] Kishida intends to have Japan become a significant exporter of nuclear energy and technology to developing countries around the world..[46]

Current prospects
By 2015, the IAEA's outlook for nuclear energy had become more promising, recognizing the importance of low-carbon generation for mitigating climate change.[47] As of 2015, the global trend was for new nuclear power stations coming online to be balanced by the number of old plants being retired.[48] In 2016, the U.S. Energy Information Administration projected for its "base case" that world nuclear power generation would increase from 2,344 terawatt hours (TWh) in 2012 to 4,500 TWh in 2040. Most of the predicted increase was expected to be in Asia.[49] As of 2018, there are over 150 nuclear reactors planned including 50 under construction.[50] In January 2019, China had 45 reactors in operation, 13 under construction, and plans to build 43 more, which would make it the world's largest generator of nuclear electricity.[51] As of 2021, 17 reactors were reported to be under construction. China built significantly fewer reactors than originally planned, its share of electricity from nuclear power was 5% in 2019[52] and observers have cautioned that, along with the risks, the changing economics of energy generation may cause new nuclear energy plants to "no longer make sense in a world that is leaning toward cheaper, more reliable renewable energy".[53][54]

In October 2021, the Japanese cabinet approved the new Plan for Electricity Generation to 2030 prepared by the Agency for Natural Resources and Energy (ANRE) and an advisory committee, following public consultation. The nuclear target for 2030 requires the restart of another ten reactors. Prime Minister Fumio Kishida in July 2022 announced that the country should consider building advanced reactors and extending operating licences beyond 60 years.[55]

As of 2022, with world oil and gas prices on the rise, while Germany is restarting its coal plants to deal with loss of Russian gas that it needs to supplement its Energiewende,[56] many other countries have announced ambitious plans to reinvigorate ageing nuclear generating capacity with new investments. French President Emmanuel Macron announced his intention to build six new reactors in coming decades, placing nuclear at the heart of France's drive for carbon neutrality by 2050.[57] Meanwhile in the United States, the Department of Energy, in collaboration with commercial entities, TerraPower and X-energy, is planning on building two different advanced nuclear reactors by 2027, with further plans for nuclear implementation in its long term green energy and energy security goals.[58]

Power plants
An animation of a pressurized water reactor in operation

Graphs are unavailable due to technical issues.
Number of electricity-generating civilian reactors by type as of 2014[59]
  PWR   BWR   GCR   PHWR   LWGR   FBR
Main articles: Nuclear power plant and Nuclear reactor
See also: List of commercial nuclear reactors and List of nuclear power stations
Nuclear power plants are thermal power stations that generate electricity by harnessing the thermal energy released from nuclear fission. A fission nuclear power plant is generally composed of: a nuclear reactor, in which the nuclear reactions generating heat take place; a cooling system, which removes the heat from inside the reactor; a steam turbine, which transforms the heat into mechanical energy; an electric generator, which transforms the mechanical energy into electrical energy.[60]

When a neutron hits the nucleus of a uranium-235 or plutonium atom, it can split the nucleus into two smaller nuclei, which is a nuclear fission reaction. The reaction releases energy and neutrons. The released neutrons can hit other uranium or plutonium nuclei, causing new fission reactions, which release more energy and more neutrons. This is called a chain reaction. In most commercial reactors, the reaction rate is contained by control rods that absorb excess neutrons. The controllability of nuclear reactors depends on the fact that a small fraction of neutrons resulting from fission are delayed. The time delay between the fission and the release of the neutrons slows down changes in reaction rates and gives time for moving the control rods to adjust the reaction rate.[60][61]

Fuel cycle

The nuclear fuel cycle begins when uranium is mined, enriched, and manufactured into nuclear fuel (1), which is delivered to a nuclear power plant. After use, the spent fuel is delivered to a reprocessing plant (2) or to a final repository (3). In nuclear reprocessing 95% of spent fuel can potentially be recycled to be returned to use in a power plant (4).
Main articles: Nuclear fuel cycle and Integrated Nuclear Fuel Cycle Information System
The life cycle of nuclear fuel starts with uranium mining. The uranium ore is then converted into a compact ore concentrate form, known as yellowcake (U3O8), to facilitate transport.[62] Fission reactors generally need uranium-235, a fissile isotope of uranium. The concentration of uranium-235 in natural uranium is very low (about 0.7%). Some reactors can use this natural uranium as fuel, depending on their neutron economy. These reactors generally have graphite or heavy water moderators. For light water reactors, the most common type of reactor, this concentration is too low, and it must be increased by a process called uranium enrichment.[62] In civilian light water reactors, uranium is typically enriched to 3.5–5% uranium-235.[63] The uranium is then generally converted into uranium oxide (UO2), a ceramic, that is then compressively sintered into fuel pellets, a stack of which forms fuel rods of the proper composition and geometry for the particular reactor.[63]

After some time in the reactor, the fuel will have reduced fissile material and increased fission products, until its use becomes impractical.[63] At this point, the spent fuel will be moved to a spent fuel pool which provides cooling for the thermal heat and shielding for ionizing radiation. After several months or years, the spent fuel is radioactively and thermally cool enough to be moved to dry storage casks or reprocessed.[63]

Uranium resources
Main articles: Uranium market, Uranium mining, and Energy development § Nuclear

Proportions of the isotopes uranium-238 (blue) and uranium-235 (red) found in natural uranium and in enriched uranium for different applications. Light water reactors use 3–5% enriched uranium, while CANDU reactors work with natural uranium.
Uranium is a fairly common element in the Earth's crust: it is approximately as common as tin or germanium, and is about 40 times more common than silver.[64] Uranium is present in trace concentrations in most rocks, dirt, and ocean water, but is generally economically extracted only where it is present in high concentrations. Uranium mining can be underground, open-pit, or in-situ leach mining. An increasing number of the highest output mines are remote underground operations, such as McArthur River uranium mine, in Canada, which by itself accounts for 13% of global production. As of 2011 the world's known resources of uranium, economically recoverable at the arbitrary price ceiling of US$130/kg, were enough to last for between 70 and 100 years.[65][66][67] In 2007, the OECD estimated 670 years of economically recoverable uranium in total conventional resources and phosphate ores assuming the then-current use rate.[68]

Light water reactors make relatively inefficient use of nuclear fuel, mostly using only the very rare uranium-235 isotope.[69] Nuclear reprocessing can make this waste reusable, and newer reactors also achieve a more efficient use of the available resources than older ones.[69] With a pure fast reactor fuel cycle with a burn up of all the uranium and actinides (which presently make up the most hazardous substances in nuclear waste), there is an estimated 160,000 years worth of uranium in total conventional resources and phosphate ore at the price of 60–100 US$/kg.[70] However, reprocessing is expensive, possibly dangerous and can be used to manufacture nuclear weapons.[71][72][73][74][75] One analysis found that for uranium prices could increase by two orders of magnitudes between 2035 and 2100 and that there could be a shortage near the end of the century.[76] A 2017 study by researchers from MIT and WHOI found that "at the current consumption rate, global conventional reserves of terrestrial uranium (approximately 7.6 million tonnes) could be depleted in a little over a century".[77] Limited uranium-235 supply may inhibit substantial expansion with the current nuclear technology.[78] While various ways to reduce dependence on such resources are being explored,[79][80][81] new nuclear technologies are considered to not be available in time for climate change mitigation purposes or competition with alternatives of renewables in addition to being more expensive and require costly research and development.[78][82][83] A study found it to be uncertain whether identified resources will be developed quickly enough to provide uninterrupted fuel supply to expanded nuclear facilities[84] and various forms of mining may be challenged by ecological barriers, costs, and land requirements.[85][86] Researchers also report considerable import dependence of nuclear energy.[87][88][89][90]

Unconventional uranium resources also exist. Uranium is naturally present in seawater at a concentration of about 3 micrograms per liter,[91][92][93] with 4.4 billion tons of uranium considered present in seawater at any time.[94] In 2014 it was suggested that it would be economically competitive to produce nuclear fuel from seawater if the process was implemented at large scale.[95] Like fossil fuels, over geological timescales, uranium extracted on an industrial scale from seawater would be replenished by both river erosion of rocks and the natural process of uranium dissolved from the surface area of the ocean floor, both of which maintain the solubility equilibria of seawater concentration at a stable level.[94] Some commentators have argued that this strengthens the case for nuclear power to be considered a renewable energy.[96]

Waste
Main article: Nuclear waste

Typical composition of uranium dioxide fuel before and after approximately three years in the once-through nuclear fuel cycle of a LWR[97]
The normal operation of nuclear power plants and facilities produce radioactive waste, or nuclear waste. This type of waste is also produced during plant decommissioning. There are two broad categories of nuclear waste: low-level waste and high-level waste.[98] The first has low radioactivity and includes contaminated items such as clothing, which poses limited threat. High-level waste is mainly the spent fuel from nuclear reactors, which is very radioactive and must be cooled and then safely disposed of or reprocessed.[98]

High-level waste
Main articles: High-level waste and Spent nuclear fuel

Activity of spent UOx fuel in comparison to the activity of natural uranium ore over time[99][97]

Dry cask storage vessels storing spent nuclear fuel assemblies
The most important waste stream from nuclear power reactors is spent nuclear fuel, which is considered high-level waste. For LWRs, spent fuel is typically composed of 95% uranium, 4% fission products, and about 1% transuranic actinides (mostly plutonium, neptunium and americium).[100] The fission products are responsible for the bulk of the short-term radioactivity, whereas the plutonium and other transuranics are responsible for the bulk of the long-term radioactivity.[101]

High-level waste (HLW) must be stored isolated from the biosphere with sufficient shielding so as to limit radiation exposure. After being removed from the reactors, used fuel bundles are stored for six to ten years in spent fuel pools, which provide cooling and shielding against radiation. After that, the fuel is cool enough that it can be safely transferred to dry cask storage.[102] The radioactivity decreases exponentially with time, such that it will have decreased by 99.5% after 100 years.[103] The more intensely radioactive short-lived fission products (SLFPs) decay into stable elements in approximately 300 years, and after about 100,000 years, the spent fuel becomes less radioactive than natural uranium ore.[97][104]

Commonly suggested methods to isolate LLFP waste from the biosphere include separation and transmutation,[97] synroc treatments, or deep geological storage.[105][106][107][108]

Thermal-neutron reactors, which presently constitute the majority of the world fleet, cannot burn up the reactor grade plutonium that is generated during the reactor operation. This limits the life of nuclear fuel to a few years. In some countries, such as the United States, spent fuel is classified in its entirety as a nuclear waste.[109] In other countries, such as France, it is largely reprocessed to produce a partially recycled fuel, known as mixed oxide fuel or MOX. For spent fuel that does not undergo reprocessing, the most concerning isotopes are the medium-lived transuranic elements, which are led by reactor-grade plutonium (half-life 24,000 years).[110] Some proposed reactor designs, such as the integral fast reactor and molten salt reactors, can use as fuel the plutonium and other actinides in spent fuel from light water reactors, thanks to their fast fission spectrum. This offers a potentially more attractive alternative to deep geological disposal.[111][112][113]

The thorium fuel cycle results in similar fission products, though creates a much smaller proportion of transuranic elements from neutron capture events within a reactor. Spent thorium fuel, although more difficult to handle than spent uranium fuel, may present somewhat lower proliferation risks.[114]

Low-level waste
Main article: Low-level waste
The nuclear industry also produces a large volume of low-level waste, with low radioactivity, in the form of contaminated items like clothing, hand tools, water purifier resins, and (upon decommissioning) the materials of which the reactor itself is built. Low-level waste can be stored on-site until radiation levels are low enough to be disposed of as ordinary waste, or it can be sent to a low-level waste disposal site.[115]

Waste relative to other types
See also: Radioactive waste § Naturally occurring radioactive material
In countries with nuclear power, radioactive wastes account for less than 1% of total industrial toxic wastes, much of which remains hazardous for long periods.[69] Overall, nuclear power produces far less waste material by volume than fossil-fuel based power plants.[116] Coal-burning plants, in particular, produce large amounts of toxic and mildly radioactive ash resulting from the concentration of naturally occurring radioactive materials in coal.[117] A 2008 report from Oak Ridge National Laboratory concluded that coal power actually results in more radioactivity being released into the environment than nuclear power operation, and that the population effective dose equivalent from radiation from coal plants is 100 times that from the operation of nuclear plants.[118] Although coal ash is much less radioactive than spent nuclear fuel by weight, coal ash is produced in much higher quantities per unit of energy generated. It is also released directly into the environment as fly ash, whereas nuclear plants use shielding to protect the environment from radioactive materials.[119]

Nuclear waste volume is small compared to the energy produced. For example, at Yankee Rowe Nuclear Power Station, which generated 44 billion kilowatt hours of electricity when in service, its complete spent fuel inventory is contained within sixteen casks.[120] It is estimated that to produce a lifetime supply of energy for a person at a western standard of living (approximately 3 GWh) would require on the order of the volume of a soda can of low enriched uranium, resulting in a similar volume of spent fuel generated.[121][122][123]

Waste disposal
See also: List of radioactive waste treatment technologies
Storage of radioactive waste at WIPP
Nuclear waste flasks generated by the United States during the Cold War are stored underground at the Waste Isolation Pilot Plant (WIPP) in New Mexico. The facility is seen as a potential demonstration for storing spent fuel from civilian reactors.
Following interim storage in a spent fuel pool, the bundles of used fuel rod assemblies of a typical nuclear power station are often stored on site in dry cask storage vessels.[124] Presently, waste is mainly stored at individual reactor sites and there are over 430 locations around the world where radioactive material continues to accumulate.

Disposal of nuclear waste is often considered the most politically divisive aspect in the lifecycle of a nuclear power facility.[125] With the lack of movement of nuclear waste in the 2 billion year old natural nuclear fission reactors in Oklo, Gabon being cited as "a source of essential information today."[126][127] Experts suggest that centralized underground repositories which are well-managed, guarded, and monitored, would be a vast improvement.[125] There is an "international consensus on the advisability of storing nuclear waste in deep geological repositories".[128] With the advent of new technologies, other methods including horizontal drillhole disposal into geologically inactive areas have been proposed.[129][130]


Most waste packaging, small-scale experimental fuel recycling chemistry and radiopharmaceutical refinement is conducted within remote-handled hot cells.
There are no commercial scale purpose built underground high-level waste repositories in operation.[128][131][132] However, in Finland the Onkalo spent nuclear fuel repository of the Olkiluoto Nuclear Power Plant is under construction as of 2015.[133]

Reprocessing
Main article: Nuclear reprocessing
See also: Plutonium Management and Disposition Agreement
Most thermal-neutron reactors run on a once-through nuclear fuel cycle, mainly due to the low price of fresh uranium. However, many reactors are also fueled with recycled fissionable materials that remain in spent nuclear fuel. The most common fissionable material that is recycled is the reactor-grade plutonium (RGPu) that is extracted from spent fuel, it is mixed with uranium oxide and fabricated into mixed-oxide or MOX fuel. Because thermal LWRs remain the most common reactor worldwide, this type of recycling is the most common. It is considered to increase the sustainability of the nuclear fuel cycle, reduce the attractiveness of spent fuel to theft, and lower the volume of high level nuclear waste.[134] Spent MOX fuel cannot generally be recycled for use in thermal-neutron reactors. This issue does not affect fast-neutron reactors, which are therefore preferred in order to achieve the full energy potential of the original uranium.[135][136]

The main constituent of spent fuel from LWRs is slightly enriched uranium. This can be recycled into reprocessed uranium (RepU), which can be used in a fast reactor, used directly as fuel in CANDU reactors, or re-enriched for another cycle through an LWR. Re-enriching of reprocessed uranium is common in France and Russia.[137] Reprocessed uranium is also safer in terms of nuclear proliferation potential.[138][139][140]

Reprocessing has the potential to recover up to 95% of the uranium and plutonium fuel in spent nuclear fuel, as well as reduce long-term radioactivity within the remaining waste. However, reprocessing has been politically controversial because of the potential for nuclear proliferation and varied perceptions of increasing the vulnerability to nuclear terrorism.[135][141] Reprocessing also leads to higher fuel cost compared to the once-through fuel cycle.[135][141] While reprocessing reduces the volume of high-level waste, it does not reduce the fission products that are the primary causes of residual heat generation and radioactivity for the first few centuries outside the reactor. Thus, reprocessed waste still requires an almost identical treatment for the initial first few hundred years.

Reprocessing of civilian fuel from power reactors is currently done in France, the United Kingdom, Russia, Japan, and India. In the United States, spent nuclear fuel is currently not reprocessed.[137] The La Hague reprocessing facility in France has operated commercially since 1976 and is responsible for half the world's reprocessing as of 2010.[142] It produces MOX fuel from spent fuel derived from several countries. More than 32,000 tonnes of spent fuel had been reprocessed as of 2015, with the majority from France, 17% from Germany, and 9% from Japan.[143]

Breeding

Nuclear fuel assemblies being inspected before entering a pressurized water reactor in the United States
Main articles: Breeder reactor and Nuclear power proposed as renewable energy
Breeding is the process of converting non-fissile material into fissile material that can be used as nuclear fuel. The non-fissile material that can be used for this process is called fertile material, and constitute the vast majority of current nuclear waste. This breeding process occurs naturally in breeder reactors. As opposed to light water thermal-neutron reactors, which use uranium-235 (0.7% of all natural uranium), fast-neutron breeder reactors use uranium-238 (99.3% of all natural uranium) or thorium. A number of fuel cycles and breeder reactor combinations are considered to be sustainable or renewable sources of energy.[144][145] In 2006 it was estimated that with seawater extraction, there was likely five billion years' worth of uranium resources for use in breeder reactors.[146]

Breeder technology has been used in several reactors, but as of 2006, the high cost of reprocessing fuel safely requires uranium prices of more than US$200/kg before becoming justified economically.[147] Breeder reactors are however being developed for their potential to burn up all of the actinides (the most active and dangerous components) in the present inventory of nuclear waste, while also producing power and creating additional quantities of fuel for more reactors via the breeding process.[148][149] As of 2017, there are two breeders producing commercial power, BN-600 reactor and the BN-800 reactor, both in Russia.[150] The Phénix breeder reactor in France was powered down in 2009 after 36 years of operation.[150] Both China and India are building breeder reactors. The Indian 500 MWe Prototype Fast Breeder Reactor is in the commissioning phase,[151] with plans to build more.[152]

Another alternative to fast-neutron breeders are thermal-neutron breeder reactors that use uranium-233 bred from thorium as fission fuel in the thorium fuel cycle.[153] Thorium is about 3.5 times more common than uranium in the Earth's crust, and has different geographic characteristics.[153] India's three-stage nuclear power programme features the use of a thorium fuel cycle in the third stage, as it has abundant thorium reserves but little uranium.[153]

Decommissioning
Main article: Nuclear decommissioning
Nuclear decommissioning is the process of dismantling a nuclear facility to the point that it no longer requires measures for radiation protection,[154] returning the facility and its parts to a safe enough level to be entrusted for other uses.[155] Due to the presence of radioactive materials, nuclear decommissioning presents technical and economic challenges.[156] The costs of decommissioning are generally spread over the lifetime of a facility and saved in a decommissioning fund.[157]

Production
Further information: Nuclear power by country and List of nuclear reactors

Share of electricity production from nuclear, 2022[158]

The status of nuclear power globally (click for legend)
2019 world electricity generation by source (total generation was 27 petawatt-hours)[159][160]

  Coal (37%)
  Natural gas (24%)
  Hydro (16%)
  Nuclear (10%)
  Wind (5%)
  Solar (3%)
  Other (5%)
Civilian nuclear power supplied 2,586 terawatt hours (TWh) of electricity in 2019, equivalent to about 10% of global electricity generation, and was the second largest low-carbon power source after hydroelectricity.[37][161] Since electricity accounts for about 25% of world energy consumption, nuclear power's contribution to global energy was about 2.5% in 2011.[162] This is a little more than the combined global electricity production from wind, solar, biomass and geothermal power, which together provided 2% of global final energy consumption in 2014.[163] Nuclear power's share of global electricity production has fallen from 16.5% in 1997, in large part because the economics of nuclear power have become more difficult.[164]

As of March 2022, there are 439 civilian fission reactors in the world, with a combined electrical capacity of 392 gigawatt (GW). There are also 56 nuclear power reactors under construction and 96 reactors planned, with a combined capacity of 62 GW and 96 GW, respectively.[165] The United States has the largest fleet of nuclear reactors, generating over 800 TWh per year with an average capacity factor of 92%.[166] Most reactors under construction are generation III reactors in Asia.[167]

Regional differences in the use of nuclear power are large. The United States produces the most nuclear energy in the world, with nuclear power providing 20% of the electricity it consumes, while France produces the highest percentage of its electrical energy from nuclear reactors—71% in 2019.[18] In the European Union, nuclear power provides 26% of the electricity as of 2018.[168] Nuclear power is the single largest low-carbon electricity source in the United States,[169] and accounts for two-thirds of the European Union's low-carbon electricity.[170] Nuclear energy policy differs among European Union countries, and some, such as Austria, Estonia, Ireland and Italy, have no active nuclear power stations.

In addition, there were approximately 140 naval vessels using nuclear propulsion in operation, powered by about 180 reactors.[171][172] These include military and some civilian ships, such as nuclear-powered icebreakers.[173]

International research is continuing into additional uses of process heat such as hydrogen production (in support of a hydrogen economy), for desalinating sea water, and for use in district heating systems.[174]

Economics
Main articles: Economics of nuclear power plants, List of companies in the nuclear sector, and cost of electricity by source
The economics of new nuclear power plants is a controversial subject and multi-billion-dollar investments depend on the choice of energy sources. Nuclear power plants typically have high capital costs for building the plant. For this reason, comparison with other power generation methods is strongly dependent on assumptions about construction timescales and capital financing for nuclear plants. Fuel costs account for about 30 percent of the operating costs, while prices are subject to the market.[175]

The high cost of construction is one of the biggest challenges for nuclear power plants. A new 1,100 MW plant is estimated to cost between $6 billion to $9 billion.[176] Nuclear power cost trends show large disparity by nation, design, build rate and the establishment of familiarity in expertise. The only two nations for which data is available that saw cost decreases in the 2000s were India and South Korea.[177]

Analysis of the economics of nuclear power must also take into account who bears the risks of future uncertainties. As of 2010, all operating nuclear power plants have been developed by state-owned or regulated electric utility monopolies.[178] Many countries have since liberalized the electricity market where these risks, and the risk of cheaper competitors emerging before capital costs are recovered, are borne by plant suppliers and operators rather than consumers, which leads to a significantly different evaluation of the economics of new nuclear power plants.[179]

The levelized cost of electricity (LCOE) from a new nuclear power plant is estimated to be 69 USD/MWh, according to an analysis by the International Energy Agency and the OECD Nuclear Energy Agency. This represents the median cost estimate for an nth-of-a-kind nuclear power plant to be completed in 2025, at a discount rate of 7%. Nuclear power was found to be the least-cost option among dispatchable technologies.[180] Variable renewables can generate cheaper electricity: the median cost of onshore wind power was estimated to be 50 USD/MWh, and utility-scale solar power 56 USD/MWh.[180] At the assumed CO2 emission cost of 30 USD/ton, power from coal (88 USD/MWh) and gas (71 USD/MWh) is more expensive than low-carbon technologies. Electricity from long-term operation of nuclear power plants by lifetime extension was found the be the least-cost option, at 32 USD/MWh.[180] Measures to mitigate global warming, such as a carbon tax or carbon emissions trading, may favor the economics of nuclear power.[181][182] Extreme weather events, including events made more severe by climate change, are decreasing all energy source reliability including nuclear energy by a small degree, depending on location siting.[183][184]

New small modular reactors, such as those developed by NuScale Power, are aimed at reducing the investment costs for new construction by making the reactors smaller and modular, so that they can be built in a factory.

Certain designs had considerable early positive economics, such as the CANDU, which realized much higher capacity factor and reliability when compared to generation II light water reactors up to the 1990s.[185]

Nuclear power plants, though capable of some grid-load following, are typically run as much as possible to keep the cost of the generated electrical energy as low as possible, supplying mostly base-load electricity.[186] Due to the on-line refueling reactor design, PHWRs (of which the CANDU design is a part) continue to hold many world record positions for longest continual electricity generation, often over 800 days.[187] The specific record as of 2019 is held by a PHWR at Kaiga Atomic Power Station, generating electricity continuously for 962 days.[188]

Costs not considered in LCOE calculations include funds for research and development, and disasters (the Fukushima disaster is estimated to cost taxpayers ≈$187 billion[189]). Governments were found to in some cases force "consumers to pay upfront for potential cost overruns"[83] or subsidize uneconomic nuclear energy[190] or be required to do so.[54] Nuclear operators are liable to pay for the waste management in the EU.[191] In the U.S. the Congress reportedly decided 40 years ago that the nation, and not private companies, would be responsible for storing radioactive waste with taxpayers paying for the costs.[192] The World Nuclear Waste Report 2019 found that "even in countries in which the polluter-pays-principle is a legal requirement, it is applied incompletely" and notes the case of the German Asse II deep geological disposal facility, where the retrieval of large amounts of waste has to be paid for by taxpayers.[193] Similarly, other forms of energy, including fossil fuels and renewables, have a portion of their costs covered by governments.[194]

Use in space

The multi-mission radioisotope thermoelectric generator (MMRTG), used in several space missions such as the Curiosity Mars rover
Main article: Nuclear power in space
The most common use of nuclear power in space is the use of radioisotope thermoelectric generators, which use radioactive decay to generate power. These power generators are relatively small scale (few kW), and they are mostly used to power space missions and experiments for long periods where solar power is not available in sufficient quantity, such as in the Voyager 2 space probe.[195] A few space vehicles have been launched using nuclear reactors: 34 reactors belong to the Soviet RORSAT series and one was the American SNAP-10A.[195]

Both fission and fusion appear promising for space propulsion applications, generating higher mission velocities with less reaction mass.[195][196]

Safety
See also: Nuclear safety and security and Nuclear reactor safety system

Death rates from air pollution and accidents related to energy production, measured in deaths in the past per terawatt hours (TWh)
Nuclear power plants have three unique characteristics that affect their safety, as compared to other power plants. Firstly, intensely radioactive materials are present in a nuclear reactor. Their release to the environment could be hazardous. Secondly, the fission products, which make up most of the intensely radioactive substances in the reactor, continue to generate a significant amount of decay heat even after the fission chain reaction has stopped. If the heat cannot be removed from the reactor, the fuel rods may overheat and release radioactive materials. Thirdly, a criticality accident (a rapid increase of the reactor power) is possible in certain reactor designs if the chain reaction cannot be controlled. These three characteristics have to be taken into account when designing nuclear reactors.[197]

All modern reactors are designed so that an uncontrolled increase of the reactor power is prevented by natural feedback mechanisms, a concept known as negative void coefficient of reactivity. If the temperature or the amount of steam in the reactor increases, the fission rate inherently decreases. The chain reaction can also be manually stopped by inserting control rods into the reactor core. Emergency core cooling systems (ECCS) can remove the decay heat from the reactor if normal cooling systems fail.[198] If the ECCS fails, multiple physical barriers limit the release of radioactive materials to the environment even in the case of an accident. The last physical barrier is the large containment building.[197]

With a death rate of 0.03 per TWh, nuclear power is the second safest energy source per unit of energy generated, after solar power, in terms of mortality when the historical track-record is considered.[199] Energy produced by coal, petroleum, natural gas and hydropower has caused more deaths per unit of energy generated due to air pollution and energy accidents. This is found when comparing the immediate deaths from other energy sources to both the immediate and the latent, or predicted, indirect cancer deaths from nuclear energy accidents.[200][201] When the direct and indirect fatalities (including fatalities resulting from the mining and air pollution) from nuclear power and fossil fuels are compared,[202] the use of nuclear power has been calculated to have prevented about 1.84 million deaths from air pollution between 1971 and 2009, by reducing the proportion of energy that would otherwise have been generated by fossil fuels.[203][204] Following the 2011 Fukushima nuclear disaster, it has been estimated that if Japan had never adopted nuclear power, accidents and pollution from coal or gas plants would have caused more lost years of life.[205]

Serious impacts of nuclear accidents are often not directly attributable to radiation exposure, but rather social and psychological effects. Evacuation and long-term displacement of affected populations created problems for many people, especially the elderly and hospital patients.[206] Forced evacuation from a nuclear accident may lead to social isolation, anxiety, depression, psychosomatic medical problems, reckless behavior, and suicide. A comprehensive 2005 study on the aftermath of the Chernobyl disaster concluded that the mental health impact is the largest public health problem caused by the accident.[207] Frank N. von Hippel, an American scientist, commented that a disproportionate fear of ionizing radiation (radiophobia) could have long-term psychological effects on the population of contaminated areas following the Fukushima disaster.[208]

Accidents

Following the 2011 Fukushima Daiichi nuclear disaster, the world's worst nuclear accident since 1986, 50,000 households were displaced after radiation leaked into the air, soil and sea.[209] Radiation checks led to bans of some shipments of vegetables and fish.[210]

Reactor decay heat as a fraction of full power after the reactor shutdown, using two different correlations. To remove the decay heat, reactors need cooling after the shutdown of the fission reactions. A loss of the ability to remove decay heat caused the Fukushima accident.
See also: Energy accidents, Nuclear and radiation accidents and incidents, and Lists of nuclear disasters and radioactive incidents
Some serious nuclear and radiation accidents have occurred. The severity of nuclear accidents is generally classified using the International Nuclear Event Scale (INES) introduced by the International Atomic Energy Agency (IAEA). The scale ranks anomalous events or accidents on a scale from 0 (a deviation from normal operation that poses no safety risk) to 7 (a major accident with widespread effects). There have been three accidents of level 5 or higher in the civilian nuclear power industry, two of which, the Chernobyl accident and the Fukushima accident, are ranked at level 7.

The first major nuclear accidents were the Kyshtym disaster in the Soviet Union and the Windscale fire in the United Kingdom, both in 1957. The first major accident at a nuclear reactor in the USA occurred in 1961 at the SL-1, a U.S. Army experimental nuclear power reactor at the Idaho National Laboratory. An uncontrolled chain reaction resulted in a steam explosion which killed the three crew members and caused a meltdown.[211][212] Another serious accident happened in 1968, when one of the two liquid-metal-cooled reactors on board the Soviet submarine K-27 underwent a fuel element failure, with the emission of gaseous fission products into the surrounding air, resulting in 9 crew fatalities and 83 injuries.[213]

The Fukushima Daiichi nuclear accident was caused by the 2011 Tohoku earthquake and tsunami. The accident has not caused any radiation-related deaths but resulted in radioactive contamination of surrounding areas. The difficult cleanup operation is expected to cost tens of billions of dollars over 40 or more years.[214][215] The Three Mile Island accident in 1979 was a smaller scale accident, rated at INES level 5. There were no direct or indirect deaths caused by the accident.[216]

The impact of nuclear accidents is controversial. According to Benjamin K. Sovacool, fission energy accidents ranked first among energy sources in terms of their total economic cost, accounting for 41 percent of all property damage attributed to energy accidents.[217] Another analysis found that coal, oil, liquid petroleum gas and hydroelectric accidents (primarily due to the Banqiao Dam disaster) have resulted in greater economic impacts than nuclear power accidents.[218] The study compares latent cancer deaths attributable to nuclear with immediate deaths from other energy sources per unit of energy generated, and does not include fossil fuel related cancer and other indirect deaths created by the use of fossil fuel consumption in its "severe accident" (an accident with more than five fatalities) classification. The Chernobyl accident in 1986 caused approximately 50 deaths from direct and indirect effects, and some temporary serious injuries from acute radiation syndrome.[219] The future predicted mortality from increases in cancer rates is estimated at 4000 in the decades to come.[220][221][222] However, the costs have been large and are increasing.

Nuclear power works under an insurance framework that limits or structures accident liabilities in accordance with national and international conventions.[223] It is often argued that this potential shortfall in liability represents an external cost not included in the cost of nuclear electricity. This cost is small, amounting to about 0.1% of the levelized cost of electricity, according to a study by the Congressional Budget Office in the United States.[224] These beyond-regular insurance costs for worst-case scenarios are not unique to nuclear power. Hydroelectric power plants are similarly not fully insured against a catastrophic event such as dam failures. For example, the failure of the Banqiao Dam caused the death of an estimated 30,000 to 200,000 people, and 11 million people lost their homes. As private insurers base dam insurance premiums on limited scenarios, major disaster insurance in this sector is likewise provided by the state.[225]

Attacks and sabotage
Main articles: Vulnerability of nuclear plants to attack, Nuclear terrorism, and Nuclear safety in the United States
Terrorists could target nuclear power plants in an attempt to release radioactive contamination into the community. The United States 9/11 Commission has said that nuclear power plants were potential targets originally considered for the September 11, 2001 attacks. An attack on a reactor's spent fuel pool could also be serious, as these pools are less protected than the reactor core. The release of radioactivity could lead to thousands of near-term deaths and greater numbers of long-term fatalities.[226]

In the United States, the NRC carries out "Force on Force" (FOF) exercises at all nuclear power plant sites at least once every three years.[226] In the United States, plants are surrounded by a double row of tall fences which are electronically monitored. The plant grounds are patrolled by a sizeable force of armed guards.[227]

Insider sabotage is also a threat because insiders can observe and work around security measures. Successful insider crimes depended on the perpetrators' observation and knowledge of security vulnerabilities.[228] A fire caused 5–10 million dollars worth of damage to New York's Indian Point Energy Center in 1971.[229] The arsonist was a plant maintenance worker.[230]

Proliferation
Further information: Nuclear proliferation
See also: Plutonium Management and Disposition Agreement

United States and USSR/Russian nuclear weapons stockpiles, 1945–2006. The Megatons to Megawatts Program was the main driving force behind the sharp reduction in the quantity of nuclear weapons worldwide since the cold war ended.[231][232]

The guided-missile cruiser USS Monterey (CG 61) receives fuel at sea (FAS) from the Nimitz-class aircraft carrier USS George Washington (CVN 73).
Nuclear proliferation is the spread of nuclear weapons, fissionable material, and weapons-related nuclear technology to states that do not already possess nuclear weapons. Many technologies and materials associated with the creation of a nuclear power program have a dual-use capability, in that they can also be used to make nuclear weapons. For this reason, nuclear power presents proliferation risks.

Nuclear power program can become a route leading to a nuclear weapon. An example of this is the concern over Iran's nuclear program.[233] The re-purposing of civilian nuclear industries for military purposes would be a breach of the Non-Proliferation Treaty, to which 190 countries adhere. As of April 2012, there are thirty one countries that have civil nuclear power plants,[234] of which nine have nuclear weapons. The vast majority of these nuclear weapons states have produced weapons before commercial nuclear power stations.

A fundamental goal for global security is to minimize the nuclear proliferation risks associated with the expansion of nuclear power.[233] The Global Nuclear Energy Partnership was an international effort to create a distribution network in which developing countries in need of energy would receive nuclear fuel at a discounted rate, in exchange for that nation agreeing to forgo their own indigenous development of a uranium enrichment program. The France-based Eurodif/European Gaseous Diffusion Uranium Enrichment Consortium is a program that successfully implemented this concept, with Spain and other countries without enrichment facilities buying a share of the fuel produced at the French-controlled enrichment facility, but without a transfer of technology.[235] Iran was an early participant from 1974 and remains a shareholder of Eurodif via Sofidif.

A 2009 United Nations report said that:

the revival of interest in nuclear power could result in the worldwide dissemination of uranium enrichment and spent fuel reprocessing technologies, which present obvious risks of proliferation as these technologies can produce fissile materials that are directly usable in nuclear weapons.[236]

On the other hand, power reactors can also reduce nuclear weapons arsenals when military-grade nuclear materials are reprocessed to be used as fuel in nuclear power plants. The Megatons to Megawatts Program is considered the single most successful non-proliferation program to date.[231] Up to 2005, the program had processed $8 billion of high enriched, weapons-grade uranium into low enriched uranium suitable as nuclear fuel for commercial fission reactors by diluting it with natural uranium. This corresponds to the elimination of 10,000 nuclear weapons.[237] For approximately two decades, this material generated nearly 10 percent of all the electricity consumed in the United States, or about half of all U.S. nuclear electricity, with a total of around 7,000 TWh of electricity produced.[238] In total it is estimated to have cost $17 billion, a "bargain for US ratepayers", with Russia profiting $12 billion from the deal.[238] Much needed profit for the Russian nuclear oversight industry, which after the collapse of the Soviet economy, had difficulties paying for the maintenance and security of the Russian Federations highly enriched uranium and warheads.[239] The Megatons to Megawatts Program was hailed as a major success by anti-nuclear weapon advocates as it has largely been the driving force behind the sharp reduction in the number of nuclear weapons worldwide since the cold war ended.[231] However, without an increase in nuclear reactors and greater demand for fissile fuel, the cost of dismantling and down blending has dissuaded Russia from continuing their disarmament. As of 2013 Russia appears to not be interested in extending the program.[240]

Environmental impact
Main article: Environmental impact of nuclear power

The Ikata Nuclear Power Plant, a pressurized water reactor that cools by utilizing a secondary coolant heat exchanger with a large body of water, an alternative cooling approach to large cooling towers
Being a low-carbon energy source with relatively little land-use requirements, nuclear energy can have a positive environmental impact. It also requires a constant supply of significant amounts of water and affects the environment through mining and milling.[241][242][243][244] Its largest potential negative impacts on the environment may arise from its transgenerational risks for nuclear weapons proliferation that may increase risks of their use in the future, risks for problems associated with the management of the radioactive waste such as groundwater contamination, risks for accidents and for risks for various forms of attacks on waste storage sites or reprocessing- and power-plants.[71][245][246][247][248][244][249][250] However, these remain mostly only risks as historically there have only been few disasters at nuclear power plants with known relatively substantial environmental impacts.

Carbon emissions
See also: Life-cycle greenhouse gas emissions of energy sources
Further information: § Historic effect on carbon emissions
Part of a series on
Climate change mitigation
Climate changeCo-benefits of mitigationGreenhouse gas emissions
Energy
Carbon capture and storageCoal phase-outEnergy transitionFossil fuel phase-outSustainable energyLow-carbon powerRenewable energyNuclear powerSolar powerWind power
Economics
Carbon sinks
Other sectors
icon Portal Glossary Index
vte

Life-cycle greenhouse gas emissions of electricity supply technologies, median values calculated by IPCC[251]
Nuclear power is one of the leading low carbon power generation methods of producing electricity, and in terms of total life-cycle greenhouse gas emissions per unit of energy generated, has emission values comparable to or lower than renewable energy.[252][253] A 2014 analysis of the carbon footprint literature by the Intergovernmental Panel on Climate Change (IPCC) reported that the embodied total life-cycle emission intensity of nuclear power has a median value of 12 g CO2eq/kWh, which is the lowest among all commercial baseload energy sources.[251][254] This is contrasted with coal and natural gas at 820 and 490 g CO2 eq/kWh.[251][254] As of 2021, nuclear reactors worldwide have helped avoid the emission of 72 billion tonnes of carbon dioxide since 1970, compared to coal-fired electricity generation, according to a report.[204][255]

Radiation
The average dose from natural background radiation is 2.4 millisievert per year (mSv/a) globally. It varies between 1 mSv/a and 13 mSv/a, depending mostly on the geology of the location. According to the United Nations (UNSCEAR), regular nuclear power plant operations, including the nuclear fuel cycle, increases this amount by 0.0002 mSv/a of public exposure as a global average. The average dose from operating nuclear power plants to the local populations around them is less than 0.0001 mSv/a.[256] For comparison, the average dose to those living within 50 miles of a coal power plant is over three times this dose, at 0.0003 mSv/a.[257]

Chernobyl resulted in the most affected surrounding populations and male recovery personnel receiving an average initial 50 to 100 mSv over a few hours to weeks, while the remaining global legacy of the worst nuclear power plant accident in average exposure is 0.002 mSv/a and is continuously dropping at the decaying rate, from the initial high of 0.04 mSv per person averaged over the entire populace of the Northern Hemisphere in the year of the accident in 1986.[256]

Debate
Main article: Nuclear power debate
See also: Nuclear energy policy, Pro-nuclear movement, and Anti-nuclear movement

A comparison of prices over time for energy from nuclear fission and from other sources. Over the presented time, thousands of wind turbines and similar were built on assembly lines in mass production resulting in an economy of scale. While nuclear remains bespoke, many first of their kind facilities added in the timeframe indicated and none are in serial production. Our World in Data notes that this cost is the global average, while the 2 projects that drove nuclear pricing upwards were in the US. The organization recognises that the median cost of the most exported and produced nuclear energy facility in the 2010s the South Korean APR1400, remained "constant", including in export.[258]
LCOE is a measure of the average net present cost of electricity generation for a generating plant over its lifetime. As a metric, it remains controversial as the lifespan of units are not independent but manufacturer projections, not a demonstrated longevity.
The nuclear power debate concerns the controversy which has surrounded the deployment and use of nuclear fission reactors to generate electricity from nuclear fuel for civilian purposes.[25][259][26]

Proponents of nuclear energy regard it as a sustainable energy source that reduces carbon emissions and increases energy security by decreasing dependence on other energy sources that are also[88][89][90] often dependent on imports.[260][261][262] For example, proponents note that annually, nuclear-generated electricity reduces 470 million metric tons of carbon dioxide emissions that would otherwise come from fossil fuels.[263] Additionally, the amount of comparatively low waste that nuclear energy does create is safely disposed of by the large scale nuclear energy production facilities or it is repurposed/recycled for other energy uses.[264] M. King Hubbert, who popularized the concept of peak oil, saw oil as a resource that would run out and considered nuclear energy its replacement.[265] Proponents also claim that the present quantity of nuclear waste is small and can be reduced through the latest technology of newer reactors and that the operational safety record of fission-electricity in terms of deaths is so far "unparalleled".[14] Kharecha and Hansen estimated that "global nuclear power has prevented an average of 1.84 million air pollution-related deaths and 64 gigatonnes of CO2-equivalent (GtCO2-eq) greenhouse gas (GHG) emissions that would have resulted from fossil fuel burning" and, if continued, it could prevent up to 7 million deaths and 240 GtCO2-eq emissions by 2050.[204]

Proponents also bring to attention the opportunity cost of utilizing other forms of electricity. For example, the Environmental Protection Agency estimates that coal kills 30,000 people a year,[266] as a result of its environmental impact, while 60 people died in the Chernobyl disaster.[267] A real world example of impact provided by proponents is the 650,000 ton increase in carbon emissions in the two months following the closure of the Vermont Yankee nuclear plant.[268]

Opponents believe that nuclear power poses many threats to people's health and environment[269][270] such as the risk of nuclear weapons proliferation, long-term safe waste management and terrorism in the future.[271][272] They also contend that nuclear power plants are complex systems where many things can and have gone wrong.[273][274] Costs of the Chernobyl disaster amount to ≈$68 billion as of 2019 and are increasing,[34] the Fukushima disaster is estimated to cost taxpayers ~$187 billion,[189] and radioactive waste management is estimated to cost the EU nuclear operators ~$250 billion by 2050.[191] However, in countries that already use nuclear energy, when not considering reprocessing, intermediate nuclear waste disposal costs could be relatively fixed to certain but unknown degrees[275] "as the main part of these costs stems from the operation of the intermediate storage facility".[276]

Critics find that one of the largest drawbacks to building new nuclear fission power plants are the large construction and operating costs when compared to alternatives of sustainable energy sources.[53][277][82][243][278] Further costs include costs for ongoing research and development, expensive reprocessing in cases where such is practiced[71][72][73][75] and decommissioning.[279][280][281] Proponents note that focussing on the Levelized Cost of Energy (LCOE), however, ignores the value premium associated with 24/7 dispatchable electricity and the cost of storage and backup systems necessary to integrate variable energy sources into a reliable electrical grid.[282] "Nuclear thus remains the dispatchable low-carbon technology with the lowest expected costs in 2025. Only large hydro reservoirs can provide a similar contribution at comparable costs but remain highly dependent on the natural endowments of individual countries."[283]


Anti-nuclear protest near nuclear waste disposal centre at Gorleben in northern Germany
Overall, many opponents find that nuclear energy cannot meaningfully contribute to climate change mitigation. In general, they find it to be, too dangerous, too expensive, to take too long for deployment, to be an obstacle to achieving a transition towards sustainability and carbon-neutrality,[82][284][285][286] effectively being a distracting[287][288] competition for resources (i.e. human, financial, time, infrastructure and expertise) for the deployment and development of alternative, sustainable, energy system technologies[83][288][82][289] (such as for wind, ocean and solar[82] – including e.g. floating solar – as well as ways to manage their intermittency other than nuclear baseload[290] generation such as dispatchable generation, renewables-diversification,[291][292] super grids, flexible energy demand and supply regulating smart grids and energy storage[293][294][295][296][297] technologies).[298][299][300][301][302][303][304][305][250]

Nevertheless, there is ongoing research and debate over costs of new nuclear, especially in regions where i.a. seasonal energy storage is difficult to provide and which aim to phase out fossil fuels in favor of low carbon power faster than the global average.[306] Some find that financial transition costs for a 100% renewables-based European energy system that has completely phased out nuclear energy could be more costly by 2050 based on current technologies (i.e. not considering potential advances in e.g. green hydrogen, transmission and flexibility capacities, ways to reduce energy needs, geothermal energy and fusion energy) when the grid only extends across Europe.[307] Arguments of economics and safety are used by both sides of the debate.

Comparison with renewable energy
See also: Renewable energy debate
Slowing global warming requires a transition to a low-carbon economy, mainly by burning far less fossil fuel. Limiting global warming to 1.5 °C is technically possible if no new fossil fuel power plants are built from 2019.[308] This has generated considerable interest and dispute in determining the best path forward to rapidly replace fossil-based fuels in the global energy mix,[309][310] with intense academic debate.[311][312] Sometimes the IEA says that countries without nuclear should develop it as well as their renewable power.[313]

World total primary energy supply of 162,494 TWh (or 13,792 Mtoe) by fuels in 2017 (IEA, 2019)[314]: 6, 8 

  Oil (32%)
  Coal/Peat/Shale (27.1%)
  Natural Gas (22.2%)
  Biofuels and waste (9.5%)
  Nuclear (4.9%)
  Hydro (2.5%)
  Others (Renewables) (1.8%)
Several studies suggest that it might be theoretically possible to cover a majority of world energy generation with new renewable sources. The Intergovernmental Panel on Climate Change (IPCC) has said that if governments were supportive, renewable energy supply could account for close to 80% of the world's energy use by 2050.[315] While in developed nations the economically feasible geography for new hydropower is lacking, with every geographically suitable area largely already exploited,[316] some proponents of wind and solar energy claim these resources alone could eliminate the need for nuclear power.[312][317]

Nuclear power is comparable to, and in some cases lower, than many renewable energy sources in terms of lives lost in the past per unit of electricity delivered.[202][200][318] Depending on recycling of renewable energy technologies, nuclear reactors may produce a much smaller volume of waste, although much more toxic, expensive to manage and longer-lived.[319][246] A nuclear plant also needs to be disassembled and removed and much of the disassembled nuclear plant needs to be stored as low-level nuclear waste for a few decades.[320] The disposal and management of the wide variety[321] of radioactive waste, of which there are over one quarter of a million tons as of 2018, can cause future damage and costs across the world for over or during hundreds of thousands of years[322][323][324] – possibly over a million years,[325][326][327][328] due to issues such as leakage,[329] malign retrieval, vulnerability to attacks (including of reprocessing[74][71] and power plants), groundwater contamination, radiation and leakage to above ground, brine leakage or bacterial corrosion.[330][325][331][332] The European Commission Joint Research Centre found that as of 2021 the necessary technologies for geological disposal of nuclear waste are now available and can be deployed.[333] Corrosion experts noted in 2020 that putting the problem of storage off any longer "isn't good for anyone".[334] Separated plutonium and enriched uranium could be used for nuclear weapons, which – even with the current centralized control (e.g. state-level) and level of prevalence – are considered to be a difficult and substantial global risk for substantial future impacts on human health, lives, civilization and the environment.[71][245][246][247][248]

Speed of transition and investment needed
Analysis in 2015 by professor Barry W. Brook and colleagues found that nuclear energy could displace or remove fossil fuels from the electric grid completely within 10 years. This finding was based on the historically modest and proven rate at which nuclear energy was added in France and Sweden during their building programs in the 1980s.[335][336] In a similar analysis, Brook had earlier determined that 50% of all global energy, including transportation synthetic fuels etc., could be generated within approximately 30 years if the global nuclear fission build rate was identical to historical proven installation rates calculated in GW per year per unit of global GDP (GW/year/$).[337] This is in contrast to the conceptual studies for 100% renewable energy systems, which would require an order of magnitude more costly global investment per year, which has no historical precedent.[338] These renewable scenarios would also need far greater land devoted to onshore wind and onshore solar projects.[337][338] Brook notes that the "principal limitations on nuclear fission are not technical, economic or fuel-related, but are instead linked to complex issues of societal acceptance, fiscal and political inertia, and inadequate critical evaluation of the real-world constraints facing [the other] low-carbon alternatives."[337]

Scientific data indicates that – assuming 2021 emissions levels – humanity only has a carbon budget equivalent to 11 years of emissions left for limiting warming to 1.5 °C[339][340] while the construction of new nuclear reactors took a median of 7.2–10.9 years in 2018–2020,[332] substantially longer than, alongside other measures, scaling up the deployment of wind and solar – especially for novel reactor types – as well as being more risky, often delayed and more dependent on state-support.[341][342][285][287][82][343][298] Researchers have cautioned that novel nuclear technologies – which have been in development since decades,[344][82][277] are less tested, have higher proliferation risks, have more new safety problems, are often far from commercialization and are more expensive[277][82][243][345] – are not available in time.[78][83][346][287][347][297][348] Critics of nuclear energy often only oppose nuclear fission energy but not nuclear fusion; however, fusion energy is unlikely to be commercially widespread before 2050.[349][350][351][352][353]

Land use
The median land area used by US nuclear power stations per 1 GW installed capacity is 1.3 square miles.[354][355] To generate the same amount of electricity annually (taking into account capacity factors) from solar PV would require about 60 square miles, and from a wind farm about 310 square miles.[354][355] Not included in this is land required for the associated transmission lines, water supply, rail lines, mining and processing of nuclear fuel, and for waste disposal.[356]

Research
Advanced fission reactor designs
Main article: Generation IV reactor
Current fission reactors in operation around the world are second or third generation systems, with most of the first-generation systems having been already retired. Research into advanced generation IV reactor types was officially started by the Generation IV International Forum (GIF) based on eight technology goals, including to improve economics, safety, proliferation resistance, natural resource utilization and the ability to consume existing nuclear waste in the production of electricity. Most of these reactors differ significantly from current operating light water reactors, and are expected to be available for commercial construction after 2030.[357]

Hybrid fusion-fission
Main article: Nuclear fusion–fission hybrid
Hybrid nuclear power is a proposed means of generating power by the use of a combination of nuclear fusion and fission processes. The concept dates to the 1950s and was briefly advocated by Hans Bethe during the 1970s, but largely remained unexplored until a revival of interest in 2009, due to delays in the realization of pure fusion. When a sustained nuclear fusion power plant is built, it has the potential to be capable of extracting all the fission energy that remains in spent fission fuel, reducing the volume of nuclear waste by orders of magnitude, and more importantly, eliminating all actinides present in the spent fuel, substances which cause security concerns.[358]

Fusion

Schematic of the ITER tokamak under construction in France
Main articles: Nuclear fusion and Fusion power
Nuclear fusion reactions have the potential to be safer and generate less radioactive waste than fission.[359][360] These reactions appear potentially viable, though technically quite difficult and have yet to be created on a scale that could be used in a functional power plant. Fusion power has been under theoretical and experimental investigation since the 1950s. Nuclear fusion research is underway but fusion energy is not likely to be commercially widespread before 2050.[361][362][363]

Several experimental nuclear fusion reactors and facilities exist. The largest and most ambitious international nuclear fusion project currently in progress is ITER, a large tokamak under construction in France. ITER is planned to pave the way for commercial fusion power by demonstrating self-sustained nuclear fusion reactions with positive energy gain. Construction of the ITER facility began in 2007, but the project has run into many delays and budget overruns. The facility is now not expected to begin operations until the year 2027–11 years after initially anticipated.[364] A follow on commercial nuclear fusion power station, DEMO, has been proposed.[349][365] There are also suggestions for a power plant based upon a different fusion approach, that of an inertial fusion power plant.

Fusion-powered electricity generation was initially believed to be readily achievable, as fission-electric power had been. However, the extreme requirements for continuous reactions and plasma containment led to projections being extended by several decades. In 2020, more than 80 years after the first attempts, commercialization of fusion power production was thought to be unlikely before 2050.[349][350][351][352][353]

To enhance and accelerate the development of fusion energy, the United States Department of Energy (DOE) granted $46 million to eight firms, including Commonwealth Fusion Systems and Tokamak Energy Inc, in 2023. This ambitious initiative aims to introduce pilot-scale fusion within a decade.[366]

See also
A nuclear reactor is a device used to initiate and control a fission nuclear chain reaction or nuclear fusion reactions. Nuclear reactors are used at nuclear power plants for electricity generation and in nuclear marine propulsion. Heat from nuclear fission is passed to a working fluid (water or gas), which in turn runs through steam turbines. These either drive a ship's propellers or turn electrical generators' shafts. Nuclear generated steam in principle can be used for industrial process heat or for district heating. Some reactors are used to produce isotopes for medical and industrial use, or for production of weapons-grade plutonium. As of 2022, the International Atomic Energy Agency reports there are 422 nuclear power reactors and 223 nuclear research reactors in operation around the world.[1][2][3]

In the early era of nuclear reactors (1940s), a reactor was known as a nuclear pile or atomic pile (so-called because the graphite moderator blocks of the first reactor to reach criticality were stacked in a pile).[4]

Operation
Main article: Nuclear reactor physics

An example of an induced nuclear fission event. A neutron is absorbed by the nucleus of a uranium-235 atom, which in turn splits into fast-moving lighter elements (fission products) and free neutrons. Though both reactors and nuclear weapons rely on nuclear chain reactions, the rate of reactions in a reactor is much slower than in a bomb.
Just as conventional thermal power stations generate electricity by harnessing the thermal energy released from burning fossil fuels, nuclear reactors convert the energy released by controlled nuclear fission into thermal energy for further conversion to mechanical or electrical forms.

Fission
Main article: Nuclear fission
When a large fissile atomic nucleus such as uranium-235, uranium-233, or plutonium-239 absorbs a neutron, it may undergo nuclear fission. The heavy nucleus splits into two or more lighter nuclei, (the fission products), releasing kinetic energy, gamma radiation, and free neutrons. A portion of these neutrons may be absorbed by other fissile atoms and trigger further fission events, which release more neutrons, and so on. This is known as a nuclear chain reaction.

To control such a nuclear chain reaction, control rods containing neutron poisons and neutron moderators can change the portion of neutrons that will go on to cause more fission.[5] Nuclear reactors generally have automatic and manual systems to shut the fission reaction down if monitoring or instrumentation detects unsafe conditions.[6]

Heat generation
The reactor core generates heat in a number of ways:

The kinetic energy of fission products is converted to thermal energy when these nuclei collide with nearby atoms.
The reactor absorbs some of the gamma rays produced during fission and converts their energy into heat.
Heat is produced by the radioactive decay of fission products and materials that have been activated by neutron absorption. This decay heat source will remain for some time even after the reactor is shut down.
A kilogram of uranium-235 (U-235) converted via nuclear processes releases approximately three million times more energy than a kilogram of coal burned conventionally (7.2 × 1013 joules per kilogram of uranium-235 versus 2.4 × 107 joules per kilogram of coal).[7][8][original research?]

The fission of one kilogram of uranium-235 releases about 19 billion kilocalories, so the energy released by 1 kg of uranium-235 corresponds to that released by burning 2.7 million kg of coal.

Cooling
A nuclear reactor coolant – usually water but sometimes a gas or a liquid metal (like liquid sodium or lead) or molten salt – is circulated past the reactor core to absorb the heat that it generates. The heat is carried away from the reactor and is then used to generate steam. Most reactor systems employ a cooling system that is physically separated from the water that will be boiled to produce pressurized steam for the turbines, like the pressurized water reactor. However, in some reactors the water for the steam turbines is boiled directly by the reactor core; for example the boiling water reactor.[9]

Reactivity control
Main articles: Nuclear reactor physics, Passive nuclear safety, Delayed neutron, Iodine pit, SCRAM, and Decay heat
The rate of fission reactions within a reactor core can be adjusted by controlling the quantity of neutrons that are able to induce further fission events. Nuclear reactors typically employ several methods of neutron control to adjust the reactor's power output. Some of these methods arise naturally from the physics of radioactive decay and are simply accounted for during the reactor's operation, while others are mechanisms engineered into the reactor design for a distinct purpose.

The fastest method for adjusting levels of fission-inducing neutrons in a reactor is via movement of the control rods. Control rods are made of neutron poisons and therefore absorb neutrons. When a control rod is inserted deeper into the reactor, it absorbs more neutrons than the material it displaces – often the moderator. This action results in fewer neutrons available to cause fission and reduces the reactor's power output. Conversely, extracting the control rod will result in an increase in the rate of fission events and an increase in power.

The physics of radioactive decay also affects neutron populations in a reactor. One such process is delayed neutron emission by a number of neutron-rich fission isotopes. These delayed neutrons account for about 0.65% of the total neutrons produced in fission, with the remainder (termed "prompt neutrons") released immediately upon fission. The fission products which produce delayed neutrons have half-lives for their decay by neutron emission that range from milliseconds to as long as several minutes, and so considerable time is required to determine exactly when a reactor reaches the critical point. Keeping the reactor in the zone of chain reactivity where delayed neutrons are necessary to achieve a critical mass state allows mechanical devices or human operators to control a chain reaction in "real time"; otherwise the time between achievement of criticality and nuclear meltdown as a result of an exponential power surge from the normal nuclear chain reaction, would be too short to allow for intervention. This last stage, where delayed neutrons are no longer required to maintain criticality, is known as the prompt critical point. There is a scale for describing criticality in numerical form, in which bare criticality is known as zero dollars and the prompt critical point is one dollar, and other points in the process interpolated in cents.

In some reactors, the coolant also acts as a neutron moderator. A moderator increases the power of the reactor by causing the fast neutrons that are released from fission to lose energy and become thermal neutrons. Thermal neutrons are more likely than fast neutrons to cause fission. If the coolant is a moderator, then temperature changes can affect the density of the coolant/moderator and therefore change power output. A higher temperature coolant would be less dense, and therefore a less effective moderator.

In other reactors, the coolant acts as a poison by absorbing neutrons in the same way that the control rods do. In these reactors, power output can be increased by heating the coolant, which makes it a less dense poison. Nuclear reactors generally have automatic and manual systems to scram the reactor in an emergency shut down. These systems insert large amounts of poison (often boron in the form of boric acid) into the reactor to shut the fission reaction down if unsafe conditions are detected or anticipated.[10]

Most types of reactors are sensitive to a process variously known as xenon poisoning, or the iodine pit. The common fission product Xenon-135 produced in the fission process acts as a neutron poison that absorbs neutrons and therefore tends to shut the reactor down. Xenon-135 accumulation can be controlled by keeping power levels high enough to destroy it by neutron absorption as fast as it is produced. Fission also produces iodine-135, which in turn decays (with a half-life of 6.57 hours) to new xenon-135. When the reactor is shut down, iodine-135 continues to decay to xenon-135, making restarting the reactor more difficult for a day or two, as the xenon-135 decays into cesium-135, which is not nearly as poisonous as xenon-135, with a half-life of 9.2 hours. This temporary state is the "iodine pit." If the reactor has sufficient extra reactivity capacity, it can be restarted. As the extra xenon-135 is transmuted to xenon-136, which is much less a neutron poison, within a few hours the reactor experiences a "xenon burnoff (power) transient". Control rods must be further inserted to replace the neutron absorption of the lost xenon-135. Failure to properly follow such a procedure was a key step in the Chernobyl disaster.[11]

Reactors used in nuclear marine propulsion (especially nuclear submarines) often cannot be run at continuous power around the clock in the same way that land-based power reactors are normally run, and in addition often need to have a very long core life without refueling. For this reason many designs use highly enriched uranium but incorporate burnable neutron poison in the fuel rods.[12] This allows the reactor to be constructed with an excess of fissionable material, which is nevertheless made relatively safe early in the reactor's fuel burn cycle by the presence of the neutron-absorbing material which is later replaced by normally produced long-lived neutron poisons (far longer-lived than xenon-135) which gradually accumulate over the fuel load's operating life.

Electrical power generation
The energy released in the fission process generates heat, some of which can be converted into usable energy. A common method of harnessing this thermal energy is to use it to boil water to produce pressurized steam which will then drive a steam turbine that turns an alternator and generates electricity.[10]

Life-times
Nuclear power plants are typically designed for average life-times between 30 and 40 years. Some believe, nuclear power plants can operate for as long as 80 years or longer with proper maintenance and management. However, some vital parts, notably the reactor vessel and the concrete structures, cannot be replaced when getting cracks and fissures due to neutron embrittlement and wear, thus limiting the life of the plant.[13] At the end of their planned life span, plants may get an extension of the operating license for some 20 years and in the US even a "subsequent license renewal" (SLR) for an additional 20 years.[14][15]

Even when a license is extended, it does not guarantee the reactor will continue to operate, particularly in the face of safety concerns or incident.[16] Many reactors are closed long before their license or design life expired and are decommissioned. The costs for replacements or improvements required for continued safe operation may be so high that they are not cost-effective. Or they may be shutdown due to technical failure.[17] Other ones have been shutdown because the area was contaminated, like Fukushima, Three Mile Island, Sellafield, Chernobyl.[18] The British branch of the French concern EDF Energy, for example, extended the operating lives of its Advanced Gas-cooled Reactors with only between 3 and 10 years.[19] All seven AGR plants are expected to be shutdown in 2022 and in decommissioning by 2028.[20] Hinkley Point B was extended from 40 to 46 years, and closed. The same happened with Hunterston B, also after 46 years.

An increasing number of reactors is reaching or crossing their design lifetimes of 30 or 40 years. In 2014, Greenpeace warned that the lifetime extension of ageing nuclear power plants amounts to entering a new era of risk. It estimated the current European nuclear liability coverage in average to be too low by a factor of between 100 and 1,000 to cover the likely costs, while at the same time, the likelihood of a serious accident happening in Europe continues to increase as the reactor fleet grows older.[21]

Early reactors
See also: Nuclear fission § History

The Chicago Pile, the first artificial nuclear reactor, built in secrecy at the University of Chicago in 1942 during World War II as part of the US's Manhattan project

Lise Meitner and Otto Hahn in their laboratory

Some of the Chicago Pile Team, including Enrico Fermi and Leó Szilárd
The neutron was discovered in 1932 by British physicist James Chadwick. The concept of a nuclear chain reaction brought about by nuclear reactions mediated by neutrons was first realized shortly thereafter, by Hungarian scientist Leó Szilárd, in 1933. He filed a patent for his idea of a simple reactor the following year while working at the Admiralty in London.[22] However, Szilárd's idea did not incorporate the idea of nuclear fission as a neutron source, since that process was not yet discovered. Szilárd's ideas for nuclear reactors using neutron-mediated nuclear chain reactions in light elements proved unworkable.

Inspiration for a new type of reactor using uranium came from the discovery by Otto Hahn, Lise Meitner, Fritz Strassmann in 1938 that bombardment of uranium with neutrons (provided by an alpha-on-beryllium fusion reaction, a "neutron howitzer") produced a barium residue, which they reasoned was created by the fissioning of the uranium nuclei. In their second publication on nuclear fission in February of 1939, Hahn and Strassmann predicted the existence and liberation of additional neutrons during the fission process, opening up the possibility of a nuclear chain reaction. Subsequent studies in early 1939 (one of them by Szilárd and Fermi) revealed that several neutrons were indeed released during the fissioning, making available the opportunity for the nuclear chain reaction that Szilárd had envisioned six years previously.

On 2 August 1939, Albert Einstein signed a letter to President Franklin D. Roosevelt (written by Szilárd) suggesting that the discovery of uranium's fission could lead to the development of "extremely powerful bombs of a new type", giving impetus to the study of reactors and fission. Szilárd and Einstein knew each other well and had worked together years previously, but Einstein had never thought about this possibility for nuclear energy until Szilard reported it to him, at the beginning of his quest to produce the Einstein-Szilárd letter to alert the U.S. government.

Shortly after, Hitler's Germany invaded Poland in 1939, starting World War II in Europe. The U.S. was not yet officially at war, but in October, when the Einstein-Szilárd letter was delivered to him, Roosevelt commented that the purpose of doing the research was to make sure "the Nazis don't blow us up." The U.S. nuclear project followed, although with some delay as there remained skepticism (some of it from Fermi) and also little action from the small number of officials in the government who were initially charged with moving the project forward.

The following year, the U.S. Government received the Frisch–Peierls memorandum from the UK, which stated that the amount of uranium needed for a chain reaction was far lower than had previously been thought. The memorandum was a product of the MAUD Committee, which was working on the UK atomic bomb project, known as Tube Alloys, later to be subsumed within the Manhattan Project.

Eventually, the first artificial nuclear reactor, Chicago Pile-1, was constructed at the University of Chicago, by a team led by Italian physicist Enrico Fermi, in late 1942. By this time, the program had been pressured for a year by U.S. entry into the war. The Chicago Pile achieved criticality on 2 December 1942[23] at 3:25 PM. The reactor support structure was made of wood, which supported a pile (hence the name) of graphite blocks, embedded in which was natural uranium oxide 'pseudospheres' or 'briquettes'.

Soon after the Chicago Pile, the Metallurgical Laboratory developed a number of nuclear reactors for the Manhattan Project starting in 1943. The primary purpose for the largest reactors (located at the Hanford Site in Washington), was the mass production of plutonium for nuclear weapons. Fermi and Szilard applied for a patent on reactors on 19 December 1944. Its issuance was delayed for 10 years because of wartime secrecy.[24]

"World's first nuclear power plant" is the claim made by signs at the site of the EBR-I, which is now a museum near Arco, Idaho. Originally called "Chicago Pile-4", it was carried out under the direction of Walter Zinn for Argonne National Laboratory.[25] This experimental LMFBR operated by the U.S. Atomic Energy Commission produced 0.8 kW in a test on 20 December 1951[26] and 100 kW (electrical) the following day,[27] having a design output of 200 kW (electrical).

Besides the military uses of nuclear reactors, there were political reasons to pursue civilian use of atomic energy. U.S. President Dwight Eisenhower made his famous Atoms for Peace speech to the UN General Assembly on 8 December 1953. This diplomacy led to the dissemination of reactor technology to U.S. institutions and worldwide.[28]

The first nuclear power plant built for civil purposes was the AM-1 Obninsk Nuclear Power Plant, launched on 27 June 1954 in the Soviet Union. It produced around 5 MW (electrical). It was built after the F-1 (nuclear reactor) which was the first reactor to go critical in Europe, and was also built by the Soviet Union.

After World War II, the U.S. military sought other uses for nuclear reactor technology. Research by the Army led to the power stations for Camp Century, Greenland and McMurdo Station, Antarctica Army Nuclear Power Program. The Air Force Nuclear Bomber project resulted in the Molten-Salt Reactor Experiment. The U.S. Navy succeeded when they steamed the USS Nautilus (SSN-571) on nuclear power 17 January 1955.

The first commercial nuclear power station, Calder Hall in Sellafield, England was opened in 1956 with an initial capacity of 50 MW (later 200 MW).[29][30]

The first portable nuclear reactor "Alco PM-2A" was used to generate electrical power (2 MW) for Camp Century from 1960 to 1963.[31]


Primary coolant system showing reactor pressure vessel (red), steam generators (purple), pressurizer (blue), and pumps (green) in the three coolant loop Hualong One pressurized water reactor design
Reactor types

  PWR: 277 (63.2%)
  BWR: 80 (18.3%)
  GCR: 15 (3.4%)
  PHWR: 49 (11.2%)
  LWGR: 15 (3.4%)
  FBR: 2 (0.5%)
Number of reactors by type (end 2014)[32]

  PWR: 257.2 (68.3%)
  BWR: 75.5 (20.1%)
  GCR: 8.2 (2.2%)
  PHWR: 24.6 (6.5%)
  LWGR: 10.2 (2.7%)
  FBR: 0.6 (0.2%)
Net power capacity (GWe) by type (end 2014)[32]

NC State's PULSTAR Reactor is a 1 MW pool-type research reactor with 4% enriched, pin-type fuel consisting of UO2 pellets in zircaloy cladding.
Classifications
By type of nuclear reaction
All commercial power reactors are based on nuclear fission. They generally use uranium and its product plutonium as nuclear fuel, though a thorium fuel cycle is also possible. Fission reactors can be divided roughly into two classes, depending on the energy of the neutrons that sustain the fission chain reaction:

Thermal-neutron reactors use slowed or thermal neutrons to keep up the fission of their fuel. Almost all current reactors are of this type. These contain neutron moderator materials that slow neutrons until their neutron temperature is thermalized, that is, until their kinetic energy approaches the average kinetic energy of the surrounding particles. Thermal neutrons have a far higher cross section (probability) of fissioning the fissile nuclei uranium-235, plutonium-239, and plutonium-241, and a relatively lower probability of neutron capture by uranium-238 (U-238) compared to the faster neutrons that originally result from fission, allowing use of low-enriched uranium or even natural uranium fuel. The moderator is often also the coolant, usually water under high pressure to increase the boiling point. These are surrounded by a reactor vessel, instrumentation to monitor and control the reactor, radiation shielding, and a containment building.
Fast-neutron reactors use fast neutrons to cause fission in their fuel. They do not have a neutron moderator, and use less-moderating coolants. Maintaining a chain reaction requires the fuel to be more highly enriched in fissile material (about 20% or more) due to the relatively lower probability of fission versus capture by U-238. Fast reactors have the potential to produce less transuranic waste because all actinides are fissionable with fast neutrons,[33] but they are more difficult to build and more expensive to operate. Overall, fast reactors are less common than thermal reactors in most applications. Some early power stations were fast reactors, as are some Russian naval propulsion units. Construction of prototypes is continuing (see fast breeder or generation IV reactors).
In principle, fusion power could be produced by nuclear fusion of elements such as the deuterium isotope of hydrogen. While an ongoing rich research topic since at least the 1940s, no self-sustaining fusion reactor for any purpose has ever been built.

By moderator material
Used by thermal reactors:

Graphite-moderated reactors
Water moderated reactors
Heavy-water reactors (Used in Canada,[34] India, Argentina, China, Pakistan, Romania and South Korea).[35]
Light-water-moderated reactors (LWRs). Light-water reactors (the most common type of thermal reactor) use ordinary water to moderate and cool the reactors.[34] Because the light hydrogen isotope is a slight neutron poison these reactors need artificially enriched fuels. When at operating temperature, if the temperature of the water increases, its density drops, and fewer neutrons passing through it are slowed enough to trigger further reactions. That negative feedback stabilizes the reaction rate. Graphite and heavy-water reactors tend to be more thoroughly thermalized than light water reactors. Due to the extra thermalization, and the absence of the light hydrogen poisoning effects these types can use natural uranium/unenriched fuel.
Light-element-moderated reactors.
Molten-salt reactors (MSRs) are moderated by light elements such as lithium or beryllium, which are constituents of the coolant/fuel matrix salts "LiF" and "BeF2", "LiCl" and "BeCl2" and other light element containing salts can all cause a moderating effect.
Liquid metal cooled reactors, such as those whose coolant is a mixture of lead and bismuth, may use BeO as a moderator.
Organically moderated reactors (OMR) use biphenyl and terphenyl as moderator and coolant.
By coolant

Treatment of the interior part of a VVER-1000 reactor frame at Atommash

In thermal nuclear reactors (LWRs in specific), the coolant acts as a moderator that must slow down the neutrons before they can be efficiently absorbed by the fuel.
Water cooled reactor. These constitute the great majority of operational nuclear reactors: as of 2014, 93% of the world's nuclear reactors are water cooled, providing about 95% of the world's total nuclear generation capacity.[32]
Pressurized water reactor (PWR) Pressurized water reactors constitute the large majority of all Western nuclear power plants.
A primary characteristic of PWRs is a pressurizer, a specialized pressure vessel. Most commercial PWRs and naval reactors use pressurizers. During normal operation, a pressurizer is partially filled with water, and a steam bubble is maintained above it by heating the water with submerged heaters. During normal operation, the pressurizer is connected to the primary reactor pressure vessel (RPV) and the pressurizer "bubble" provides an expansion space for changes in water volume in the reactor. This arrangement also provides a means of pressure control for the reactor by increasing or decreasing the steam pressure in the pressurizer using the pressurizer heaters.
Pressurized heavy water reactors are a subset of pressurized water reactors, sharing the use of a pressurized, isolated heat transport loop, but using heavy water as coolant and moderator for the greater neutron economies it offers.
Boiling water reactor (BWR)
BWRs are characterized by boiling water around the fuel rods in the lower portion of a primary reactor pressure vessel. A boiling water reactor uses 235U, enriched as uranium dioxide, as its fuel. The fuel is assembled into rods housed in a steel vessel that is submerged in water. The nuclear fission causes the water to boil, generating steam. This steam flows through pipes into turbines. The turbines are driven by the steam, and this process generates electricity.[36] During normal operation, pressure is controlled by the amount of steam flowing from the reactor pressure vessel to the turbine.
Supercritical water reactor (SCWR)
SCWRs are a Generation IV reactor concept where the reactor is operated at supercritical pressures and water is heated to a supercritical fluid, which never undergoes a transition to steam yet behaves like saturated steam, to power a steam generator.
Reduced moderation water reactor [RMWR] which use more highly enriched fuel with the fuel elements set closer together to allow a faster neutron spectrum sometimes called an Epithermal neutron Spectrum.
Pool-type reactor can refer to unpressurized water cooled open pool reactors,[37] but not to be confused with pool type LMFBRs which are sodium cooled
Some reactors have been cooled by heavy water which also served as a moderator. Examples include:
Early CANDU reactors (later ones use heavy water moderator but light water coolant)
DIDO class research reactors
Liquid metal cooled reactor. Since water is a moderator, it cannot be used as a coolant in a fast reactor. Liquid metal coolants have included sodium, NaK, lead, lead-bismuth eutectic, and in early reactors, mercury.
Sodium-cooled fast reactor
Lead-cooled fast reactor
Gas cooled reactors are cooled by a circulating gas. In commercial nuclear power plants carbon dioxide has usually been used, for example in current British AGR nuclear power plants and formerly in a number of first generation British, French, Italian, & Japanese plants. Nitrogen[38] and helium have also been used, helium being considered particularly suitable for high temperature designs. Utilization of the heat varies, depending on the reactor. Commercial nuclear power plants run the gas through a heat exchanger to make steam for a steam turbine. Some experimental designs run hot enough that the gas can directly power a gas turbine.
Molten-salt reactors (MSRs) are cooled by circulating a molten salt, typically a eutectic mixture of fluoride salts, such as FLiBe. In a typical MSR, the coolant is also used as a matrix in which the fissile material is dissolved. Other eutectic salt combinations used include "ZrF4" with "NaF" and "LiCl" with "BeCl2".
Organic nuclear reactors use organic fluids such as biphenyl and terphenyl as coolant rather than water.
By generation
Generation I reactor (early prototypes such as Shippingport Atomic Power Station, research reactors, non-commercial power producing reactors)
Generation II reactor (most current nuclear power plants, 1965–1996)
Generation III reactor (evolutionary improvements of existing designs, 1996–2016)
Generation III+ reactor (evolutionary development of Gen III reactors, offering improvements in safety over Gen III reactor designs, 2017–2021)[39]
Generation IV reactor (technologies still under development; unknown start date, see below)[40]
Generation V reactor (designs which are theoretically possible, but which are not being actively considered or researched at present).
In 2003, the French Commissariat à l'Énergie Atomique (CEA) was the first to refer to "Gen II" types in Nucleonics Week.[41]

The first mention of "Gen III" was in 2000, in conjunction with the launch of the Generation IV International Forum (GIF) plans.

"Gen IV" was named in 2000, by the United States Department of Energy (DOE), for developing new plant types.[42]

By phase of fuel
Solid fueled
Fluid fueled
Aqueous homogeneous reactor
Molten-salt reactor
Gas fueled (theoretical)
By shape of the core
Cubical
Cylindrical
Octagonal
Spherical
Slab
Annulus
By use
Electricity
Nuclear power plants including small modular reactors
Propulsion, see nuclear propulsion
Nuclear marine propulsion
Various proposed forms of rocket propulsion
Other uses of heat
Desalination
Heat for domestic and industrial heating
Hydrogen production for use in a hydrogen economy
Production reactors for transmutation of elements
Breeder reactors are capable of producing more fissile material than they consume during the fission chain reaction (by converting fertile U-238 to Pu-239, or Th-232 to U-233). Thus, a uranium breeder reactor, once running, can be refueled with natural or even depleted uranium, and a thorium breeder reactor can be refueled with thorium; however, an initial stock of fissile material is required.[43]
Creating various radioactive isotopes, such as americium for use in smoke detectors, and cobalt-60, molybdenum-99 and others, used for imaging and medical treatment.
Production of materials for nuclear weapons such as weapons-grade plutonium
Providing a source of neutron radiation (for example with the pulsed Godiva device) and positron radiation[clarification needed] (e.g. neutron activation analysis and potassium-argon dating[clarification needed])
Research reactor: Typically reactors used for research and training, materials testing, or the production of radioisotopes for medicine and industry. These are much smaller than power reactors or those propelling ships, and many are on university campuses. There are about 280 such reactors operating, in 56 countries. Some operate with high-enriched uranium fuel, and international efforts are underway to substitute low-enriched fuel.[44]
Current technologies

This section does not cite any sources. Please help improve this section by adding citations to reliable sources. Unsourced material may be challenged and removed. (June 2015) (Learn how and when to remove this template message)

Diablo Canyon – a PWR
Pressurized water reactors (PWR) [moderator: high-pressure water; coolant: high-pressure water]
These reactors use a pressure vessel to contain the nuclear fuel, control rods, moderator, and coolant. The hot radioactive water that leaves the pressure vessel is looped through a steam generator, which in turn heats a secondary (nonradioactive) loop of water to steam that can run turbines. They represent the majority (around 80%) of current reactors. This is a thermal neutron reactor design, the newest of which are the Russian VVER-1200, Japanese Advanced Pressurized Water Reactor, American AP1000, Chinese Hualong Pressurized Reactor and the Franco-German European Pressurized Reactor. All the United States Naval reactors are of this type.
Boiling water reactors (BWR) [moderator: low-pressure water; coolant: low-pressure water]
A BWR is like a PWR without the steam generator. The lower pressure of its cooling water allows it to boil inside the pressure vessel, producing the steam that runs the turbines. Unlike a PWR, there is no primary and secondary loop. The thermal efficiency of these reactors can be higher, and they can be simpler, and even potentially more stable and safe. This is a thermal-neutron reactor design, the newest of which are the Advanced Boiling Water Reactor and the Economic Simplified Boiling Water Reactor.

The CANDU Qinshan Nuclear Power Plant
Pressurized Heavy Water Reactor (PHWR) [moderator: high-pressure heavy water; coolant: high-pressure heavy water]
A Canadian design (known as CANDU), very similar to PWRs but using heavy water. While heavy water is significantly more expensive than ordinary water, it has greater neutron economy (creates a higher number of thermal neutrons), allowing the reactor to operate without fuel enrichment facilities. Instead of using a single large pressure vessel as in a PWR, the fuel is contained in hundreds of pressure tubes. These reactors are fueled with natural uranium and are thermal-neutron reactor designs. PHWRs can be refueled while at full power, (online refueling) which makes them very efficient in their use of uranium (it allows for precise flux control in the core). CANDU PHWRs have been built in Canada, Argentina, China, India, Pakistan, Romania, and South Korea. India also operates a number of PHWRs, often termed 'CANDU derivatives', built after the Government of Canada halted nuclear dealings with India following the 1974 Smiling Buddha nuclear weapon test.

The Ignalina Nuclear Power Plant – a RBMK type (closed 2009)
Reaktor Bolshoy Moschnosti Kanalniy (High Power Channel Reactor) (RBMK) [moderator: graphite; coolant: high-pressure water]
A Soviet design, RBMKs are in some respects similar to CANDU in that they are refuelable during power operation and employ a pressure tube design instead of a PWR-style pressure vessel. However, unlike CANDU they are very unstable and large, making containment buildings for them expensive. A series of critical safety flaws have also been identified with the RBMK design, though some of these were corrected following the Chernobyl disaster. Their main attraction is their use of light water and unenriched uranium. As of 2022, 8 remain open, mostly due to safety improvements and help from international safety agencies such as the DOE. Despite these safety improvements, RBMK reactors are still considered one of the most dangerous reactor designs in use. RBMK reactors were deployed only in the former Soviet Union.

The Magnox Sizewell A nuclear power station

The Torness nuclear power station – an AGR
Gas-cooled reactor (GCR) and advanced gas-cooled reactor (AGR) [moderator: graphite; coolant: carbon dioxide]
These designs have a high thermal efficiency compared with PWRs due to higher operating temperatures. There are a number of operating reactors of this design, mostly in the United Kingdom, where the concept was developed. Older designs (i.e. Magnox stations) are either shut down or will be in the near future. However, the AGRs have an anticipated life of a further 10 to 20 years. This is a thermal-neutron reactor design. Decommissioning costs can be high due to large volume of reactor core.
Liquid metal fast-breeder reactor (LMFBR) [moderator: none; coolant: liquid metal]

Scaled-down model of TOPAZ nuclear reactor
This totally unmoderated reactor design produces more fuel than it consumes. They are said to "breed" fuel, because they produce fissionable fuel during operation because of neutron capture. These reactors can function much like a PWR in terms of efficiency, and do not require much high-pressure containment, as the liquid metal does not need to be kept at high pressure, even at very high temperatures. These reactors are fast neutron, not thermal neutron designs. These reactors come in two types:

The Superphénix, closed in 1998, was one of the few FBRs.
Lead-cooled
Using lead as the liquid metal provides excellent radiation shielding, and allows for operation at very high temperatures. Also, lead is (mostly) transparent to neutrons, so fewer neutrons are lost in the coolant, and the coolant does not become radioactive. Unlike sodium, lead is mostly inert, so there is less risk of explosion or accident, but such large quantities of lead may be problematic from toxicology and disposal points of view. Often a reactor of this type would use a lead-bismuth eutectic mixture. In this case, the bismuth would present some minor radiation problems, as it is not quite as transparent to neutrons, and can be transmuted to a radioactive isotope more readily than lead. The Russian Alfa class submarine uses a lead-bismuth-cooled fast reactor as its main power plant.
Sodium-cooled
Most LMFBRs are of this type. The TOPAZ, BN-350 and BN-600 in USSR; Superphénix in France; and Fermi-I in the United States were reactors of this type. The sodium is relatively easy to obtain and work with, and it also manages to actually prevent corrosion on the various reactor parts immersed in it. However, sodium explodes violently when exposed to water, so care must be taken, but such explosions would not be more violent than (for example) a leak of superheated fluid from a pressurized-water reactor. The Monju reactor in Japan suffered a sodium leak in 1995 and could not be restarted until May 2010. The EBR-I, the first reactor to have a core meltdown, in 1955, was also a sodium-cooled reactor.
Pebble-bed reactors (PBR) [moderator: graphite; coolant: helium]
These use fuel molded into ceramic balls, and then circulate gas through the balls. The result is an efficient, low-maintenance, very safe reactor with inexpensive, standardized fuel. The prototypes were the AVR and the THTR-300 in Germany, which produced up to 308MW of electricity between 1985 and 1989 until it was shut down after experiencing a series of incidents and technical difficulties. The HTR-10 is operating in China, where the HTR-PM is being developed. The HTR-PM is expected to be the first generation IV reactor to enter operation.[45]
Molten-salt reactors (MSR) [moderator: graphite, or none for fast spectrum MSRs; coolant: molten salt mixture]
These dissolve the fuels in fluoride or chloride salts, or use such salts for coolant. MSRs potentially have many safety features, including the absence of high pressures or highly flammable components in the core. They were initially designed for aircraft propulsion due to their high efficiency and high power density. One prototype, the Molten-Salt Reactor Experiment, was built to confirm the feasibility of the Liquid fluoride thorium reactor, a thermal spectrum reactor which would breed fissile uranium-233 fuel from thorium.
Aqueous homogeneous reactor (AHR) [moderator: high-pressure light or heavy water; coolant: high-pressure light or heavy water]
These reactors use as fuel soluble nuclear salts (usually uranium sulfate or uranium nitrate) dissolved in water and mixed with the coolant and the moderator. As of April 2006, only five AHRs were in operation.[46]
Future and developing technologies
Advanced reactors
More than a dozen advanced reactor designs are in various stages of development.[47] Some are evolutionary from the PWR, BWR and PHWR designs above, some are more radical departures. The former include the advanced boiling water reactor (ABWR), two of which are now operating with others under construction, and the planned passively safe Economic Simplified Boiling Water Reactor (ESBWR) and AP1000 units (see Nuclear Power 2010 Program).

The integral fast reactor (IFR) was built, tested and evaluated during the 1980s and then retired under the Clinton administration in the 1990s due to nuclear non-proliferation policies of the administration. Recycling spent fuel is the core of its design and it therefore produces only a fraction of the waste of current reactors.[48]
The pebble-bed reactor, a high-temperature gas-cooled reactor (HTGCR), is designed so high temperatures reduce power output by Doppler broadening of the fuel's neutron cross-section. It uses ceramic fuels so its safe operating temperatures exceed the power-reduction temperature range. Most designs are cooled by inert helium. Helium is not subject to steam explosions, resists neutron absorption leading to radioactivity, and does not dissolve contaminants that can become radioactive. Typical designs have more layers (up to 7) of passive containment than light water reactors (usually 3). A unique feature that may aid safety is that the fuel balls actually form the core's mechanism, and are replaced one by one as they age. The design of the fuel makes fuel reprocessing expensive.
The small, sealed, transportable, autonomous reactor (SSTAR) is being primarily researched and developed in the US, intended as a fast breeder reactor that is passively safe and could be remotely shut down in case the suspicion arises that it is being tampered with.
The Clean and Environmentally Safe Advanced Reactor (CAESAR) is a nuclear reactor concept that uses steam as a moderator – this design is still in development.
The reduced moderation water reactor builds upon the Advanced boiling water reactor ABWR) that is presently in use, it is not a complete fast reactor instead using mostly epithermal neutrons, which are between thermal and fast neutrons in speed.
The hydrogen-moderated self-regulating nuclear power module (HPM) is a reactor design emanating from the Los Alamos National Laboratory that uses uranium hydride as fuel.
Subcritical reactors are designed to be safer and more stable, but pose a number of engineering and economic difficulties. One example is the energy amplifier.
Thorium-based reactors — It is possible to convert Thorium-232 into U-233 in reactors specially designed for the purpose. In this way, thorium, which is four times more abundant than uranium, can be used to breed U-233 nuclear fuel.[49] U-233 is also believed to have favourable nuclear properties as compared to traditionally used U-235, including better neutron economy and lower production of long lived transuranic waste.
Advanced heavy-water reactor (AHWR) — A proposed heavy water moderated nuclear power reactor that will be the next generation design of the PHWR type. Under development in the Bhabha Atomic Research Centre (BARC), India.
KAMINI – A unique reactor using Uranium-233 isotope for fuel. Built in India by BARC and Indira Gandhi Center for Atomic Research (IGCAR).
India is also planning to build fast breeder reactors using the thorium – Uranium-233 fuel cycle. The FBTR (Fast Breeder Test Reactor) in operation at Kalpakkam (India) uses Plutonium as a fuel and liquid sodium as a coolant.
China, which has control of the Cerro Impacto deposit, has a reactor and hopes to replace coal energy with nuclear energy.[50]
Rolls-Royce aims to sell nuclear reactors for the production of synfuel for aircraft.[51]

Generation IV reactors
Generation IV reactors are a set of theoretical nuclear reactor designs. These are generally not expected to be available for commercial use before 2040–2050,[52] although the World Nuclear Association suggested that some might enter commercial operation before 2030.[40] Current reactors in operation around the world are generally considered second- or third-generation systems, with the first-generation systems having been retired some time ago. Research into these reactor types was officially started by the Generation IV International Forum (GIF) based on eight technology goals. The primary goals being to improve nuclear safety, improve proliferation resistance, minimize waste and natural resource utilization, and to decrease the cost to build and run such plants.[53]

Gas-cooled fast reactor
Lead-cooled fast reactor
Molten-salt reactor
Sodium-cooled fast reactor
Supercritical water reactor
Very-high-temperature reactor
Generation V+ reactors
Generation V reactors are designs which are theoretically possible, but which are not being actively considered or researched at present. Though some generation V reactors could potentially be built with current or near term technology, they trigger little interest for reasons of economics, practicality, or safety.

Liquid-core reactor. A closed loop liquid-core nuclear reactor, where the fissile material is molten uranium or uranium solution cooled by a working gas pumped in through holes in the base of the containment vessel.
Gas-core reactor. A closed loop version of the nuclear lightbulb rocket, where the fissile material is gaseous uranium hexafluoride contained in a fused silica vessel. A working gas (such as hydrogen) would flow around this vessel and absorb the UV light produced by the reaction. This reactor design could also function as a rocket engine, as featured in Harry Harrison's 1976 science-fiction novel Skyfall. In theory, using UF6 as a working fuel directly (rather than as a stage to one, as is done now) would mean lower processing costs, and very small reactors. In practice, running a reactor at such high power densities would probably produce unmanageable neutron flux, weakening most reactor materials, and therefore as the flux would be similar to that expected in fusion reactors, it would require similar materials to those selected by the International Fusion Materials Irradiation Facility.
Gas core EM reactor. As in the gas core reactor, but with photovoltaic arrays converting the UV light directly to electricity.[54] This approach is similar to the experimentally proved photoelectric effect that would convert the X-rays generated from aneutronic fusion into electricity, by passing the high energy photons through an array of conducting foils to transfer some of their energy to electrons, the energy of the photon is captured electrostatically, similar to a capacitor. Since X-rays can go through far greater material thickness than electrons, many hundreds or thousands of layers are needed to absorb the X-rays.[55]
Fission fragment reactor. A fission fragment reactor is a nuclear reactor that generates electricity by decelerating an ion beam of fission byproducts instead of using nuclear reactions to generate heat. By doing so, it bypasses the Carnot cycle and can achieve efficiencies of up to 90% instead of 40–45% attainable by efficient turbine-driven thermal reactors. The fission fragment ion beam would be passed through a magnetohydrodynamic generator to produce electricity.
Hybrid nuclear fusion. Would use the neutrons emitted by fusion to fission a blanket of fertile material, like U-238 or Th-232 and transmute other reactor's spent nuclear fuel/nuclear waste into relatively more benign isotopes.
Fusion reactors
Main article: Fusion power
Controlled nuclear fusion could in principle be used in fusion power plants to produce power without the complexities of handling actinides, but significant scientific and technical obstacles remain. Despite research having started in the 1950s, no commercial fusion reactor is expected before 2050. The ITER project is currently leading the effort to harness fusion power.

Nuclear fuel cycle
Main article: Nuclear fuel cycle
Thermal reactors generally depend on refined and enriched uranium. Some nuclear reactors can operate with a mixture of plutonium and uranium (see MOX). The process by which uranium ore is mined, processed, enriched, used, possibly reprocessed and disposed of is known as the nuclear fuel cycle.

Under 1% of the uranium found in nature is the easily fissionable U-235 isotope and as a result most reactor designs require enriched fuel. Enrichment involves increasing the percentage of U-235 and is usually done by means of gaseous diffusion or gas centrifuge. The enriched result is then converted into uranium dioxide powder, which is pressed and fired into pellet form. These pellets are stacked into tubes which are then sealed and called fuel rods. Many of these fuel rods are used in each nuclear reactor.

Most BWR and PWR commercial reactors use uranium enriched to about 4% U-235, and some commercial reactors with a high neutron economy do not require the fuel to be enriched at all (that is, they can use natural uranium). According to the International Atomic Energy Agency there are at least 100 research reactors in the world fueled by highly enriched (weapons-grade/90% enrichment) uranium. Theft risk of this fuel (potentially used in the production of a nuclear weapon) has led to campaigns advocating conversion of this type of reactor to low-enrichment uranium (which poses less threat of proliferation).[56]

Fissile U-235 and non-fissile but fissionable and fertile U-238 are both used in the fission process. U-235 is fissionable by thermal (i.e. slow-moving) neutrons. A thermal neutron is one which is moving about the same speed as the atoms around it. Since all atoms vibrate proportionally to their absolute temperature, a thermal neutron has the best opportunity to fission U-235 when it is moving at this same vibrational speed. On the other hand, U-238 is more likely to capture a neutron when the neutron is moving very fast. This U-239 atom will soon decay into plutonium-239, which is another fuel. Pu-239 is a viable fuel and must be accounted for even when a highly enriched uranium fuel is used. Plutonium fissions will dominate the U-235 fissions in some reactors, especially after the initial loading of U-235 is spent. Plutonium is fissionable with both fast and thermal neutrons, which make it ideal for either nuclear reactors or nuclear bombs.

Most reactor designs in existence are thermal reactors and typically use water as a neutron moderator (moderator means that it slows down the neutron to a thermal speed) and as a coolant. But in a fast breeder reactor, some other kind of coolant is used which will not moderate or slow the neutrons down much. This enables fast neutrons to dominate, which can effectively be used to constantly replenish the fuel supply. By merely placing cheap unenriched uranium into such a core, the non-fissionable U-238 will be turned into Pu-239, "breeding" fuel.

In thorium fuel cycle thorium-232 absorbs a neutron in either a fast or thermal reactor. The thorium-233 beta decays to protactinium-233 and then to uranium-233, which in turn is used as fuel. Hence, like uranium-238, thorium-232 is a fertile material.

Fueling of nuclear reactors
The amount of energy in the reservoir of nuclear fuel is frequently expressed in terms of "full-power days," which is the number of 24-hour periods (days) a reactor is scheduled for operation at full power output for the generation of heat energy. The number of full-power days in a reactor's operating cycle (between refueling outage times) is related to the amount of fissile uranium-235 (U-235) contained in the fuel assemblies at the beginning of the cycle. A higher percentage of U-235 in the core at the beginning of a cycle will permit the reactor to be run for a greater number of full-power days.

At the end of the operating cycle, the fuel in some of the assemblies is "spent", having spent four to six years in the reactor producing power. This spent fuel is discharged and replaced with new (fresh) fuel assemblies.[citation needed] Though considered "spent," these fuel assemblies contain a large quantity of fuel.[citation needed] In practice it is economics that determines the lifetime of nuclear fuel in a reactor. Long before all possible fission has taken place, the reactor is unable to maintain 100%, full output power, and therefore, income for the utility lowers as plant output power lowers. Most nuclear plants operate at a very low profit margin due to operating overhead, mainly regulatory costs, so operating below 100% power is not economically viable for very long.[citation needed] The fraction of the reactor's fuel core replaced during refueling is typically one-third, but depends on how long the plant operates between refueling. Plants typically operate on 18 month refueling cycles, or 24 month refueling cycles. This means that one refueling, replacing only one-third of the fuel, can keep a nuclear reactor at full power for nearly two years.[citation needed] The disposition and storage of this spent fuel is one of the most challenging aspects of the operation of a commercial nuclear power plant. This nuclear waste is highly radioactive and its toxicity presents a danger for thousands of years.[36] After being discharged from the reactor, spent nuclear fuel is transferred to the on-site spent fuel pool. The spent fuel pool is a large pool of water that provides cooling and shielding of the spent nuclear fuel as well as limit radiation exposure to on-site personnel. Once the energy has decayed somewhat (approximately five years), the fuel can be transferred from the fuel pool to dry shielded casks, that can be safely stored for thousands of years. After loading into dry shielded casks, the casks are stored on-site in a specially guarded facility in impervious concrete bunkers. On-site fuel storage facilities are designed to withstand the impact of commercial airliners, with little to no damage to the spent fuel. An average on-site fuel storage facility can hold 30 years of spent fuel in a space smaller than a football field.[citation needed]

Not all reactors need to be shut down for refueling; for example, pebble bed reactors, RBMK reactors, molten-salt reactors, Magnox, AGR and CANDU reactors allow fuel to be shifted through the reactor while it is running. In a CANDU reactor, this also allows individual fuel elements to be situated within the reactor core that are best suited to the amount of U-235 in the fuel element.

The amount of energy extracted from nuclear fuel is called its burnup, which is expressed in terms of the heat energy produced per initial unit of fuel weight. Burnup is commonly expressed as megawatt days thermal per metric ton of initial heavy metal.

Nuclear safety
Main article: Nuclear safety
See also: Nuclear reactor safety system
Nuclear safety covers the actions taken to prevent nuclear and radiation accidents and incidents or to limit their consequences. The nuclear power industry has improved the safety and performance of reactors, and has proposed new, safer (but generally untested) reactor designs but there is no guarantee that the reactors will be designed, built and operated correctly.[57] Mistakes do occur and the designers of reactors at Fukushima in Japan did not anticipate that a tsunami generated by an earthquake would disable the backup systems that were supposed to stabilize the reactor after the earthquake,[58] despite multiple warnings by the NRG and the Japanese nuclear safety administration.[citation needed] According to UBS AG, the Fukushima I nuclear accidents have cast doubt on whether even an advanced economy like Japan can master nuclear safety.[59] Catastrophic scenarios involving terrorist attacks are also conceivable.[57] An interdisciplinary team from MIT has estimated that given the expected growth of nuclear power from 2005 to 2055, at least four serious nuclear accidents would be expected in that period.[60]

Nuclear accidents
See also: Lists of nuclear disasters and radioactive incidents

Three of the reactors at Fukushima I overheated, causing the coolant water to dissociate and led to the hydrogen explosions. This along with fuel meltdowns released large amounts of radioactive material into the air.[61]
Serious, though rare, nuclear and radiation accidents have occurred. These include the Windscale fire (October 1957), the SL-1 accident (1961), the Three Mile Island accident (1979), Chernobyl disaster (April 1986), and the Fukushima Daiichi nuclear disaster (March 2011).[62] Nuclear-powered submarine mishaps include the K-19 reactor accident (1961),[63] the K-27 reactor accident (1968),[64] and the K-431 reactor accident (1985).[62]

Nuclear reactors have been launched into Earth orbit at least 34 times. A number of incidents connected with the unmanned nuclear-reactor-powered Soviet RORSAT especially Kosmos 954 radar satellite which resulted in nuclear fuel reentering the Earth's atmosphere from orbit and being dispersed in northern Canada (January 1978).

Natural nuclear reactors
Main article: Natural nuclear fission reactor
Almost two billion years ago a series of self-sustaining nuclear fission "reactors" self-assembled in the area now known as Oklo in Gabon, West Africa. The conditions at that place and time allowed a natural nuclear fission to occur with circumstances that are similar to the conditions in a constructed nuclear reactor.[65] Fifteen fossil natural fission reactors have so far been found in three separate ore deposits at the Oklo uranium mine in Gabon. First discovered in 1972 by French physicist Francis Perrin, they are collectively known as the Oklo Fossil Reactors. Self-sustaining nuclear fission reactions took place in these reactors approximately 1.5 billion years ago, and ran for a few hundred thousand years, averaging 100 kW of power output during that time.[66] The concept of a natural nuclear reactor was theorized as early as 1956 by Paul Kuroda at the University of Arkansas.[67][68]

Such reactors can no longer form on Earth in its present geologic period. Radioactive decay of formerly more abundant uranium-235 over the time span of hundreds of millions of years has reduced the proportion of this naturally occurring fissile isotope to below the amount required to sustain a chain reaction with only plain water as a moderator.

The natural nuclear reactors formed when a uranium-rich mineral deposit became inundated with groundwater that acted as a neutron moderator, and a strong chain reaction took place. The water moderator would boil away as the reaction increased, slowing it back down again and preventing a meltdown. The fission reaction was sustained for hundreds of thousands of years, cycling on the order of hours to a few days.

These natural reactors are extensively studied by scientists interested in geologic radioactive waste disposal. They offer a case study of how radioactive isotopes migrate through the Earth's crust. This is a significant area of controversy as opponents of geologic waste disposal fear that isotopes from stored waste could end up in water supplies or be carried into the environment.

Emissions
Nuclear reactors produce tritium as part of normal operations, which is eventually released into the environment in trace quantities.

As an isotope of hydrogen, tritium (T) frequently binds to oxygen and forms T2O. This molecule is chemically identical to H2O and so is both colorless and odorless, however the additional neutrons in the hydrogen nuclei cause the tritium to undergo beta decay with a half-life of 12.3 years. Despite being measurable, the tritium released by nuclear power plants is minimal. The United States NRC estimates that a person drinking water for one year out of a well contaminated by what they would consider to be a significant tritiated water spill would receive a radiation dose of 0.3 millirem.[69] For comparison, this is an order of magnitude less than the 4 millirem a person receives on a round trip flight from Washington, D.C. to Los Angeles, a consequence of less atmospheric protection against highly energetic cosmic rays at high altitudes.[69]

The amounts of strontium-90 released from nuclear power plants under normal operations is so low as to be undetectable above natural background radiation. Detectable strontium-90 in ground water and the general environment can be traced to weapons testing that occurred during the mid-20th century (accounting for 99% of the Strontium-90 in the environment) and the Chernobyl accident (accounting for the remaining 1%).[70]
A nuclear weapon[a] is an explosive device that derives its destructive force from nuclear reactions, either fission (fission bomb) or a combination of fission and fusion reactions (thermonuclear bomb), producing a nuclear explosion. Both bomb types release large quantities of energy from relatively small amounts of matter.

The first test of a fission ("atomic") bomb released an amount of energy approximately equal to 20,000 tons of TNT (84 TJ).[1] The first thermonuclear ("hydrogen") bomb test released energy approximately equal to 10 million tons of TNT (42 PJ). Nuclear bombs have had yields between 10 tons TNT (the W54) and 50 megatons for the Tsar Bomba (see TNT equivalent). A thermonuclear weapon weighing as little as 600 pounds (270 kg) can release energy equal to more than 1.2 megatonnes of TNT (5.0 PJ).[2]

A nuclear device no larger than a conventional bomb can devastate an entire city by blast, fire, and radiation. Since they are weapons of mass destruction, the proliferation of nuclear weapons is a focus of international relations policy. Nuclear weapons have been deployed twice in war, by the United States against the Japanese cities of Hiroshima and Nagasaki in 1945 during World War II.

Testing and deployment
Nuclear weapons have only twice been used in warfare, both times by the United States against Japan at the end of World War II. On August 6, 1945, the United States Army Air Forces (USAAF) detonated a uranium gun-type fission bomb nicknamed "Little Boy" over the Japanese city of Hiroshima; three days later, on August 9, the USAAF[3] detonated a plutonium implosion-type fission bomb nicknamed "Fat Man" over the Japanese city of Nagasaki. These bombings caused injuries that resulted in the deaths of approximately 200,000 civilians and military personnel.[4] The ethics of these bombings and their role in Japan's surrender are to this day, still subjects of debate.

Since the atomic bombings of Hiroshima and Nagasaki, nuclear weapons have been detonated over 2,000 times for testing and demonstration. Only a few nations possess such weapons or are suspected of seeking them. The only countries known to have detonated nuclear weapons—and acknowledge possessing them—are (chronologically by date of first test) the United States, the Soviet Union (succeeded as a nuclear power by Russia), the United Kingdom, France, China, India, Pakistan, and North Korea. Israel is believed to possess nuclear weapons, though, in a policy of deliberate ambiguity, it does not acknowledge having them. Germany, Italy, Turkey, Belgium, the Netherlands, and Belarus are nuclear weapons sharing states.[5][6][b] South Africa is the only country to have independently developed and then renounced and dismantled its nuclear weapons.[7]

The Treaty on the Non-Proliferation of Nuclear Weapons aims to reduce the spread of nuclear weapons, but there are different views of its effectiveness.[8]

Types
Main article: Nuclear weapon design

The Trinity test of the Manhattan Project was the first detonation of a nuclear weapon, which led J. Robert Oppenheimer to recall verses from the Hindu scripture Bhagavad Gita: "If the radiance of a thousand suns were to burst at once into the sky, that would be like the splendor of the mighty one "... "I am become Death, the destroyer of worlds".[9]

Robert Oppenheimer, principal leader of the Manhattan Project, often referred to as the "father of the atomic bomb".
There are two basic types of nuclear weapons: those that derive the majority of their energy from nuclear fission reactions alone, and those that use fission reactions to begin nuclear fusion reactions that produce a large amount of the total energy output.[10]

Fission weapons

The two basic fission weapon designs
All existing nuclear weapons derive some of their explosive energy from nuclear fission reactions. Weapons whose explosive output is exclusively from fission reactions are commonly referred to as atomic bombs or atom bombs (abbreviated as A-bombs). This has long been noted as something of a misnomer, as their energy comes from the nucleus of the atom, just as it does with fusion weapons.

In fission weapons, a mass of fissile material (enriched uranium or plutonium) is forced into supercriticality—allowing an exponential growth of nuclear chain reactions—either by shooting one piece of sub-critical material into another (the "gun" method) or by compression of a sub-critical sphere or cylinder of fissile material using chemically fueled explosive lenses. The latter approach, the "implosion" method, is more sophisticated and more efficient (smaller, less massive, and requiring less of the expensive fissile fuel) than the former.

A major challenge in all nuclear weapon designs is to ensure that a significant fraction of the fuel is consumed before the weapon destroys itself. The amount of energy released by fission bombs can range from the equivalent of just under a ton to upwards of 500,000 tons (500 kilotons) of TNT (4.2 to 2.1×106 GJ).[11]

All fission reactions generate fission products, the remains of the split atomic nuclei. Many fission products are either highly radioactive (but short-lived) or moderately radioactive (but long-lived), and as such, they are a serious form of radioactive contamination. Fission products are the principal radioactive component of nuclear fallout. Another source of radioactivity is the burst of free neutrons produced by the weapon. When they collide with other nuclei in the surrounding material, the neutrons transmute those nuclei into other isotopes, altering their stability and making them radioactive.

The most commonly used fissile materials for nuclear weapons applications have been uranium-235 and plutonium-239. Less commonly used has been uranium-233. Neptunium-237 and some isotopes of americium may be usable for nuclear explosives as well, but it is not clear that this has ever been implemented, and their plausible use in nuclear weapons is a matter of dispute.[12]

Fusion weapons
Main article: Thermonuclear weapon

The basics of the Teller–Ulam design for a hydrogen bomb: a fission bomb uses radiation to compress and heat a separate section of fusion fuel.
The other basic type of nuclear weapon produces a large proportion of its energy in nuclear fusion reactions. Such fusion weapons are generally referred to as thermonuclear weapons or more colloquially as hydrogen bombs (abbreviated as H-bombs), as they rely on fusion reactions between isotopes of hydrogen (deuterium and tritium). All such weapons derive a significant portion of their energy from fission reactions used to "trigger" fusion reactions, and fusion reactions can themselves trigger additional fission reactions.[13]

Only six countries—the United States, Russia, the United Kingdom, China, France, and India—have conducted thermonuclear weapon tests. Whether India has detonated a "true" multi-staged thermonuclear weapon is controversial.[14] North Korea claims to have tested a fusion weapon as of January 2016, though this claim is disputed.[15] Thermonuclear weapons are considered much more difficult to successfully design and execute than primitive fission weapons. Almost all of the nuclear weapons deployed today use the thermonuclear design because it is more efficient.[16]

Thermonuclear bombs work by using the energy of a fission bomb to compress and heat fusion fuel. In the Teller-Ulam design, which accounts for all multi-megaton yield hydrogen bombs, this is accomplished by placing a fission bomb and fusion fuel (tritium, deuterium, or lithium deuteride) in proximity within a special, radiation-reflecting container. When the fission bomb is detonated, gamma rays and X-rays emitted first compress the fusion fuel, then heat it to thermonuclear temperatures. The ensuing fusion reaction creates enormous numbers of high-speed neutrons, which can then induce fission in materials not normally prone to it, such as depleted uranium. Each of these components is known as a "stage", with the fission bomb as the "primary" and the fusion capsule as the "secondary". In large, megaton-range hydrogen bombs, about half of the yield comes from the final fissioning of depleted uranium.[11]

Virtually all thermonuclear weapons deployed today use the "two-stage" design described to the right, but it is possible to add additional fusion stages—each stage igniting a larger amount of fusion fuel in the next stage. This technique can be used to construct thermonuclear weapons of arbitrarily large yield. This is in contrast to fission bombs, which are limited in their explosive power due to criticality danger (premature nuclear chain reaction caused by too-large amounts of pre-assembled fissile fuel). The largest nuclear weapon ever detonated, the Tsar Bomba of the USSR, which released an energy equivalent of over 50 megatons of TNT (210 PJ), was a three-stage weapon. Most thermonuclear weapons are considerably smaller than this, due to practical constraints from missile warhead space and weight requirements.[17] In the early 1950s the Livermore Laboratory in the United States had plans for the testing of two massive bombs, Gnomon and Sundial, 1 gigaton of TNT and 10 gigatons of TNT respectively.[18][19]


Edward Teller, often referred to as the "father of the hydrogen bomb"
Fusion reactions do not create fission products, and thus contribute far less to the creation of nuclear fallout than fission reactions, but because all thermonuclear weapons contain at least one fission stage, and many high-yield thermonuclear devices have a final fission stage, thermonuclear weapons can generate at least as much nuclear fallout as fission-only weapons. Furthermore, high yield thermonuclear explosions (most dangerously ground bursts) have the force to lift radioactive debris upwards past the tropopause into the stratosphere, where the calm non-turbulent winds permit the debris to travel great distances from the burst, eventually settling and unpredictably contaminating areas far removed from the target of the explosion.

Other types
Main articles: Boosted fission weapon, Neutron bomb, Radiological warfare, Induced gamma emission, and Antimatter weapon
There are other types of nuclear weapons as well. For example, a boosted fission weapon is a fission bomb that increases its explosive yield through a small number of fusion reactions, but it is not a fusion bomb. In the boosted bomb, the neutrons produced by the fusion reactions serve primarily to increase the efficiency of the fission bomb. There are two types of boosted fission bomb: internally boosted, in which a deuterium-tritium mixture is injected into the bomb core, and externally boosted, in which concentric shells of lithium-deuteride and depleted uranium are layered on the outside of the fission bomb core. The external method of boosting enabled the USSR to field the first partially thermonuclear weapons, but it is now obsolete because it demands a spherical bomb geometry, which was adequate during the 1950s arms race when bomber aircraft were the only available delivery vehicles.

The detonation of any nuclear weapon is accompanied by a blast of neutron radiation. Surrounding a nuclear weapon with suitable materials (such as cobalt or gold) creates a weapon known as a salted bomb. This device can produce exceptionally large quantities of long-lived radioactive contamination. It has been conjectured that such a device could serve as a "doomsday weapon" because such a large quantity of radioactivities with half-lives of decades, lifted into the stratosphere where winds would distribute it around the globe, would make all life on the planet extinct.

In connection with the Strategic Defense Initiative, research into the nuclear pumped laser was conducted under the DOD program Project Excalibur but this did not result in a working weapon. The concept involves the tapping of the energy of an exploding nuclear bomb to power a single-shot laser that is directed at a distant target.

During the Starfish Prime high-altitude nuclear test in 1962, an unexpected effect was produced which is called a nuclear electromagnetic pulse. This is an intense flash of electromagnetic energy produced by a rain of high-energy electrons which in turn are produced by a nuclear bomb's gamma rays. This flash of energy can permanently destroy or disrupt electronic equipment if insufficiently shielded. It has been proposed to use this effect to disable an enemy's military and civilian infrastructure as an adjunct to other nuclear or conventional military operations. By itself it could as well be useful to terrorists for crippling a nation's economic electronics-based infrastructure. Because the effect is most effectively produced by high altitude nuclear detonations (by military weapons delivered by air, though ground bursts also produce EMP effects over a localized area), it can produce damage to electronics over a wide, even continental, geographical area.[20]

Research has been done into the possibility of pure fusion bombs: nuclear weapons that consist of fusion reactions without requiring a fission bomb to initiate them. Such a device might provide a simpler path to thermonuclear weapons than one that required the development of fission weapons first, and pure fusion weapons would create significantly less nuclear fallout than other thermonuclear weapons because they would not disperse fission products. In 1998, the United States Department of Energy divulged that the United States had, "...made a substantial investment" in the past to develop pure fusion weapons, but that, "The U.S. does not have and is not developing a pure fusion weapon", and that, "No credible design for a pure fusion weapon resulted from the DOE investment".[21]

Nuclear isomers provide a possible pathway to fissionless fusion bombs. These are naturally occurring isotopes (178m2Hf being a prominent example) which exist in an elevated energy state. Mechanisms to release this energy as bursts of gamma radiation (as in the hafnium controversy) have been proposed as possible triggers for conventional thermonuclear reactions.

Antimatter, which consists of particles resembling ordinary matter particles in most of their properties but having opposite electric charge, has been considered as a trigger mechanism for nuclear weapons.[22][23][24] A major obstacle is the difficulty of producing antimatter in large enough quantities, and there is no evidence that it is feasible beyond the military domain.[25] However, the U.S. Air Force funded studies of the physics of antimatter in the Cold War, and began considering its possible use in weapons, not just as a trigger, but as the explosive itself.[26] A fourth generation nuclear weapon design[22] is related to, and relies upon, the same principle as antimatter-catalyzed nuclear pulse propulsion.[27]

Most variation in nuclear weapon design is for the purpose of achieving different yields for different situations, and in manipulating design elements to attempt to minimize weapon size,[11] radiation hardness or requirements for special materials, especially fissile fuel or tritium.

Tactical nuclear weapons

Soviet OTR-21 Tochka missile. Capable of firing a 100-kiloton nuclear warhead a distance of 185 km
Some nuclear weapons are designed for special purposes; most of these are for non-strategic (decisively war-winning) purposes and are referred to as tactical nuclear weapons.

The neutron bomb purportedly conceived by Sam Cohen is a thermonuclear weapon that yields a relatively small explosion but a relatively large amount of neutron radiation. Such a weapon could, according to tacticians, be used to cause massive biological casualties while leaving inanimate infrastructure mostly intact and creating minimal fallout. Because high energy neutrons are capable of penetrating dense matter, such as tank armor, neutron warheads were procured in the 1980s (though not deployed in Europe) for use as tactical payloads for US Army artillery shells (200 mm W79 and 155 mm W82) and short range missile forces. Soviet authorities announced similar intentions for neutron warhead deployment in Europe; indeed, they claimed to have originally invented the neutron bomb, but their deployment on USSR tactical nuclear forces is unverifiable.[citation needed]

A type of nuclear explosive most suitable for use by ground special forces was the Special Atomic Demolition Munition, or SADM, sometimes popularly known as a suitcase nuke. This is a nuclear bomb that is man-portable, or at least truck-portable, and though of a relatively small yield (one or two kilotons) is sufficient to destroy important tactical targets such as bridges, dams, tunnels, important military or commercial installations, etc. either behind enemy lines or pre-emptively on friendly territory soon to be overtaken by invading enemy forces. These weapons require plutonium fuel and are particularly "dirty". They also demand especially stringent security precautions in their storage and deployment.[citation needed]

Small "tactical" nuclear weapons were deployed for use as antiaircraft weapons. Examples include the USAF AIR-2 Genie, the AIM-26 Falcon and US Army Nike Hercules. Missile interceptors such as the Sprint and the Spartan also used small nuclear warheads (optimized to produce neutron or X-ray flux) but were for use against enemy strategic warheads.[citation needed]

Other small, or tactical, nuclear weapons were deployed by naval forces for use primarily as antisubmarine weapons. These included nuclear depth bombs or nuclear armed torpedoes. Nuclear mines for use on land or at sea are also possibilities.[citation needed]

Weapons delivery
See also: Nuclear weapons delivery, Nuclear triad, Strategic bomber, Intercontinental ballistic missile, and Submarine-launched ballistic missile

The first nuclear weapons were gravity bombs, such as this "Fat Man" weapon dropped on Nagasaki, Japan. They were large and could only be delivered by heavy bomber aircraft

A demilitarized, commercial launch of the Russian Strategic Rocket Forces R-36 ICBM; also known by the NATO reporting name: SS-18 Satan. Upon its first fielding in the late 1960s, the SS-18 remains the single highest throw weight missile delivery system ever built.
The system used to deliver a nuclear weapon to its target is an important factor affecting both nuclear weapon design and nuclear strategy. The design, development, and maintenance of delivery systems are among the most expensive parts of a nuclear weapons program; they account, for example, for 57% of the financial resources spent by the United States on nuclear weapons projects since 1940.[28]

The simplest method for delivering a nuclear weapon is a gravity bomb dropped from aircraft; this was the method used by the United States against Japan. This method places few restrictions on the size of the weapon. It does, however, limit attack range, response time to an impending attack, and the number of weapons that a country can field at the same time. With miniaturization, nuclear bombs can be delivered by both strategic bombers and tactical fighter-bombers. This method is the primary means of nuclear weapons delivery; the majority of U.S. nuclear warheads, for example, are free-fall gravity bombs, namely the B61, which is being improved upon to this day.[11][needs update][29]


Montage of an inert test of a United States Trident SLBM (submarine launched ballistic missile), from submerged to the terminal, or re-entry phase, of the multiple independently targetable reentry vehicles
Preferable from a strategic point of view is a nuclear weapon mounted on a missile, which can use a ballistic trajectory to deliver the warhead over the horizon. Although even short-range missiles allow for a faster and less vulnerable attack, the development of long-range intercontinental ballistic missiles (ICBMs) and submarine-launched ballistic missiles (SLBMs) has given some nations the ability to plausibly deliver missiles anywhere on the globe with a high likelihood of success.

More advanced systems, such as multiple independently targetable reentry vehicles (MIRVs), can launch multiple warheads at different targets from one missile, reducing the chance of a successful missile defense. Today, missiles are most common among systems designed for delivery of nuclear weapons. Making a warhead small enough to fit onto a missile, though, can be difficult.[11]

Tactical weapons have involved the most variety of delivery types, including not only gravity bombs and missiles but also artillery shells, land mines, and nuclear depth charges and torpedoes for anti-submarine warfare. An atomic mortar has been tested by the United States. Small, two-man portable tactical weapons (somewhat misleadingly referred to as suitcase bombs), such as the Special Atomic Demolition Munition, have been developed, although the difficulty of combining sufficient yield with portability limits their military utility.[11]

Nuclear strategy
Main articles: Nuclear strategy and Deterrence theory
See also: Pre-emptive nuclear strike, Nuclear peace, Essentials of Post–Cold War Deterrence, Single Integrated Operational Plan, Nuclear warfare, and On Thermonuclear War
Nuclear warfare strategy is a set of policies that deal with preventing or fighting a nuclear war. The policy of trying to prevent an attack by a nuclear weapon from another country by threatening nuclear retaliation is known as the strategy of nuclear deterrence. The goal in deterrence is to always maintain a second strike capability (the ability of a country to respond to a nuclear attack with one of its own) and potentially to strive for first strike status (the ability to destroy an enemy's nuclear forces before they could retaliate). During the Cold War, policy and military theorists considered the sorts of policies that might prevent a nuclear attack, and they developed game theory models that could lead to stable deterrence conditions.[30]


The now decommissioned United States' Peacekeeper missile was an ICBM developed to replace the Minuteman missile in the late 1980s. Each missile, like the heavier lift Russian SS-18 Satan, could contain up to ten nuclear warheads (shown in red), each of which could be aimed at a different target. A factor in the development of MIRVs was to make complete missile defense difficult for an enemy country.
Different forms of nuclear weapons delivery (see above) allow for different types of nuclear strategies. The goals of any strategy are generally to make it difficult for an enemy to launch a pre-emptive strike against the weapon system and difficult to defend against the delivery of the weapon during a potential conflict. This can mean keeping weapon locations hidden, such as deploying them on submarines or land mobile transporter erector launchers whose locations are difficult to track, or it can mean protecting weapons by burying them in hardened missile silo bunkers. Other components of nuclear strategies included using missile defenses to destroy the missiles before they land or implementing civil defense measures using early-warning systems to evacuate citizens to safe areas before an attack.

Weapons designed to threaten large populations or to deter attacks are known as strategic weapons. Nuclear weapons for use on a battlefield in military situations are called tactical weapons.

Critics of nuclear war strategy often suggest that a nuclear war between two nations would result in mutual annihilation. From this point of view, the significance of nuclear weapons is to deter war because any nuclear war would escalate out of mutual distrust and fear, resulting in mutually assured destruction. This threat of national, if not global, destruction has been a strong motivation for anti-nuclear weapons activism.

Critics from the peace movement and within the military establishment[citation needed] have questioned the usefulness of such weapons in the current military climate. According to an advisory opinion issued by the International Court of Justice in 1996, the use of (or threat of use of) such weapons would generally be contrary to the rules of international law applicable in armed conflict, but the court did not reach an opinion as to whether or not the threat or use would be lawful in specific extreme circumstances such as if the survival of the state were at stake.


Ballistic missile submarines have been of great strategic importance for the United States, Russia, and other nuclear powers since they entered service in the Cold War, as they can hide from reconnaissance satellites and fire their nuclear weapons with virtual impunity.
Another deterrence position is that nuclear proliferation can be desirable. In this case, it is argued that, unlike conventional weapons, nuclear weapons deter all-out war between states, and they succeeded in doing this during the Cold War between the U.S. and the Soviet Union.[31] In the late 1950s and early 1960s, Gen. Pierre Marie Gallois of France, an adviser to Charles de Gaulle, argued in books like The Balance of Terror: Strategy for the Nuclear Age (1961) that mere possession of a nuclear arsenal was enough to ensure deterrence, and thus concluded that the spread of nuclear weapons could increase international stability. Some prominent neo-realist scholars, such as Kenneth Waltz and John Mearsheimer, have argued, along the lines of Gallois, that some forms of nuclear proliferation would decrease the likelihood of total war, especially in troubled regions of the world where there exists a single nuclear-weapon state. Aside from the public opinion that opposes proliferation in any form, there are two schools of thought on the matter: those, like Mearsheimer, who favored selective proliferation,[32] and Waltz, who was somewhat more non-interventionist.[33][34] Interest in proliferation and the stability-instability paradox that it generates continues to this day, with ongoing debate about indigenous Japanese and South Korean nuclear deterrent against North Korea.[35]

The threat of potentially suicidal terrorists possessing nuclear weapons (a form of nuclear terrorism) complicates the decision process. The prospect of mutually assured destruction might not deter an enemy who expects to die in the confrontation. Further, if the initial act is from a stateless terrorist instead of a sovereign nation, there might not be a nation or specific target to retaliate against. It has been argued, especially after the September 11, 2001, attacks, that this complication calls for a new nuclear strategy, one that is distinct from that which gave relative stability during the Cold War.[36] Since 1996, the United States has had a policy of allowing the targeting of its nuclear weapons at terrorists armed with weapons of mass destruction.[37]


A Minuteman III ICBM test launch from Vandenberg Air Force Base, United States. MIRVed land-based ICBMs are considered destabilizing because they tend to put a premium on striking first.
Robert Gallucci argues that although traditional deterrence is not an effective approach toward terrorist groups bent on causing a nuclear catastrophe, Gallucci believes that "the United States should instead consider a policy of expanded deterrence, which focuses not solely on the would-be nuclear terrorists but on those states that may deliberately transfer or inadvertently leak nuclear weapons and materials to them. By threatening retaliation against those states, the United States may be able to deter that which it cannot physically prevent.".[38]

Graham Allison makes a similar case, arguing that the key to expanded deterrence is coming up with ways of tracing nuclear material to the country that forged the fissile material. "After a nuclear bomb detonates, nuclear forensics cops would collect debris samples and send them to a laboratory for radiological analysis. By identifying unique attributes of the fissile material, including its impurities and contaminants, one could trace the path back to its origin."[39] The process is analogous to identifying a criminal by fingerprints. "The goal would be twofold: first, to deter leaders of nuclear states from selling weapons to terrorists by holding them accountable for any use of their weapons; second, to give leaders every incentive to tightly secure their nuclear weapons and materials."[39]

According to the Pentagon's June 2019 "Doctrine for Joint Nuclear Operations" of the Joint Chiefs of Staffs website Publication, "Integration of nuclear weapons employment with conventional and special operations forces is essential to the success of any mission or operation."[40][41]

Governance, control, and law
Main articles: Treaty on the Non-Proliferation of Nuclear Weapons, Strategic Arms Limitation Talks, Intermediate-Range Nuclear Forces Treaty, START I, START II, Strategic Offensive Reductions Treaty, Comprehensive Nuclear-Test-Ban Treaty, Lahore Declaration, New START, and Treaty on the Prohibition of Nuclear Weapons
See also: Anti-nuclear movement

The International Atomic Energy Agency was created in 1957 to encourage peaceful development of nuclear technology while providing international safeguards against nuclear proliferation.
Because they are weapons of mass destruction, the proliferation and possible use of nuclear weapons are important issues in international relations and diplomacy. In most countries, the use of nuclear force can only be authorized by the head of government or head of state.[c] Despite controls and regulations governing nuclear weapons, there is an inherent danger of "accidents, mistakes, false alarms, blackmail, theft, and sabotage".[42]

In the late 1940s, lack of mutual trust prevented the United States and the Soviet Union from making progress on arms control agreements. The Russell–Einstein Manifesto was issued in London on July 9, 1955, by Bertrand Russell in the midst of the Cold War. It highlighted the dangers posed by nuclear weapons and called for world leaders to seek peaceful resolutions to international conflict. The signatories included eleven pre-eminent intellectuals and scientists, including Albert Einstein, who signed it just days before his death on April 18, 1955. A few days after the release, philanthropist Cyrus S. Eaton offered to sponsor a conference—called for in the manifesto—in Pugwash, Nova Scotia, Eaton's birthplace. This conference was to be the first of the Pugwash Conferences on Science and World Affairs, held in July 1957.

By the 1960s, steps were taken to limit both the proliferation of nuclear weapons to other countries and the environmental effects of nuclear testing. The Partial Nuclear Test Ban Treaty (1963) restricted all nuclear testing to underground nuclear testing, to prevent contamination from nuclear fallout, whereas the Treaty on the Non-Proliferation of Nuclear Weapons (1968) attempted to place restrictions on the types of activities signatories could participate in, with the goal of allowing the transference of non-military nuclear technology to member countries without fear of proliferation.


UN vote on adoption of the Treaty on the Prohibition of Nuclear Weapons on July 7, 2017
  Yes
  No
  Did not vote
In 1957, the International Atomic Energy Agency (IAEA) was established under the mandate of the United Nations to encourage development of peaceful applications of nuclear technology, provide international safeguards against its misuse, and facilitate the application of safety measures in its use. In 1996, many nations signed the Comprehensive Nuclear-Test-Ban Treaty,[43] which prohibits all testing of nuclear weapons. A testing ban imposes a significant hindrance to nuclear arms development by any complying country.[44] The Treaty requires the ratification by 44 specific states before it can go into force; as of 2012, the ratification of eight of these states is still required.[43]

Additional treaties and agreements have governed nuclear weapons stockpiles between the countries with the two largest stockpiles, the United States and the Soviet Union, and later between the United States and Russia. These include treaties such as SALT II (never ratified), START I (expired), INF, START II (never in effect), SORT, and New START, as well as non-binding agreements such as SALT I and the Presidential Nuclear Initiatives[45] of 1991. Even when they did not enter into force, these agreements helped limit and later reduce the numbers and types of nuclear weapons between the United States and the Soviet Union/Russia.

Nuclear weapons have also been opposed by agreements between countries. Many nations have been declared Nuclear-Weapon-Free Zones, areas where nuclear weapons production and deployment are prohibited, through the use of treaties. The Treaty of Tlatelolco (1967) prohibited any production or deployment of nuclear weapons in Latin America and the Caribbean, and the Treaty of Pelindaba (1964) prohibits nuclear weapons in many African countries. As recently as 2006 a Central Asian Nuclear Weapon Free Zone was established among the former Soviet republics of Central Asia prohibiting nuclear weapons.


Large stockpile with global range (dark blue), smaller stockpile with global range (medium blue), small stockpile with regional range (light blue).
In 1996, the International Court of Justice, the highest court of the United Nations, issued an Advisory Opinion concerned with the "Legality of the Threat or Use of Nuclear Weapons". The court ruled that the use or threat of use of nuclear weapons would violate various articles of international law, including the Geneva Conventions, the Hague Conventions, the UN Charter, and the Universal Declaration of Human Rights. Given the unique, destructive characteristics of nuclear weapons, the International Committee of the Red Cross calls on States to ensure that these weapons are never used, irrespective of whether they consider them lawful or not.[46]

Additionally, there have been other, specific actions meant to discourage countries from developing nuclear arms. In the wake of the tests by India and Pakistan in 1998, economic sanctions were (temporarily) levied against both countries, though neither were signatories with the Nuclear Non-Proliferation Treaty. One of the stated casus belli for the initiation of the 2003 Iraq War was an accusation by the United States that Iraq was actively pursuing nuclear arms (though this was soon discovered not to be the case as the program had been discontinued). In 1981, Israel had bombed a nuclear reactor being constructed in Osirak, Iraq, in what it called an attempt to halt Iraq's previous nuclear arms ambitions; in 2007, Israel bombed another reactor being constructed in Syria.

In 2013, Mark Diesendorf said that governments of France, India, North Korea, Pakistan, UK, and South Africa have used nuclear power or research reactors to assist nuclear weapons development or to contribute to their supplies of nuclear explosives from military reactors.[47]

In 2017, 122 countries mainly in the Global South voted in favor of adopting the Treaty on the Prohibition of Nuclear Weapons, which eventually entered into force in 2021.[48]

The Doomsday Clock measures the likelihood of a human-made global catastrophe and is published annually by the Bulletin of the Atomic Scientists. The two years with the highest likelihood had previously been 1953, when the Clock was set to two minutes until midnight after the U.S. and the Soviet Union began testing hydrogen bombs, and 2018, following the failure of world leaders to address tensions relating to nuclear weapons and climate change issues.[49] In 2023, following the escalation of nuclear threats during the Russian invasion of Ukraine, the doomsday clock was set to 90 seconds, the highest likelihood of global catastrophe since the existence of the Doomsday Clock.[50]

Disarmament
Main article: Nuclear disarmament
For statistics on possession and deployment, see List of states with nuclear weapons.

The USSR and United States nuclear weapon stockpiles throughout the Cold War until 2015, with a precipitous drop in total numbers following the end of the Cold War in 1991.
Nuclear disarmament refers to both the act of reducing or eliminating nuclear weapons and to the end state of a nuclear-free world, in which nuclear weapons are eliminated.

Beginning with the 1963 Partial Test Ban Treaty and continuing through the 1996 Comprehensive Nuclear-Test-Ban Treaty, there have been many treaties to limit or reduce nuclear weapons testing and stockpiles. The 1968 Nuclear Non-Proliferation Treaty has as one of its explicit conditions that all signatories must "pursue negotiations in good faith" towards the long-term goal of "complete disarmament". The nuclear-weapon states have largely treated that aspect of the agreement as "decorative" and without force.[51]

Only one country—South Africa—has ever fully renounced nuclear weapons they had independently developed. The former Soviet republics of Belarus, Kazakhstan, and Ukraine returned Soviet nuclear arms stationed in their countries to Russia after the collapse of the USSR.

Proponents of nuclear disarmament say that it would lessen the probability of nuclear war, especially accidentally. Critics of nuclear disarmament say that it would undermine the present nuclear peace and deterrence and would lead to increased global instability. Various American elder statesmen,[52] who were in office during the Cold War period, have been advocating the elimination of nuclear weapons. These officials include Henry Kissinger, George Shultz, Sam Nunn, and William Perry. In January 2010, Lawrence M. Krauss stated that "no issue carries more importance to the long-term health and security of humanity than the effort to reduce, and perhaps one day, rid the world of nuclear weapons".[53]


Ukrainian workers use equipment provided by the U.S. Defense Threat Reduction Agency to dismantle a Soviet-era missile silo. After the end of the Cold War, Ukraine and the other non-Russian, post-Soviet republics relinquished Soviet nuclear stockpiles to Russia.
In January 1986, Soviet leader Mikhail Gorbachev publicly proposed a three-stage program for abolishing the world's nuclear weapons by the end of the 20th century.[54] In the years after the end of the Cold War, there have been numerous campaigns to urge the abolition of nuclear weapons, such as that organized by the Global Zero movement, and the goal of a "world without nuclear weapons" was advocated by United States President Barack Obama in an April 2009 speech in Prague.[55] A CNN poll from April 2010 indicated that the American public was nearly evenly split on the issue.[56]

Some analysts have argued that nuclear weapons have made the world relatively safer, with peace through deterrence and through the stability–instability paradox, including in south Asia.[57][58] Kenneth Waltz has argued that nuclear weapons have helped keep an uneasy peace, and further nuclear weapon proliferation might even help avoid the large scale conventional wars that were so common before their invention at the end of World War II.[34] But former Secretary Henry Kissinger says there is a new danger, which cannot be addressed by deterrence: "The classical notion of deterrence was that there was some consequences before which aggressors and evildoers would recoil. In a world of suicide bombers, that calculation doesn't operate in any comparable way".[59] George Shultz has said, "If you think of the people who are doing suicide attacks, and people like that get a nuclear weapon, they are almost by definition not deterrable".[60]

As of early 2019, more than 90% of world's 13,865 nuclear weapons were owned by Russia and the United States.[61][62]

United Nations
Main article: United Nations Office for Disarmament Affairs
The UN Office for Disarmament Affairs (UNODA) is a department of the United Nations Secretariat established in January 1998 as part of the United Nations Secretary-General Kofi Annan's plan to reform the UN as presented in his report to the General Assembly in July 1997.[63]

Its goal is to promote nuclear disarmament and non-proliferation and the strengthening of the disarmament regimes in respect to other weapons of mass destruction, chemical and biological weapons. It also promotes disarmament efforts in the area of conventional weapons, especially land mines and small arms, which are often the weapons of choice in contemporary conflicts.

Controversy
See also: Nuclear weapons debate and History of the anti-nuclear movement
Ethics
Main article: Nuclear ethics

Anti-nuclear weapons protest march in Oxford, 1980
Even before the first nuclear weapons had been developed, scientists involved with the Manhattan Project were divided over the use of the weapon. The role of the two atomic bombings of the country in Japan's surrender and the U.S.'s ethical justification for them has been the subject of scholarly and popular debate for decades. The question of whether nations should have nuclear weapons, or test them, has been continually and nearly universally controversial.[64]

Notable nuclear weapons accidents
Main articles: Nuclear and radiation accidents and incidents and List of military nuclear accidents
See also: List of nuclear close calls
August 21, 1945: While conducting experiments on a plutonium-gallium core at Los Alamos National Laboratory, physicist Harry Daghlian received a lethal dose of radiation when an error caused it to enter prompt criticality. He died 25 days later, on September 15, 1945, from radiation poisoning.[65]
May 21, 1946: While conducting further experiments on the same core at Los Alamos National Laboratory, physicist Louis Slotin accidentally caused the core to become briefly supercritical. He received a lethal dose of gamma and neutron radiation, and died nine days later on May 30, 1946. After the death of Daghlian and Slotin, the mass became known as the "demon core". It was ultimately used to construct a bomb for use on the Nevada Test Range.[66]
February 13, 1950: a Convair B-36B crashed in northern British Columbia after jettisoning a Mark IV atomic bomb. This was the first such nuclear weapon loss in history. The accident was designated a "Broken Arrow"—an accident involving a nuclear weapon, but which does not present a risk of war. Experts believe that up to 50 nuclear weapons were lost during the Cold War.[67]
May 22, 1957: a 42,000-pound (19,000 kg) Mark-17 hydrogen bomb accidentally fell from a bomber near Albuquerque, New Mexico. The detonation of the device's conventional explosives destroyed it on impact and formed a crater 25 feet (7.6 m) in diameter on land owned by the University of New Mexico. According to a researcher at the Natural Resources Defense Council, it was one of the most powerful bombs made to date.[68]
June 7, 1960: the 1960 Fort Dix IM-99 accident destroyed a Boeing CIM-10 Bomarc nuclear missile and shelter and contaminated the BOMARC Missile Accident Site in New Jersey.
January 24, 1961: the 1961 Goldsboro B-52 crash occurred near Goldsboro, North Carolina. A Boeing B-52 Stratofortress carrying two Mark 39 nuclear bombs broke up in mid-air, dropping its nuclear payload in the process.[69]
1965 Philippine Sea A-4 crash, where a Skyhawk attack aircraft with a nuclear weapon fell into the sea.[70] The pilot, the aircraft, and the B43 nuclear bomb were never recovered.[71] It was not until 1989 that the Pentagon revealed the loss of the one-megaton bomb.[72]
January 17, 1966: the 1966 Palomares B-52 crash occurred when a B-52G bomber of the USAF collided with a KC-135 tanker during mid-air refuelling off the coast of Spain. The KC-135 was completely destroyed when its fuel load ignited, killing all four crew members. The B-52G broke apart, killing three of the seven crew members aboard.[73] Of the four Mk28 type hydrogen bombs the B-52G carried,[74] three were found on land near Almería, Spain. The non-nuclear explosives in two of the weapons detonated upon impact with the ground, resulting in the contamination of a 2-square-kilometer (490-acre) (0.78 square mile) area by radioactive plutonium. The fourth, which fell into the Mediterranean Sea, was recovered intact after a 21⁄2-month-long search.[75]
January 21, 1968: the 1968 Thule Air Base B-52 crash involved a United States Air Force (USAF) B-52 bomber. The aircraft was carrying four hydrogen bombs when a cabin fire forced the crew to abandon the aircraft. Six crew members ejected safely, but one who did not have an ejection seat was killed while trying to bail out. The bomber crashed onto sea ice in Greenland, causing the nuclear payload to rupture and disperse, which resulted in widespread radioactive contamination.[76] One of the bombs remains lost.[77]
September 18-19, 1980: the Damascus Accident occurred in Damascus, Arkansas, where a Titan Missile equipped with a nuclear warhead exploded. The accident was caused by a maintenance man who dropped a socket from a socket wrench down an 80-foot (24 m) shaft, puncturing a fuel tank on the rocket. Leaking fuel resulted in a hypergolic fuel explosion, jettisoning the W-53 warhead beyond the launch site.[78][79][80]
Nuclear testing and fallout
Main article: Nuclear fallout
See also: Downwinders

Over 2,000 nuclear tests have been conducted in over a dozen different sites around the world. Red Russia/Soviet Union, blue France, light blue United States, violet Britain, yellow China, orange India, brown Pakistan, green North Korea and light green (territories exposed to nuclear bombs). The Black dot indicates the location of the Vela incident.

This view of downtown Las Vegas shows a mushroom cloud in the background. Scenes such as this were typical during the 1950s. From 1951 to 1962 the government conducted 100 atmospheric tests at the nearby Nevada Test Site.
Over 500 atmospheric nuclear weapons tests were conducted at various sites around the world from 1945 to 1980. Radioactive fallout from nuclear weapons testing was first drawn to public attention in 1954 when the Castle Bravo hydrogen bomb test at the Pacific Proving Grounds contaminated the crew and catch of the Japanese fishing boat Lucky Dragon.[81] One of the fishermen died in Japan seven months later, and the fear of contaminated tuna led to a temporary boycotting of the popular staple in Japan. The incident caused widespread concern around the world, especially regarding the effects of nuclear fallout and atmospheric nuclear testing, and "provided a decisive impetus for the emergence of the anti-nuclear weapons movement in many countries".[81]

As public awareness and concern mounted over the possible health hazards associated with exposure to the nuclear fallout, various studies were done to assess the extent of the hazard. A Centers for Disease Control and Prevention/ National Cancer Institute study claims that fallout from atmospheric nuclear tests would lead to perhaps 11,000 excess deaths among people alive during atmospheric testing in the United States from all forms of cancer, including leukemia, from 1951 to well into the 21st century.[82][83] As of March 2009, the U.S. is the only nation that compensates nuclear test victims. Since the Radiation Exposure Compensation Act of 1990, more than $1.38 billion in compensation has been approved. The money is going to people who took part in the tests, notably at the Nevada Test Site, and to others exposed to the radiation.[84][85]

In addition, leakage of byproducts of nuclear weapon production into groundwater has been an ongoing issue, particularly at the Hanford site.[86]

Effects of nuclear explosions
Main article: Effects of nuclear explosions
Effects of nuclear explosions on human health
Main article: Effects of nuclear explosions on human health

A photograph of Sumiteru Taniguchi's back injuries taken in January 1946 by a U.S. Marine photographer
Some scientists estimate that a nuclear war with 100 Hiroshima-size nuclear explosions on cities could cost the lives of tens of millions of people from long-term climatic effects alone. The climatology hypothesis is that if each city firestorms, a great deal of soot could be thrown up into the atmosphere which could blanket the earth, cutting out sunlight for years on end, causing the disruption of food chains, in what is termed a nuclear winter.[87][88]

People near the Hiroshima explosion and who managed to survive the explosion subsequently suffered a variety of horrible medical effects. Some of these effects are still present to this day:[89]

Initial stage—the first 1–9 weeks, in which are the greatest number of deaths, with 90% due to thermal injury or blast effects and 10% due to super-lethal radiation exposure.
Intermediate stage—from 10 to 12 weeks. The deaths in this period are from ionizing radiation in the median lethal range – LD50
Late period—lasting from 13 to 20 weeks. This period has some improvement in survivors' condition.
Delayed period—from 20+ weeks. Characterized by numerous complications, mostly related to healing of thermal and mechanical injuries, and if the individual was exposed to a few hundred to a thousand millisieverts of radiation, it is coupled with infertility, sub-fertility and blood disorders. Furthermore, ionizing radiation above a dose of around 50–100 millisievert exposure has been shown to statistically begin increasing one's chance of dying of cancer sometime in their lifetime over the normal unexposed rate of ~25%, in the long term, a heightened rate of cancer, proportional to the dose received, would begin to be observed after ~5+ years, with lesser problems such as eye cataracts and other more minor effects in other organs and tissue also being observed over the long term.
Fallout exposure—depending on if further afield individuals shelter in place or evacuate perpendicular to the direction of the wind, and therefore avoid contact with the fallout plume, and stay there for the days and weeks after the nuclear explosion, their exposure to fallout, and therefore their total dose, will vary. With those who do shelter in place, and or evacuate, experiencing a total dose that would be negligible in comparison to someone who just went about their life as normal.[90][91]

Staying indoors until after the most hazardous fallout isotope, I-131 decays away to 0.1% of its initial quantity after ten half-lifes—which is represented by 80 days in I-131s case, would make the difference between likely contracting Thyroid cancer or escaping completely from this substance depending on the actions of the individual.[92]

Effects of nuclear war
See also: Nuclear holocaust, Doomsday Clock, Doomsday device, World War III, and Nuclear famine

Mushroom cloud from the explosion of Castle Bravo, the largest nuclear weapon detonated by the U.S., in 1954
Nuclear war could yield unprecedented human death tolls and habitat destruction. Detonating large numbers of nuclear weapons would have an immediate, short term and long-term effects on the climate, potentially causing cold weather known as a "nuclear winter".[93][94] In 1982, Brian Martin estimated that a US–Soviet nuclear exchange might kill 400–450 million directly, mostly in the United States, Europe and Russia, and maybe several hundred million more through follow-up consequences in those same areas.[95] Many scholars have posited that a global thermonuclear war with Cold War-era stockpiles, or even with the current smaller stockpiles, may lead to the extinction of the human race.[96] The International Physicians for the Prevention of Nuclear War believe that nuclear war could indirectly contribute to human extinction via secondary effects, including environmental consequences, societal breakdown, and economic collapse. It has been estimated that a relatively small-scale nuclear exchange between India and Pakistan involving 100 Hiroshima yield (15 kilotons) weapons, could cause a nuclear winter and kill more than a billion people.[97]

According to a peer-reviewed study published in the journal Nature Food in August 2022, a full-scale nuclear war between the U.S. and Russia would directly kill 360 million people and more than 5 billion people would die from starvation. More than 2 billion people could die from a smaller-scale nuclear war between India and Pakistan.[94][98][99]

Public opposition
See also: Nuclear disarmament and International Day against Nuclear Tests

Protest in Bonn against the nuclear arms race between the U.S./NATO and the Warsaw Pact, 1981

Demonstration against nuclear testing in Lyon, France, in the 1980s.
Peace movements emerged in Japan and in 1954 they converged to form a unified "Japan Council against Atomic and Hydrogen Bombs." Japanese opposition to nuclear weapons tests in the Pacific Ocean was widespread, and "an estimated 35 million signatures were collected on petitions calling for bans on nuclear weapons".[100]

In the United Kingdom, the first Aldermaston March organised by the Campaign for Nuclear Disarmament(CND) took place at Easter 1958, when, according to the CND, several thousand people marched for four days from Trafalgar Square, London, to the Atomic Weapons Research Establishment close to Aldermaston in Berkshire, England, to demonstrate their opposition to nuclear weapons.[101][102] The Aldermaston marches continued into the late 1960s when tens of thousands of people took part in the four-day marches.[100]

In 1959, a letter in the Bulletin of the Atomic Scientists was the start of a successful campaign to stop the Atomic Energy Commission dumping radioactive waste in the sea 19 kilometres from Boston.[103] In 1962, Linus Pauling won the Nobel Peace Prize for his work to stop the atmospheric testing of nuclear weapons, and the "Ban the Bomb" movement spread.[64]

In 1963, many countries ratified the Partial Test Ban Treaty prohibiting atmospheric nuclear testing. Radioactive fallout became less of an issue and the anti-nuclear weapons movement went into decline for some years.[81][104] A resurgence of interest occurred amid European and American fears of nuclear war in the 1980s.[105]

Costs and technology spin-offs
See also: Global Positioning System, Nuclear weapons delivery, History of computing hardware, ENIAC, and Swords to ploughshares
According to an audit by the Brookings Institution, between 1940 and 1996, the U.S. spent $10.9 trillion in present-day terms[106] on nuclear weapons programs. 57% of which was spent on building nuclear weapons delivery systems. 6.3% of the total, $681 billion in present-day terms, was spent on environmental remediation and nuclear waste management, for example cleaning up the Hanford site, and 7% of the total, $763 billion was spent on making nuclear weapons themselves.[107]

Non-weapons uses
Main article: Peaceful nuclear explosion
Peaceful nuclear explosions are nuclear explosions conducted for non-military purposes, such as activities related to economic development including the creation of canals. During the 1960s and 1970s, both the United States and the Soviet Union conducted a number of PNEs. Six of the explosions by the Soviet Union are considered to have been of an applied nature, not just tests.

The United States and the Soviet Union later halted their programs. Definitions and limits are covered in the Peaceful Nuclear Explosions Treaty of 1976.[108][109] The stalled Comprehensive Nuclear-Test-Ban Treaty of 1996 would prohibit all nuclear explosions, regardless of whether they are for peaceful purposes or not.[110]

History of development
Main article: History of nuclear weapons
See also: Soviet atomic bomb project, Manhattan Project, Cold War, and History of the Teller–Ulam design
This section is an excerpt from History of nuclear weapons § Background.[edit]

In nuclear fission, the nucleus of a fissile atom (in this case, enriched uranium) absorbs a thermal neutron, becomes unstable and splits into two new atoms, releasing some energy and between one and three new neutrons, which can perpetuate the process.
In the first decades of the 20th century, physics was revolutionized with developments in the understanding of the nature of atoms including the discoveries in atomic theory by John Dalton.[111] Around the turn of the 20th century, it was discovered by Hans Geiger and Ernest Marsden and then Ernest Rutherford, that atoms had a highly dense, very small, charged central core called an atomic nucleus. In 1898, Pierre and Marie Curie discovered that pitchblende, an ore of uranium, contained a substance—which they named radium—that emitted large amounts of radiation. Ernest Rutherford and Frederick Soddy identified that atoms were breaking down and turning into different elements. Hopes were raised among scientists and laymen that the elements around us could contain tremendous amounts of unseen energy, waiting to be harnessed.

In Paris in 1934, Irène and Frédéric Joliot-Curie discovered that artificial radioactivity could be induced in stable elements by bombarding them with alpha particles; in Italy Enrico Fermi reported similar results when bombarding uranium with neutrons.

In December 1938, Otto Hahn and Fritz Strassmann reported that they had detected the element barium after bombarding uranium with neutrons. Lise Meitner and Otto Robert Frisch correctly interpreted these results as being due to the splitting of the uranium atom. Frisch confirmed this experimentally on January 13, 1939.[112] They gave the process the name "fission" because of its similarity to the splitting of a cell into two new cells. Even before it was published, news of Meitner's and Frisch's interpretation crossed the Atlantic.[113] In their second publication on nuclear fission in February of 1939, Hahn and Strassmann predicted the existence and liberation of additional neutrons during the fission process, opening up the possibility of a nuclear chain reaction.


Leo Szilard, pictured in about 1960, invented the electron microscope, nuclear chain reaction and patented the nuclear reactor
After learning about the German fission in 1939, Leo Szilard concluded that uranium would be the element which can realize his 1933 idea about nuclear chain reaction.[114]

Uranium appears in nature primarily in two isotopes: uranium-238 and uranium-235. When the nucleus of uranium-235 absorbs a neutron, it undergoes nuclear fission, releasing energy and, on average, 2.5 neutrons. Because uranium-235 releases more neutrons than it absorbs, it can support a chain reaction and so is described as fissile. Uranium-238, on the other hand, is not fissile as it does not normally undergo fission when it absorbs a neutron.

By the start of the war in September 1939, many scientists likely to be persecuted by the Nazis had already escaped. Physicists on both sides were well aware of the possibility of utilizing nuclear fission as a weapon, but no one was quite sure how it could be engineered. In August 1939, concerned that Germany might have its own project to develop fission-based weapons, Albert Einstein signed a letter to U.S. President Franklin D. Roosevelt warning him of the threat.[115]


Major General Leslie Groves and Robert Oppenheimer at the Trinity test site in 1945
Roosevelt responded by setting up the Uranium Committee under Lyman James Briggs but, with little initial funding ($6,000), progress was slow. It was not until the U.S. entered the war in December 1941 that Washington decided to commit the necessary resources to a top-secret high priority bomb project.[116]

Organized research first began in Britain and Canada as part of the Tube Alloys project: the world's first nuclear weapons project. The Maud Committee was set up following the work of Frisch and Rudolf Peierls who calculated uranium-235's critical mass and found it to be much smaller than previously thought which meant that a deliverable bomb should be possible.[117] In the February 1940 Frisch–Peierls memorandum they stated that: "The energy liberated in the explosion of such a super-bomb...will, for an instant, produce a temperature comparable to that of the interior of the sun. The blast from such an explosion would destroy life in a wide area. The size of this area is difficult to estimate, but it will probably cover the centre of a big city."

Edgar Sengier, a director of Shinkolobwe Mine in the Congo which produced by far the highest quality uranium ore in the world, had become aware of uranium's possible use in a bomb. In late 1940, fearing that it might be seized by the Germans, he shipped the mine's entire stockpile of ore to a warehouse in New York.[118]
Nuclear medicine or nucleology[1] is a medical specialty involving the application of radioactive substances in the diagnosis and treatment of disease. Nuclear imaging, in a sense, is "radiology done inside out" because it records radiation emitted from within the body rather than radiation that is transmitted through the body from external sources like X-ray generators. In addition, nuclear medicine scans differ from radiology, as the emphasis is not on imaging anatomy, but on the function. For such reason, it is called a physiological imaging modality. Single photon emission computed tomography (SPECT) and positron emission tomography (PET) scans are the two most common imaging modalities in nuclear medicine.[2]

Diagnostic medical imaging
Diagnostic
In nuclear medicine imaging, radiopharmaceuticals are taken internally, for example, through inhalation, intravenously, or orally. Then, external detectors (gamma cameras) capture and form images from the radiation emitted by the radiopharmaceuticals. This process is unlike a diagnostic X-ray, where external radiation is passed through the body to form an image.[citation needed]

There are several techniques of diagnostic nuclear medicine.

2D: Scintigraphy ("scint") is the use of internal radionuclides to create two-dimensional images.[3]
Nuclear medicine tests differ from most other imaging modalities in that nuclear medicine scans primarily show the physiological function of the system being investigated as opposed to traditional anatomical imaging such as CT or MRI. Nuclear medicine imaging studies are generally more organ-, tissue- or disease-specific (e.g.: lungs scan, heart scan, bone scan, brain scan, tumor, infection, Parkinson etc.) than those in conventional radiology imaging, which focus on a particular section of the body (e.g.: chest X-ray, abdomen/pelvis CT scan, head CT scan, etc.). In addition, there are nuclear medicine studies that allow imaging of the whole body based on certain cellular receptors or functions. Examples are whole body PET scans or PET/CT scans, gallium scans, indium white blood cell scans, MIBG and octreotide scans.


Iodine-123 whole body scan for thyroid cancer evaluation. The study above was performed after the total thyroidectomy and TSH stimulation with thyroid hormone medication withdrawal. The study shows a small residual thyroid tissue in the neck and a mediastinum lesion, consistent with the thyroid cancer metastatic disease. The observable uptakes in the stomach and bladder are normal physiologic findings.
While the ability of nuclear metabolism to image disease processes from differences in metabolism is unsurpassed, it is not unique. Certain techniques such as fMRI image tissues (particularly cerebral tissues) by blood flow and thus show metabolism. Also, contrast-enhancement techniques in both CT and MRI show regions of tissue that are handling pharmaceuticals differently, due to an inflammatory process.

Diagnostic tests in nuclear medicine exploit the way that the body handles substances differently when there is disease or pathology present. The radionuclide introduced into the body is often chemically bound to a complex that acts characteristically within the body; this is commonly known as a tracer. In the presence of disease, a tracer will often be distributed around the body and/or processed differently. For example, the ligand methylene-diphosphonate (MDP) can be preferentially taken up by bone. By chemically attaching technetium-99m to MDP, radioactivity can be transported and attached to bone via the hydroxyapatite for imaging. Any increased physiological function, such as due to a fracture in the bone, will usually mean increased concentration of the tracer. This often results in the appearance of a "hot spot", which is a focal increase in radio accumulation or a general increase in radio accumulation throughout the physiological system. Some disease processes result in the exclusion of a tracer, resulting in the appearance of a "cold spot". Many tracer complexes have been developed to image or treat many different organs, glands, and physiological processes.

Hybrid scanning techniques
In some centers, the nuclear medicine scans can be superimposed, using software or hybrid cameras, on images from modalities such as CT or MRI to highlight the part of the body in which the radiopharmaceutical is concentrated. This practice is often referred to as image fusion or co-registration, for example SPECT/CT and PET/CT. The fusion imaging technique in nuclear medicine provides information about the anatomy and function, which would otherwise be unavailable or would require a more invasive procedure or surgery.

Normal whole body PET/CT scan with FDG-18. The whole body PET/CT scan is commonly used in the detection, staging and follow-up of various cancers.
Normal whole body PET/CT scan with FDG-18. The whole body PET/CT scan is commonly used in the detection, staging and follow-up of various cancers.

 
Abnormal whole body PET/CT scan with multiple metastases from a cancer. The whole body PET/CT scan has become an important tool in the evaluation of cancer.
Abnormal whole body PET/CT scan with multiple metastases from a cancer. The whole body PET/CT scan has become an important tool in the evaluation of cancer.

Practical concerns in nuclear imaging
Although the risks of low-level radiation exposures are not well understood, a cautious approach has been universally adopted that all human radiation exposures should be kept As Low As Reasonably Practicable, "ALARP". (Originally, this was known as "As Low As Reasonably Achievable" (ALARA), but this has changed in modern draftings of the legislation to add more emphasis on the "Reasonably" and less on the "Achievable".)

Working with the ALARP principle, before a patient is exposed for a nuclear medicine examination, the benefit of the examination must be identified. This needs to take into account the particular circumstances of the patient in question, where appropriate. For instance, if a patient is unlikely to be able to tolerate a sufficient amount of the procedure to achieve a diagnosis, then it would be inappropriate to proceed with injecting the patient with the radioactive tracer.

When the benefit does justify the procedure, then the radiation exposure (the amount of radiation given to the patient) should also be kept as low as reasonably practicable. This means that the images produced in nuclear medicine should never be better than required for confident diagnosis. Giving larger radiation exposures can reduce the noise in an image and make it more photographically appealing, but if the clinical question can be answered without this level of detail, then this is inappropriate.

As a result, the radiation dose from nuclear medicine imaging varies greatly depending on the type of study. The effective radiation dose can be lower than or comparable to or can far exceed the general day-to-day environmental annual background radiation dose. Likewise, it can also be less than, in the range of, or higher than the radiation dose from an abdomen/pelvis CT scan.

Some nuclear medicine procedures require special patient preparation before the study to obtain the most accurate result. Pre-imaging preparations may include dietary preparation or the withholding of certain medications. Patients are encouraged to consult with the nuclear medicine department prior to a scan.

Analysis
The result of the nuclear medicine imaging process is a dataset comprising one or more images. In multi-image datasets the array of images may represent a time sequence (i.e. cine or movie) often called a "dynamic" dataset, a cardiac gated time sequence, or a spatial sequence where the gamma-camera is moved relative to the patient. SPECT (single photon emission computed tomography) is the process by which images acquired from a rotating gamma-camera are reconstructed to produce an image of a "slice" through the patient at a particular position. A collection of parallel slices form a slice-stack, a three-dimensional representation of the distribution of radionuclide in the patient.

The nuclear medicine computer may require millions of lines of source code to provide quantitative analysis packages for each of the specific imaging techniques available in nuclear medicine.[citation needed]

Time sequences can be further analysed using kinetic models such as multi-compartment models or a Patlak plot.

Interventional nuclear medicine
Main articles: Unsealed source radiotherapy and Brachytherapy
Radionuclide therapy can be used to treat conditions such as hyperthyroidism, thyroid cancer, skin cancer and blood disorders.

In nuclear medicine therapy, the radiation treatment dose is administered internally (e.g. intravenous or oral routes) or externally direct above the area to treat in form of a compound (e.g. in case of skin cancer).

The radiopharmaceuticals used in nuclear medicine therapy emit ionizing radiation that travels only a short distance, thereby minimizing unwanted side effects and damage to noninvolved organs or nearby structures. Most nuclear medicine therapies can be performed as outpatient procedures since there are few side effects from the treatment and the radiation exposure to the general public can be kept within a safe limit.

Common nuclear medicine (unsealed source) therapies

Substance	Condition
Iodine-131-sodium iodide	hyperthyroidism and thyroid cancer
Yttrium-90-ibritumomab tiuxetan (Zevalin) and Iodine-131-tositumomab (Bexxar)	refractory lymphoma
131I-MIBG (metaiodobenzylguanidine)	neuroendocrine tumors
Samarium-153 or Strontium-89	palliative bone pain treatment
Rhenium-188	squamous cell carcinoma or basal cell carcinoma of the skin
In some centers the nuclear medicine department may also use implanted capsules of isotopes (brachytherapy) to treat cancer.

Commonly used radiation sources (radionuclides) for brachytherapy[4]

Radionuclide	Type	Half-life	Energy
Caesium-137 (137Cs)	γ-ray	30.17 years	0.662 MeV
Cobalt-60 (60Co)	γ-ray	5.26 years	1.17, 1.33 MeV
Iridium-192 (192Ir)	β−-particles	73.8 days	0.38 MeV (mean)
Iodine-125 (125I)	γ-rays	59.6 days	27.4, 31.4 and 35.5 keV
Palladium-103 (103Pd)	γ-ray	17.0 days	21 keV (mean)
Ruthenium-106 (106Ru)	β−-particles	1.02 years	3.54 MeV
History
The history of nuclear medicine contains contributions from scientists across different disciplines in physics, chemistry, engineering, and medicine. The multidisciplinary nature of nuclear medicine makes it difficult for medical historians to determine the birthdate of nuclear medicine. This can probably be best placed between the discovery of artificial radioactivity in 1934 and the production of radionuclides by Oak Ridge National Laboratory for medicine-related use, in 1946.[5]

The origins of this medical idea date back as far as the mid-1920s in Freiburg, Germany, when George de Hevesy made experiments with radionuclides administered to rats, thus displaying metabolic pathways of these substances and establishing the tracer principle. Possibly, the genesis of this medical field took place in 1936, when John Lawrence, known as "the father of nuclear medicine", took a leave of absence from his faculty position at Yale Medical School, to visit his brother Ernest Lawrence at his new radiation laboratory (now known as the Lawrence Berkeley National Laboratory) in Berkeley, California. Later on, John Lawrence made the first application in patients of an artificial radionuclide when he used phosphorus-32 to treat leukemia.[6][7]

Many historians consider the discovery of artificially produced radionuclides by Frédéric Joliot-Curie and Irène Joliot-Curie in 1934 as the most significant milestone in nuclear medicine.[5] In February 1934, they reported the first artificial production of radioactive material in the journal Nature, after discovering radioactivity in aluminum foil that was irradiated with a polonium preparation. Their work built upon earlier discoveries by Wilhelm Konrad Roentgen for X-ray, Henri Becquerel for radioactive uranium salts, and Marie Curie (mother of Irène Curie) for radioactive thorium, polonium and coining the term "radioactivity." Taro Takemi studied the application of nuclear physics to medicine in the 1930s. The history of nuclear medicine will not be complete without mentioning these early pioneers.

Nuclear medicine gained public recognition as a potential specialty when on May 11, 1946, an article in the Journal of the American Medical Association (JAMA) by Massachusetts General Hospital's Dr. Saul Hertz and Massachusetts Institute of Technology's Dr. Arthur Roberts, described the successful use of treating Graves' Disease with radioactive iodine (RAI) was published.[8] Additionally, Sam Seidlin.[9] brought further development in the field describing a successful treatment of a patient with thyroid cancer metastases using radioiodine (I-131). These articles are considered by many historians as the most important articles ever published in nuclear medicine.[10] Although the earliest use of I-131 was devoted to therapy of thyroid cancer, its use was later expanded to include imaging of the thyroid gland, quantification of the thyroid function, and therapy for hyperthyroidism. Among the many radionuclides that were discovered for medical-use, none were as important as the discovery and development of Technetium-99m. It was first discovered in 1937 by C. Perrier and E. Segre as an artificial element to fill space number 43 in the Periodic Table. The development of a generator system to produce Technetium-99m in the 1960s became a practical method for medical use. Today, Technetium-99m is the most utilized element in nuclear medicine and is employed in a wide variety of nuclear medicine imaging studies.

Widespread clinical use of nuclear medicine began in the early 1950s, as knowledge expanded about radionuclides, detection of radioactivity, and using certain radionuclides to trace biochemical processes. Pioneering works by Benedict Cassen in developing the first rectilinear scanner and Hal O. Anger's scintillation camera (Anger camera) broadened the young discipline of nuclear medicine into a full-fledged medical imaging specialty.

By the early 1960s, in southern Scandinavia, Niels A. Lassen, David H. Ingvar, and Erik Skinhøj developed techniques that provided the first blood flow maps of the brain, which initially involved xenon-133 inhalation;[11] an intra-arterial equivalent was developed soon after, enabling measurement of the local distribution of cerebral activity for patients with neuropsychiatric disorders such as schizophrenia.[12] Later versions would have 254 scintillators so a two-dimensional image could be produced on a color monitor. It allowed them to construct images reflecting brain activation from speaking, reading, visual or auditory perception and voluntary movement.[13] The technique was also used to investigate, e.g., imagined sequential movements, mental calculation and mental spatial navigation.[14][15]

By the 1970s most organs of the body could be visualized using nuclear medicine procedures. In 1971, American Medical Association officially recognized nuclear medicine as a medical specialty.[16] In 1972, the American Board of Nuclear Medicine was established, and in 1974, the American Osteopathic Board of Nuclear Medicine was established, cementing nuclear medicine as a stand-alone medical specialty.

In the 1980s, radiopharmaceuticals were designed for use in diagnosis of heart disease. The development of single photon emission computed tomography (SPECT), around the same time, led to three-dimensional reconstruction of the heart and establishment of the field of nuclear cardiology.

More recent developments in nuclear medicine include the invention of the first positron emission tomography scanner (PET). The concept of emission and transmission tomography, later developed into single photon emission computed tomography (SPECT), was introduced by David E. Kuhl and Roy Edwards in the late 1950s.[citation needed] Their work led to the design and construction of several tomographic instruments at the University of Pennsylvania. Tomographic imaging techniques were further developed at the Washington University School of Medicine. These innovations led to fusion imaging with SPECT and CT by Bruce Hasegawa from University of California, San Francisco (UCSF), and the first PET/CT prototype by D. W. Townsend from University of Pittsburgh in 1998.[citation needed]

PET and PET/CT imaging experienced slower growth in its early years owing to the cost of the modality and the requirement for an on-site or nearby cyclotron. However, an administrative decision to approve medical reimbursement of limited PET and PET/CT applications in oncology has led to phenomenal growth and widespread acceptance over the last few years, which also was facilitated by establishing 18F-labelled tracers for standard procedures, allowing work at non-cyclotron-equipped sites. PET/CT imaging is now an integral part of oncology for diagnosis, staging and treatment monitoring. A fully integrated MRI/PET scanner is on the market from early 2011.[citation needed]

Sources of radionuclides
See also: Radiopharmacology
99mTc is normally supplied to hospitals through a radionuclide generator containing the parent radionuclide molybdenum-99. 99Mo is typically obtained as a fission product of 235U in nuclear reactors, however global supply shortages have led to the exploration of other methods of production. About a third of the world's supply, and most of Europe's supply, of medical isotopes is produced at the Petten nuclear reactor in the Netherlands. Another third of the world's supply, and most of North America's supply, was produced at the Chalk River Laboratories in Chalk River, Ontario, Canada until its permanent shutdown in 2018.[17]

The most commonly used radioisotope in PET, 18F, is not produced in a nuclear reactor, but rather in a circular accelerator called a cyclotron. The cyclotron is used to accelerate protons to bombard the stable heavy isotope of oxygen 18O. The 18O constitutes about 0.20% of ordinary oxygen (mostly oxygen-16), from which it is extracted. The 18F is then typically used to make FDG.

Common isotopes used in nuclear medicine [18][19][20]
isotope	symbol	Z	T1/2	decay	gamma (keV)	Maximum β
energy (keV) /

Abundance[21]

Imaging:
fluorine-18	18F	9	109.77 m	β+	511 (193%)	634 (97%)
gallium-67	67Ga	31	3.26 d	ec	93 (39%),
185 (21%),
300 (17%)	-
krypton-81m	81mKr	36	13.1 s	IT	190 (68%)	-
rubidium-82	82Rb	37	1.27 m	β+	511 (191%)	3381 (81.8%)
2605 (13.1%)

1906 (0.14%)

1209 (0.32%)

nitrogen-13	13N	7	9.97 m	β+	511 (200%)	1198 (99.8%)
technetium-99m	99mTc	43	6.01 h	IT	140 (89%)	-
indium-111	111In	49	2.80 d	ec	171 (90%),
245 (94%)	-
iodine-123	123I	53	13.3 h	ec	159 (83%)	-
xenon-133	133Xe	54	5.24 d	β−	81 (31%)	346 (99.1%)
267 (0.9%)

thallium-201	201Tl	81	3.04 d	ec	69–83* (94%),
167 (10%)	-
Therapy:
yttrium-90	90Y	39	2.67 d	β−	-	2279 (99.98%)
iodine-131	131I	53	8.02 d	β−	364 (81%)	807 (0.4%)
606 (89.4%)

334 (7.2%)

248 (2.1%)

lutetium-177	177Lu	71	6.65 d	β−	113 (6.6%),
208 (11%)

498 (79.3%)
385 (9.1%)

177 (11.6%)

Z = atomic number, the number of protons; T1/2 = half-life; decay = mode of decay
photons = principle photon energies in kilo-electron volts, keV, (abundance/decay)
β = beta maximum energy in kilo-electron volts, keV, (abundance/decay)
β+ = β+ decay; β− = β− decay; IT = isomeric transition; ec = electron capture
* X-rays from progeny, mercury, Hg

A typical nuclear medicine study involves administration of a radionuclide into the body by intravenous injection in liquid or aggregate form, ingestion while combined with food, inhalation as a gas or aerosol, or rarely, injection of a radionuclide that has undergone micro-encapsulation. Some studies require the labeling of a patient's own blood cells with a radionuclide (leukocyte scintigraphy and red blood cell scintigraphy). Most diagnostic radionuclides emit gamma rays either directly from their decay or indirectly through electron–positron annihilation, while the cell-damaging properties of beta particles are used in therapeutic applications. Refined radionuclides for use in nuclear medicine are derived from fission or fusion processes in nuclear reactors, which produce radionuclides with longer half-lives, or cyclotrons, which produce radionuclides with shorter half-lives, or take advantage of natural decay processes in dedicated generators, i.e. molybdenum/technetium or strontium/rubidium.

The most commonly used intravenous radionuclides are technetium-99m, iodine-123, iodine-131, thallium-201, gallium-67, fluorine-18 fluorodeoxyglucose, and indium-111 labeled leukocytes.[citation needed] The most commonly used gaseous/aerosol radionuclides are xenon-133, krypton-81m, (aerosolised) technetium-99m.[22]

Policies and procedures
Radiation dose
A patient undergoing a nuclear medicine procedure will receive a radiation dose. Under present international guidelines it is assumed that any radiation dose, however small, presents a risk. The radiation dose delivered to a patient in a nuclear medicine investigation, though unproven, is generally accepted to present a very small risk of inducing cancer. In this respect it is similar to the risk from X-ray investigations except that the dose is delivered internally rather than from an external source such as an X-ray machine, and dosage amounts are typically significantly higher than those of X-rays.

The radiation dose from a nuclear medicine investigation is expressed as an effective dose with units of sieverts (usually given in millisieverts, mSv). The effective dose resulting from an investigation is influenced by the amount of radioactivity administered in megabecquerels (MBq), the physical properties of the radiopharmaceutical used, its distribution in the body and its rate of clearance from the body.

Effective doses can range from 6 μSv (0.006 mSv) for a 3 MBq chromium-51 EDTA measurement of glomerular filtration rate to 11.2 mSv (11,200 μSv) for a 80 MBq thallium-201 myocardial imaging procedure. The common bone scan with 600 MBq of technetium-99m MDP has an effective dose of approximately 2.9 mSv (2,900 μSv).[23]

Formerly, units of measurement were the curie (Ci), being 3.7E10 Bq, and also 1.0 grams of Radium (Ra-226); the rad (radiation absorbed dose), now replaced by the gray; and the rem (Röntgen equivalent man), now replaced with the sievert.[24] The rad and rem are essentially equivalent for almost all nuclear medicine procedures, and only alpha radiation will produce a higher Rem or Sv value, due to its much higher Relative Biological Effectiveness (RBE). Alpha emitters are nowadays rarely used in nuclear medicine, but were used extensively before the advent of nuclear reactor and accelerator produced radionuclides. The concepts involved in radiation exposure to humans are covered by the field of Health Physics; the development and practice of safe and effective nuclear medicinal techniques is a key focus of Medical Physics.

Regulatory frameworks and guidelines
Different countries around the world maintain regulatory frameworks that are responsible for the management and use of radionuclides in different medical settings. For example, in the US, the Nuclear Regulatory Commission (NRC) and the Food and Drug Administration (FDA) have guidelines in place for hospitals to follow.[25] With the NRC, if radioactive materials aren't involved, like X-rays for example, they are not regulated by the agency and instead are regulated by the individual states.[26] International organizations, such as the International Atomic Energy Agency (IAEA), have regularly published different articles and guidelines for best practices in nuclear medicine as well as reporting on emerging technologies in nuclear medicine.[27][28] Other factors that are considered in nuclear medicine include a patient's medical history as well as post-treatment management. Groups like International Commission on Radiological Protection have published information on how to manage the release of patients from a hospital with unsealed radionuclides
Radiation therapy or radiotherapy (RT, RTx, or XRT) is a treatment using ionizing radiation, generally provided as part of cancer therapy to either kill or control the growth of malignant cells. It is normally delivered by a linear particle accelerator. Radiation therapy may be curative in a number of types of cancer if they are localized to one area of the body, and have not spread to other parts. It may also be used as part of adjuvant therapy, to prevent tumor recurrence after surgery to remove a primary malignant tumor (for example, early stages of breast cancer). Radiation therapy is synergistic with chemotherapy, and has been used before, during, and after chemotherapy in susceptible cancers. The subspecialty of oncology concerned with radiotherapy is called radiation oncology. A physician who practices in this subspecialty is a radiation oncologist.

Radiation therapy is commonly applied to the cancerous tumor because of its ability to control cell growth. Ionizing radiation works by damaging the DNA of cancerous tissue leading to cellular death. To spare normal tissues (such as skin or organs which radiation must pass through to treat the tumor), shaped radiation beams are aimed from several angles of exposure to intersect at the tumor, providing a much larger absorbed dose there than in the surrounding healthy tissue. Besides the tumor itself, the radiation fields may also include the draining lymph nodes if they are clinically or radiologically involved with the tumor, or if there is thought to be a risk of subclinical malignant spread. It is necessary to include a margin of normal tissue around the tumor to allow for uncertainties in daily set-up and internal tumor motion. These uncertainties can be caused by internal movement (for example, respiration and bladder filling) and movement of external skin marks relative to the tumor position.

Radiation oncology is the medical specialty concerned with prescribing radiation, and is distinct from radiology, the use of radiation in medical imaging and diagnosis. Radiation may be prescribed by a radiation oncologist with intent to cure or for adjuvant therapy. It may also be used as palliative treatment (where cure is not possible and the aim is for local disease control or symptomatic relief) or as therapeutic treatment (where the therapy has survival benefit and can be curative).[1] It is also common to combine radiation therapy with surgery, chemotherapy, hormone therapy, immunotherapy or some mixture of the four. Most common cancer types can be treated with radiation therapy in some way.

The precise treatment intent (curative, adjuvant, neoadjuvant therapeutic, or palliative) will depend on the tumor type, location, and stage, as well as the general health of the patient. Total body irradiation (TBI) is a radiation therapy technique used to prepare the body to receive a bone marrow transplant. Brachytherapy, in which a radioactive source is placed inside or next to the area requiring treatment, is another form of radiation therapy that minimizes exposure to healthy tissue during procedures to treat cancers of the breast, prostate, and other organs. Radiation therapy has several applications in non-malignant conditions, such as the treatment of trigeminal neuralgia, acoustic neuromas, severe thyroid eye disease, pterygium, pigmented villonodular synovitis, and prevention of keloid scar growth, vascular restenosis, and heterotopic ossification.[1][2][3][4] The use of radiation therapy in non-malignant conditions is limited partly by worries about the risk of radiation-induced cancers.

Medical uses

Radiation therapy for a patient with a diffuse intrinsic pontine glioma, with radiation dose color-coded
It is estimated that half of the US' 1.2M invasive cancer cases diagnosed in 2022 received radiation therapy in their treatment program.[5] Different cancers respond to radiation therapy in different ways.[6][7][8]

The response of a cancer to radiation is described by its radiosensitivity. Highly radiosensitive cancer cells are rapidly killed by modest doses of radiation. These include leukemias, most lymphomas, and germ cell tumors. The majority of epithelial cancers are only moderately radiosensitive, and require a significantly higher dose of radiation (60–70 Gy) to achieve a radical cure. Some types of cancer are notably radioresistant, that is, much higher doses are required to produce a radical cure than may be safe in clinical practice. Renal cell cancer and melanoma are generally considered to be radioresistant but radiation therapy is still a palliative option for many patients with metastatic melanoma. Combining radiation therapy with immunotherapy is an active area of investigation and has shown some promise for melanoma and other cancers.[9]

It is important to distinguish the radiosensitivity of a particular tumor, which to some extent is a laboratory measure, from the radiation "curability" of a cancer in actual clinical practice. For example, leukemias are not generally curable with radiation therapy, because they are disseminated through the body. Lymphoma may be radically curable if it is localised to one area of the body. Similarly, many of the common, moderately radioresponsive tumors are routinely treated with curative doses of radiation therapy if they are at an early stage. For example, non-melanoma skin cancer, head and neck cancer, breast cancer, non-small cell lung cancer, cervical cancer, anal cancer, and prostate cancer. With the exception of oligometastatic disease, metastatic cancers are incurable with radiation therapy because it is not possible to treat the whole body.

Modern radiation therapy relies on a CT scan to identify the tumor and surrounding normal structures and to perform dose calculations for the creation of a complex radiation treatment plan. The patient receives small skin marks to guide the placement of treatment fields.[10] Patient positioning is crucial at this stage as the patient will have to be placed in an identical position during each treatment. Many patient positioning devices have been developed for this purpose, including masks and cushions which can be molded to the patient. Image-guided radiation therapy is a method that uses imaging to correct for positional errors of each treatment session.

The response of a tumor to radiation therapy is also related to its size. Due to complex radiobiology, very large tumors respond less well to radiation than smaller tumors or microscopic disease. Various strategies are used to overcome this effect. The most common technique is surgical resection prior to radiation therapy. This is most commonly seen in the treatment of breast cancer with wide local excision or mastectomy followed by adjuvant radiation therapy. Another method is to shrink the tumor with neoadjuvant chemotherapy prior to radical radiation therapy. A third technique is to enhance the radiosensitivity of the cancer by giving certain drugs during a course of radiation therapy. Examples of radiosensitizing drugs include cisplatin, nimorazole, and cetuximab.[11]

The impact of radiotherapy varies between different types of cancer and different groups.[12] For example, for breast cancer after breast-conserving surgery, radiotherapy has been found to halve the rate at which the disease recurs.[13] In pancreatic cancer, radiotherapy has increased survival times for inoperable tumors.[14]

Side effects
Radiation therapy (RT) is in itself painless. Many low-dose palliative treatments (for example, radiation therapy to bony metastases) cause minimal or no side effects, although short-term pain flare-up can be experienced in the days following treatment due to oedema compressing nerves in the treated area. Higher doses can cause varying side effects during treatment (acute side effects), in the months or years following treatment (long-term side effects), or after re-treatment (cumulative side effects). The nature, severity, and longevity of side effects depends on the organs that receive the radiation, the treatment itself (type of radiation, dose, fractionation, concurrent chemotherapy), and the patient. Serious radiation complications may occur in 5% of RT cases. Acute (near immediate) or sub-acute (2 to 3 months post RT) radiation side effects may develop after 50 Gy RT dosing. Late or delayed radiation injury (6 months to decades) may develop after 65 Gy.[5]

Most side effects are predictable and expected. Side effects from radiation are usually limited to the area of the patient's body that is under treatment. Side effects are dose-dependent; for example, higher doses of head and neck radiation can be associated with cardiovascular complications, thyroid dysfunction, and pituitary axis dysfunction.[15] Modern radiation therapy aims to reduce side effects to a minimum and to help the patient understand and deal with side effects that are unavoidable.

The main side effects reported are fatigue and skin irritation, like a mild to moderate sun burn. The fatigue often sets in during the middle of a course of treatment and can last for weeks after treatment ends. The irritated skin will heal, but may not be as elastic as it was before.[16]

Acute side effects
Nausea and vomiting
This is not a general side effect of radiation therapy, and mechanistically is associated only with treatment of the stomach or abdomen (which commonly react a few hours after treatment), or with radiation therapy to certain nausea-producing structures in the head during treatment of certain head and neck tumors, most commonly the vestibules of the inner ears.[17] As with any distressing treatment, some patients vomit immediately during radiotherapy, or even in anticipation of it, but this is considered a psychological response. Nausea for any reason can be treated with antiemetics.[18]
Damage to the epithelial surfaces
Epithelial surfaces may sustain damage from radiation therapy.[19] Depending on the area being treated, this may include the skin, oral mucosa, pharyngeal, bowel mucosa, and ureter. The rates of onset of damage and recovery from it depend upon the turnover rate of epithelial cells. Typically the skin starts to become pink and sore several weeks into treatment. The reaction may become more severe during the treatment and for up to about one week following the end of radiation therapy, and the skin may break down. Although this moist desquamation is uncomfortable, recovery is usually quick. Skin reactions tend to be worse in areas where there are natural folds in the skin, such as underneath the female breast, behind the ear, and in the groin.
Mouth, throat and stomach sores
If the head and neck area is treated, temporary soreness and ulceration commonly occur in the mouth and throat.[20] If severe, this can affect swallowing, and the patient may need painkillers and nutritional support/food supplements. The esophagus can also become sore if it is treated directly, or if, as commonly occurs, it receives a dose of collateral radiation during treatment of lung cancer. When treating liver malignancies and metastases, it is possible for collateral radiation to cause gastric, stomach, or duodenal ulcers[21][22] This collateral radiation is commonly caused by non-targeted delivery (reflux) of the radioactive agents being infused.[23] Methods, techniques and devices are available to lower the occurrence of this type of adverse side effect.[24]
Intestinal discomfort
The lower bowel may be treated directly with radiation (treatment of rectal or anal cancer) or be exposed by radiation therapy to other pelvic structures (prostate, bladder, female genital tract). Typical symptoms are soreness, diarrhoea, and nausea. Nutritional interventions may be able to help with diarrhoea associated with radiotherapy.[25] Studies in people having pelvic radiotherapy as part of anticancer treatment for a primary pelvic cancer found that changes in dietary fat, fibre and lactose during radiotherapy reduced diarrhoea at the end of treatment.[25]
Swelling
As part of the general inflammation that occurs, swelling of soft tissues may cause problems during radiation therapy. This is a concern during treatment of brain tumors and brain metastases, especially where there is pre-existing raised intracranial pressure or where the tumor is causing near-total obstruction of a lumen (e.g., trachea or main bronchus). Surgical intervention may be considered prior to treatment with radiation. If surgery is deemed unnecessary or inappropriate, the patient may receive steroids during radiation therapy to reduce swelling.
Infertility
The gonads (ovaries and testicles) are very sensitive to radiation. They may be unable to produce gametes following direct exposure to most normal treatment doses of radiation. Treatment planning for all body sites is designed to minimize, if not completely exclude dose to the gonads if they are not the primary area of treatment.
Late side effects
Late side effects occur months to years after treatment and are generally limited to the area that has been treated. They are often due to damage of blood vessels and connective tissue cells. Many late effects are reduced by fractionating treatment into smaller parts.

Fibrosis
Tissues which have been irradiated tend to become less elastic over time due to a diffuse scarring process.
Epilation
Epilation (hair loss) may occur on any hair bearing skin with doses above 1 Gy. It only occurs within the radiation field/s. Hair loss may be permanent with a single dose of 10 Gy, but if the dose is fractionated permanent hair loss may not occur until dose exceeds 45 Gy.
Dryness
The salivary glands and tear glands have a radiation tolerance of about 30 Gy in 2 Gy fractions, a dose which is exceeded by most radical head and neck cancer treatments. Dry mouth (xerostomia) and dry eyes (xerophthalmia) can become irritating long-term problems and severely reduce the patient's quality of life. Similarly, sweat glands in treated skin (such as the armpit) tend to stop working, and the naturally moist vaginal mucosa is often dry following pelvic irradiation.
Chronic sinus drainage
Radiation therapy treatments to the head and neck regions for soft tissue, palate or bone cancer can cause chronic sinus tract draining and fistulae from the bone.[5]
Lymphedema
Lymphedema, a condition of localized fluid retention and tissue swelling, can result from damage to the lymphatic system sustained during radiation therapy. It is the most commonly reported complication in breast radiation therapy patients who receive adjuvant axillary radiotherapy following surgery to clear the axillary lymph nodes .[26]
Cancer
Radiation is a potential cause of cancer, and secondary malignancies are seen in some patients. Cancer survivors are already more likely than the general population to develop malignancies due to a number of factors including lifestyle choices, genetics, and previous radiation treatment. It is difficult to directly quantify the rates of these secondary cancers from any single cause. Studies have found radiation therapy as the cause of secondary malignancies for only a small minority of patients.[27][28] New techniques such as proton beam therapy and carbon ion radiotherapy which aim to reduce dose to healthy tissues will lower these risks.[29][30] It starts to occur 4–6 years following treatment, although some haematological malignancies may develop within 3 years. In the vast majority of cases, this risk is greatly outweighed by the reduction in risk conferred by treating the primary cancer even in pediatric malignancies which carry a higher burden of secondary malignancies.[31]
Cardiovascular disease
Radiation can increase the risk of heart disease and death as observed in previous breast cancer RT regimens.[32] Therapeutic radiation increases the risk of a subsequent cardiovascular event (i.e., heart attack or stroke) by 1.5 to 4 times a person's normal rate, aggravating factors included.[33] The increase is dose dependent, related to the RT's dose strength, volume and location. Use of concomitant chemotherapy, e.g. anthracyclines, is an aggravating risk factor.[34] The occurrence rate of RT induced cardiovascular disease is estimated between 10 and 30%.[34]
Cardiovascular late side effects have been termed radiation-induced heart disease (RIHD) and radiation-induced cardiovascular disease (RIVD).[35][36] Symptoms are dose dependent and include cardiomyopathy, myocardial fibrosis, valvular heart disease, coronary artery disease, heart arrhythmia and peripheral artery disease. Radiation-induced fibrosis, vascular cell damage and oxidative stress can lead to these and other late side effect symptoms.[35] Most radiation-induced cardiovascular diseases occur 10 or more years post treatment, making causality determinations more difficult.[33]
Cognitive decline
In cases of radiation applied to the head radiation therapy may cause cognitive decline. Cognitive decline was especially apparent in young children, between the ages of 5 and 11. Studies found, for example, that the IQ of 5-year-old children declined each year after treatment by several IQ points.[37]
Radiation enteropathy

Histopathology of radiation cystitis, including atypical stromal cells ("radiation fibroblasts")
The gastrointestinal tract can be damaged following abdominal and pelvic radiotherapy.[38] Atrophy, fibrosis and vascular changes produce malabsorption, diarrhea, steatorrhea and bleeding with bile acid diarrhea and vitamin B12 malabsorption commonly found due to ileal involvement. Pelvic radiation disease includes radiation proctitis, producing bleeding, diarrhoea and urgency,[39] and can also cause radiation cystitis when the bladder is affected.
Radiation-induced polyneuropathy
Radiation treatments may damage nerves near the target area or within the delivery path as nerve tissue is also radiosensitive.[40] Nerve damage from ionizing radiation occurs in phases, the initial phase from microvascular injury, capillary damage and nerve demyelination.[41] Subsequent damage occurs from vascular constriction and nerve compression due to uncontrolled fibrous tissue growth caused by radiation.[41] Radiation-induced polyneuropathy, ICD-10-CM Code G62.82, occurs in approximately 1–5% of those receiving radiation therapy.[41][40]
Depending upon the irradiated zone, late effect neuropathy may occur in either the central nervous system (CNS) or the peripheral nervous system (PNS). In the CNS for example, cranial nerve injury typically presents as a visual acuity loss 1–14 years post treatment.[41] In the PNS, injury to the plexus nerves presents as radiation-induced brachial plexopathy or radiation-induced lumbosacral plexopathy appearing up to 3 decades post treatment.[41]
Radiation necrosis
Radiation necrosis is the death of healthy tissue near the irradiated site. It is a type of coagulative necrosis that occurs because the radiation directly or indirectly damages blood vessels in the area, which reduces the blood supply to the remaining healthy tissue, causing it to die by ischemia, similar to what happens in an ischemic stroke.[42] Because it is an indirect effect of the treatment, it occurs months to decades after radiation exposure.[42] Radiation necrosis most commonly presents as osteoradionecrosis, vaginal radionecrosis, soft tissue radionecrosis, or laryngeal radionecrosis.[5]
Cumulative side effects
Cumulative effects from this process should not be confused with long-term effects – when short-term effects have disappeared and long-term effects are subclinical, reirradiation can still be problematic.[43] These doses are calculated by the radiation oncologist and many factors are taken into account before the subsequent radiation takes place.

Effects on reproduction
During the first two weeks after fertilization, radiation therapy is lethal but not teratogenic.[44] High doses of radiation during pregnancy induce anomalies, impaired growth and intellectual disability, and there may be an increased risk of childhood leukemia and other tumors in the offspring.[44]

In males previously having undergone radiotherapy, there appears to be no increase in genetic defects or congenital malformations in their children conceived after therapy.[44] However, the use of assisted reproductive technologies and micromanipulation techniques might increase this risk.[44]

Effects on pituitary system
Hypopituitarism commonly develops after radiation therapy for sellar and parasellar neoplasms, extrasellar brain tumors, head and neck tumors, and following whole body irradiation for systemic malignancies.[45] 40–50% of children treated for childhood cancer develop some endocrine side effect.[46] Radiation-induced hypopituitarism mainly affects growth hormone and gonadal hormones.[45] In contrast, adrenocorticotrophic hormone (ACTH) and thyroid stimulating hormone (TSH) deficiencies are the least common among people with radiation-induced hypopituitarism.[45] Changes in prolactin-secretion is usually mild, and vasopressin deficiency appears to be very rare as a consequence of radiation.[45]

Effects on subsequent surgery
Delayed tissue injury with impaired wound healing capability often develops after receiving doses in excess of 65 Gy. A diffuse injury pattern due to the external beam radiotherapy's holographic isodosing occurs. While the targeted tumor receives the majority of radiation, healthy tissue at incremental distances from the center of the tumor are also irradiated in a diffuse pattern due to beam divergence. These wounds demonstrate progressive, proliferative endarteritis, inflamed arterial linings that disrupt the tissue's blood supply. Such tissue ends up chronically hypoxic, fibrotic, and without an adequate nutrient and oxygen supply. Surgery of previously irradiated tissue has a very high failure rate, e.g. women who have received radiation for breast cancer develop late effect chest wall tissue fibrosis and hypovascularity, making successful reconstruction and healing difficult, if not impossible.[5]

Radiation therapy accidents
There are rigorous procedures in place to minimise the risk of accidental overexposure of radiation therapy to patients. However, mistakes do occasionally occur; for example, the radiation therapy machine Therac-25 was responsible for at least six accidents between 1985 and 1987, where patients were given up to one hundred times the intended dose; two people were killed directly by the radiation overdoses. From 2005 to 2010, a hospital in Missouri overexposed 76 patients (most with brain cancer) during a five-year period because new radiation equipment had been set up incorrectly.[47]

Although medical errors are exceptionally rare, radiation oncologists, medical physicists and other members of the radiation therapy treatment team are working to eliminate them. In 2010 the American Society for Radiation Oncology (ASTRO) launched a safety initiative called Target Safely that, among other things, aimed to record errors nationwide so that doctors can learn from each and every mistake and prevent them from recurring. ASTRO also publishes a list of questions for patients to ask their doctors about radiation safety to ensure every treatment is as safe as possible.[48]

Use in non-cancerous diseases

The beam's eye view of the radiotherapy portal on the hand's surface with the lead shield cut-out placed in the machine's gantry
Radiation therapy is used to treat early stage Dupuytren's disease and Ledderhose disease. When Dupuytren's disease is at the nodules and cords stage or fingers are at a minimal deformation stage of less than 10 degrees, then radiation therapy is used to prevent further progress of the disease. Radiation therapy is also used post surgery in some cases to prevent the disease continuing to progress. Low doses of radiation are used typically three gray of radiation for five days, with a break of three months followed by another phase of three gray of radiation for five days.[49]

Technique
Mechanism of action
Radiation therapy works by damaging the DNA of cancer cells and can cause them to undergo mitotic catastrophe.[50] This DNA damage is caused by one of two types of energy, photon or charged particle. This damage is either direct or indirect ionization of the atoms which make up the DNA chain. Indirect ionization happens as a result of the ionization of water, forming free radicals, notably hydroxyl radicals, which then damage the DNA.

In photon therapy, most of the radiation effect is through free radicals. Cells have mechanisms for repairing single-strand DNA damage and double-stranded DNA damage. However, double-stranded DNA breaks are much more difficult to repair, and can lead to dramatic chromosomal abnormalities and genetic deletions. Targeting double-stranded breaks increases the probability that cells will undergo cell death. Cancer cells are generally less differentiated and more stem cell-like; they reproduce more than most healthy differentiated cells, and have a diminished ability to repair sub-lethal damage. Single-strand DNA damage is then passed on through cell division; damage to the cancer cells' DNA accumulates, causing them to die or reproduce more slowly.

One of the major limitations of photon radiation therapy is that the cells of solid tumors become deficient in oxygen. Solid tumors can outgrow their blood supply, causing a low-oxygen state known as hypoxia. Oxygen is a potent radiosensitizer, increasing the effectiveness of a given dose of radiation by forming DNA-damaging free radicals. Tumor cells in a hypoxic environment may be as much as 2 to 3 times more resistant to radiation damage than those in a normal oxygen environment.[51] Much research has been devoted to overcoming hypoxia including the use of high pressure oxygen tanks, hyperthermia therapy (heat therapy which dilates blood vessels to the tumor site), blood substitutes that carry increased oxygen, hypoxic cell radiosensitizer drugs such as misonidazole and metronidazole, and hypoxic cytotoxins (tissue poisons), such as tirapazamine. Newer research approaches are currently being studied, including preclinical and clinical investigations into the use of an oxygen diffusion-enhancing compound such as trans sodium crocetinate as a radiosensitizer.[52]

Charged particles such as protons and boron, carbon, and neon ions can cause direct damage to cancer cell DNA through high-LET (linear energy transfer) and have an antitumor effect independent of tumor oxygen supply because these particles act mostly via direct energy transfer usually causing double-stranded DNA breaks. Due to their relatively large mass, protons and other charged particles have little lateral side scatter in the tissue – the beam does not broaden much, stays focused on the tumor shape, and delivers small dose side-effects to surrounding tissue. They also more precisely target the tumor using the Bragg peak effect. See proton therapy for a good example of the different effects of intensity-modulated radiation therapy (IMRT) vs. charged particle therapy. This procedure reduces damage to healthy tissue between the charged particle radiation source and the tumor and sets a finite range for tissue damage after the tumor has been reached. In contrast, IMRT's use of uncharged particles causes its energy to damage healthy cells when it exits the body. This exiting damage is not therapeutic, can increase treatment side effects, and increases the probability of secondary cancer induction.[53] This difference is very important in cases where the close proximity of other organs makes any stray ionization very damaging (example: head and neck cancers). This X-ray exposure is especially bad for children, due to their growing bodies, and while depending on a multitude of factors, they are around 10 times more sensitive to developing secondary malignancies after radiotherapy as compared to adults.[54]

Dose
The amount of radiation used in photon radiation therapy is measured in grays (Gy), and varies depending on the type and stage of cancer being treated. For curative cases, the typical dose for a solid epithelial tumor ranges from 60 to 80 Gy, while lymphomas are treated with 20 to 40 Gy.

Preventive (adjuvant) doses are typically around 45–60 Gy in 1.8–2 Gy fractions (for breast, head, and neck cancers.) Many other factors are considered by radiation oncologists when selecting a dose, including whether the patient is receiving chemotherapy, patient comorbidities, whether radiation therapy is being administered before or after surgery, and the degree of success of surgery.

Delivery parameters of a prescribed dose are determined during treatment planning (part of dosimetry). Treatment planning is generally performed on dedicated computers using specialized treatment planning software. Depending on the radiation delivery method, several angles or sources may be used to sum to the total necessary dose. The planner will try to design a plan that delivers a uniform prescription dose to the tumor and minimizes dose to surrounding healthy tissues.

In radiation therapy, three-dimensional dose distributions may be evaluated using the dosimetry technique known as gel dosimetry.[55]

Fractionation
This section only applies to photon radiotherapy although other types of radiation therapy may be fractionated
Main article: Dose fractionation
The total dose is fractionated (spread out over time) for several important reasons. Fractionation allows normal cells time to recover, while tumor cells are generally less efficient in repair between fractions. Fractionation also allows tumor cells that were in a relatively radio-resistant phase of the cell cycle during one treatment to cycle into a sensitive phase of the cycle before the next fraction is given. Similarly, tumor cells that were chronically or acutely hypoxic (and therefore more radioresistant) may reoxygenate between fractions, improving the tumor cell kill.[56]

Fractionation regimens are individualised between different radiation therapy centers and even between individual doctors. In North America, Australia, and Europe, the typical fractionation schedule for adults is 1.8 to 2 Gy per day, five days a week. In some cancer types, prolongation of the fraction schedule over too long can allow for the tumor to begin repopulating, and for these tumor types, including head-and-neck and cervical squamous cell cancers, radiation treatment is preferably completed within a certain amount of time. For children, a typical fraction size may be 1.5 to 1.8 Gy per day, as smaller fraction sizes are associated with reduced incidence and severity of late-onset side effects in normal tissues.

In some cases, two fractions per day are used near the end of a course of treatment. This schedule, known as a concomitant boost regimen or hyperfractionation, is used on tumors that regenerate more quickly when they are smaller. In particular, tumors in the head-and-neck demonstrate this behavior.

Patients receiving palliative radiation to treat uncomplicated painful bone metastasis should not receive more than a single fraction of radiation.[57] A single treatment gives comparable pain relief and morbidity outcomes to multiple-fraction treatments, and for patients with limited life expectancy, a single treatment is best to improve patient comfort.[57]

Schedules for fractionation
One fractionation schedule that is increasingly being used and continues to be studied is hypofractionation. This is a radiation treatment in which the total dose of radiation is divided into large doses. Typical doses vary significantly by cancer type, from 2.2 Gy/fraction to 20 Gy/fraction, the latter being typical of stereotactic treatments (stereotactic ablative body radiotherapy, or SABR – also known as SBRT, or stereotactic body radiotherapy) for subcranial lesions, or SRS (stereotactic radiosurgery) for intracranial lesions. The rationale of hypofractionation is to reduce the probability of local recurrence by denying clonogenic cells the time they require to reproduce and also to exploit the radiosensitivity of some tumors.[58] In particular, stereotactic treatments are intended to destroy clonogenic cells by a process of ablation, i.e., the delivery of a dose intended to destroy clonogenic cells directly, rather than to interrupt the process of clonogenic cell division repeatedly (apoptosis), as in routine radiotherapy.

Estimation of dose based on target sensitivity
Different cancer types have different radiation sensitivity. While predicting the sensitivity based on genomic or proteomic analyses of biopsy samples has proven challenging,[59][60] the predictions of radiation effect on individual patients from genomic signatures of intrinsic cellular radiosensitivity have been shown to associate with clinical outcome.[61] An alternative approach to genomics and proteomics was offered by the discovery that radiation protection in microbes is offered by non-enzymatic complexes of manganese and small organic metabolites.[62] The content and variation of manganese (measurable by electron paramagnetic resonance) were found to be good predictors of radiosensitivity, and this finding extends also to human cells.[63] An association was confirmed between total cellular manganese contents and their variation, and clinically inferred radioresponsiveness in different tumor cells, a finding that may be useful for more precise radiodosages and improved treatment of cancer patients.[64]

Types
Historically, the three main divisions of radiation therapy are:

external beam radiation therapy (EBRT or XRT) or teletherapy;
brachytherapy or sealed source radiation therapy; and
systemic radioisotope therapy or unsealed source radiotherapy.
The differences relate to the position of the radiation source; external is outside the body, brachytherapy uses sealed radioactive sources placed precisely in the area under treatment, and systemic radioisotopes are given by infusion or oral ingestion. Brachytherapy can use temporary or permanent placement of radioactive sources. The temporary sources are usually placed by a technique called afterloading. In afterloading a hollow tube or applicator is placed surgically in the organ to be treated, and the sources are loaded into the applicator after the applicator is implanted. This minimizes radiation exposure to health care personnel.

Particle therapy is a special case of external beam radiation therapy where the particles are protons or heavier ions.

A review of radiation therapy randomised clinical trials from 2018 to 2021 found many practice-changing data and new concepts that emerge from RCTs, identifying techniques that improve the therapeutic ratio, techniques that lead to more tailored treatments, stressing the importance of patient satisfaction, and identifying areas that require further study.[65][66]

External beam radiation therapy
Main article: External beam radiation therapy
The following three sections refer to treatment using X-rays.

Conventional external beam radiation therapy

A teletherapy radiation capsule composed of the following:
an international standard source holder (usually lead),
a retaining ring, and
a teletherapy "source" composed of
two nested stainless steel canisters welded to
two stainless steel lids surrounding
a protective internal shield (usually uranium metal or a tungsten alloy) and
a cylinder of radioactive source material, often but not always cobalt-60. The diameter of the "source" is 30 mm.
Historically conventional external beam radiation therapy (2DXRT) was delivered via two-dimensional beams using kilovoltage therapy X-ray units, medical linear accelerators that generate high-energy X-rays, or with machines that were similar to a linear accelerator in appearance, but used a sealed radioactive source like the one shown above.[67][68] 2DXRT mainly consists of a single beam of radiation delivered to the patient from several directions: often front or back, and both sides.

Conventional refers to the way the treatment is planned or simulated on a specially calibrated diagnostic X-ray machine known as a simulator because it recreates the linear accelerator actions (or sometimes by eye), and to the usually well-established arrangements of the radiation beams to achieve a desired plan. The aim of simulation is to accurately target or localize the volume which is to be treated. This technique is well established and is generally quick and reliable. The worry is that some high-dose treatments may be limited by the radiation toxicity capacity of healthy tissues which lie close to the target tumor volume.

An example of this problem is seen in radiation of the prostate gland, where the sensitivity of the adjacent rectum limited the dose which could be safely prescribed using 2DXRT planning to such an extent that tumor control may not be easily achievable. Prior to the invention of the CT, physicians and physicists had limited knowledge about the true radiation dosage delivered to both cancerous and healthy tissue. For this reason, 3-dimensional conformal radiation therapy has become the standard treatment for almost all tumor sites. More recently other forms of imaging are used including MRI, PET, SPECT and Ultrasound.[69]

Stereotactic radiation
Main article: Radiosurgery
Stereotactic radiation is a specialized type of external beam radiation therapy. It uses focused radiation beams targeting a well-defined tumor using extremely detailed imaging scans. Radiation oncologists perform stereotactic treatments, often with the help of a neurosurgeon for tumors in the brain or spine.

There are two types of stereotactic radiation. Stereotactic radiosurgery (SRS) is when doctors use a single or several stereotactic radiation treatments of the brain or spine. Stereotactic body radiation therapy (SBRT) refers to one or several stereotactic radiation treatments with the body, such as the lungs.[70]

Some doctors say an advantage to stereotactic treatments is that they deliver the right amount of radiation to the cancer in a shorter amount of time than traditional treatments, which can often take 6 to 11 weeks. Plus treatments are given with extreme accuracy, which should limit the effect of the radiation on healthy tissues. One problem with stereotactic treatments is that they are only suitable for certain small tumors.

Stereotactic treatments can be confusing because many hospitals call the treatments by the name of the manufacturer rather than calling it SRS or SBRT. Brand names for these treatments include Axesse, Cyberknife, Gamma Knife, Novalis, Primatom, Synergy, X-Knife, TomoTherapy, Trilogy and Truebeam.[71] This list changes as equipment manufacturers continue to develop new, specialized technologies to treat cancers.

Virtual simulation, and 3-dimensional conformal radiation therapy
The planning of radiation therapy treatment has been revolutionized by the ability to delineate tumors and adjacent normal structures in three dimensions using specialized CT and/or MRI scanners and planning software.[72]

Virtual simulation, the most basic form of planning, allows more accurate placement of radiation beams than is possible using conventional X-rays, where soft-tissue structures are often difficult to assess and normal tissues difficult to protect.

An enhancement of virtual simulation is 3-dimensional conformal radiation therapy (3DCRT), in which the profile of each radiation beam is shaped to fit the profile of the target from a beam's eye view (BEV) using a multileaf collimator (MLC) and a variable number of beams. When the treatment volume conforms to the shape of the tumor, the relative toxicity of radiation to the surrounding normal tissues is reduced, allowing a higher dose of radiation to be delivered to the tumor than conventional techniques would allow.[10]

Intensity-modulated radiation therapy (IMRT)

Varian TrueBeam Linear Accelerator, used for delivering IMRT
Intensity-modulated radiation therapy (IMRT) is an advanced type of high-precision radiation that is the next generation of 3DCRT.[73] IMRT also improves the ability to conform the treatment volume to concave tumor shapes,[10] for example when the tumor is wrapped around a vulnerable structure such as the spinal cord or a major organ or blood vessel.[74] Computer-controlled X-ray accelerators distribute precise radiation doses to malignant tumors or specific areas within the tumor. The pattern of radiation delivery is determined using highly tailored computing applications to perform optimization and treatment simulation (Treatment Planning). The radiation dose is consistent with the 3-D shape of the tumor by controlling, or modulating, the radiation beam's intensity. The radiation dose intensity is elevated near the gross tumor volume while radiation among the neighboring normal tissues is decreased or avoided completely. This results in better tumor targeting, lessened side effects, and improved treatment outcomes than even 3DCRT.

3DCRT is still used extensively for many body sites but the use of IMRT is growing in more complicated body sites such as CNS, head and neck, prostate, breast, and lung. Unfortunately, IMRT is limited by its need for additional time from experienced medical personnel. This is because physicians must manually delineate the tumors one CT image at a time through the entire disease site which can take much longer than 3DCRT preparation. Then, medical physicists and dosimetrists must be engaged to create a viable treatment plan. Also, the IMRT technology has only been used commercially since the late 1990s even at the most advanced cancer centers, so radiation oncologists who did not learn it as part of their residency programs must find additional sources of education before implementing IMRT.

Proof of improved survival benefit from either of these two techniques over conventional radiation therapy (2DXRT) is growing for many tumor sites, but the ability to reduce toxicity is generally accepted. This is particularly the case for head and neck cancers in a series of pivotal trials performed by Professor Christopher Nutting of the Royal Marsden Hospital. Both techniques enable dose escalation, potentially increasing usefulness. There has been some concern, particularly with IMRT,[75] about increased exposure of normal tissue to radiation and the consequent potential for secondary malignancy. Overconfidence in the accuracy of imaging may increase the chance of missing lesions that are invisible on the planning scans (and therefore not included in the treatment plan) or that move between or during a treatment (for example, due to respiration or inadequate patient immobilization). New techniques are being developed to better control this uncertainty – for example, real-time imaging combined with real-time adjustment of the therapeutic beams. This new technology is called image-guided radiation therapy or four-dimensional radiation therapy.

Another technique is the real-time tracking and localization of one or more small implantable electric devices implanted inside or close to the tumor. There are various types of medical implantable devices that are used for this purpose. It can be a magnetic transponder which senses the magnetic field generated by several transmitting coils, and then transmits the measurements back to the positioning system to determine the location.[76] The implantable device can also be a small wireless transmitter sending out an RF signal which then will be received by a sensor array and used for localization and real-time tracking of the tumor position.[77][78]

A well-studied issue with IMRT is the "tongue and groove effect" which results in unwanted underdosing, due to irradiating through extended tongues and grooves of overlapping MLC (multileaf collimator) leaves.[79] While solutions to this issue have been developed, which either reduce the TG effect to negligible amounts or remove it completely, they depend upon the method of IMRT being used and some of them carry costs of their own.[79] Some texts distinguish "tongue and groove error" from "tongue or groove error", according as both or one side of the aperture is occluded.[80]

Volumetric modulated arc therapy (VMAT)
Volumetric modulated arc therapy (VMAT) is a radiation technique introduced in 2007[81] which can achieve highly conformal dose distributions on target volume coverage and sparing of normal tissues. The specificity of this technique is to modify three parameters during the treatment. VMAT delivers radiation by rotating gantry (usually 360° rotating fields with one or more arcs), changing speed and shape of the beam with a multileaf collimator (MLC) ("sliding window" system of moving) and fluence output rate (dose rate) of the medical linear accelerator. VMAT has an advantage in patient treatment, compared with conventional static field intensity modulated radiotherapy (IMRT), of reduced radiation delivery times.[82][83] Comparisons between VMAT and conventional IMRT for their sparing of healthy tissues and Organs at Risk (OAR) depends upon the cancer type. In the treatment of nasopharyngeal, oropharyngeal and hypopharyngeal carcinomas VMAT provides equivalent or better protection of the organ at risk (OAR).[81][82][83] In the treatment of prostate cancer the OAR protection result is mixed[81] with some studies favoring VMAT, others favoring IMRT.[84]

Temporally feathered radiation therapy (TFRT)
Temporally feathered radiation therapy (TFRT) is a radiation technique introduced in 2018[85] which aims to use the inherent non-linearities in normal tissue repair to allow for sparing of these tissues without affecting the dose delivered to the tumor. The application of this technique, which has yet to be automated, has been described carefully to enhance the ability of departments to perform it, and in 2021 it was reported as feasible in a small clinical trial,[86] though its efficacy has yet to be formally studied.

Automated planning
Automated treatment planning has become an integrated part of radiotherapy treatment planning. There are in general two approaches of automated planning. 1) Knowledge based planning where the treatment planning system has a library of high quality plans, from which it can predict the target and dose-volume histogram of the organ at risk.[87] 2) The other approach is commonly called protocol based planning, where the treatment planning system tried to mimic an experienced treatment planner and through an iterative process evaluates the plan quality from on the basis of the protocol.[88][89][90][91]

Particle therapy
Main article: Particle therapy
In particle therapy (proton therapy being one example), energetic ionizing particles (protons or carbon ions) are directed at the target tumor.[92] The dose increases while the particle penetrates the tissue, up to a maximum (the Bragg peak) that occurs near the end of the particle's range, and it then drops to (almost) zero. The advantage of this energy deposition profile is that less energy is deposited into the healthy tissue surrounding the target tissue.

Auger therapy
Main article: Auger therapy
Auger therapy (AT) makes use of a very high dose[93] of ionizing radiation in situ that provides molecular modifications at an atomic scale. AT differs from conventional radiation therapy in several aspects; it neither relies upon radioactive nuclei to cause cellular radiation damage at a cellular dimension, nor engages multiple external pencil-beams from different directions to zero-in to deliver a dose to the targeted area with reduced dose outside the targeted tissue/organ locations. Instead, the in situ delivery of a very high dose at the molecular level using AT aims for in situ molecular modifications involving molecular breakages and molecular re-arrangements such as a change of stacking structures as well as cellular metabolic functions related to the said molecule structures.

Motion compensation
In many types of external beam radiotherapy, motion can negatively impact the treatment delivery by moving target tissue out of, or other healthy tissue into, the intended beam path. Some form of patient immobilisation is common, to prevent the large movements of the body during treatment, however this cannot prevent all motion, for example as a result of breathing. Several techniques have been developed to account for motion like this.[94][95] Deep inspiration breath-hold (DIBH) is commonly used for breast treatments where it is important to avoid irradiating the heart. In DIBH the patient holds their breath after breathing in to provide a stable position for the treatment beam to be turned on. This can be done automatically using an external monitoring system such as a spirometer or a camera and markers.[96] The same monitoring techniques, as well as 4DCT imaging, can also be for respiratory gated treatment, where the patient breathes freely and the beam is only engaged at certain points in the breathing cycle.[97] Other techniques include using 4DCT imaging to plan treatments with margins that account for motion, and active movement of the treatment couch, or beam, to follow motion.[98]

Contact X-ray brachytherapy
Contact X-ray brachytherapy (also called "CXB", "electronic brachytherapy" or the "Papillon Technique") is a type of radiation therapy using kilovoltage X-rays applied close to the tumor to treat rectal cancer. The process involves inserting the X-ray tube through the anus into the rectum and placing it against the cancerous tissue, then high doses of X-rays are emitted directly into the tumor at two weekly intervals. It is typically used for treating early rectal cancer in patients who may not be candidates for surgery.[99][100][101] A 2015 NICE review found the main side effect to be bleeding that occurred in about 38% of cases, and radiation-induced ulcer which occurred in 27% of cases.[99]

Brachytherapy (sealed source radiotherapy)
Main article: Brachytherapy

A SAVI brachytherapy device
Brachytherapy is delivered by placing radiation source(s) inside or next to the area requiring treatment. Brachytherapy is commonly used as an effective treatment for cervical,[102] prostate,[103] breast,[104] and skin cancer[105] and can also be used to treat tumors in many other body sites.[106]

In brachytherapy, radiation sources are precisely placed directly at the site of the cancerous tumor. This means that the irradiation only affects a very localized area – exposure to radiation of healthy tissues further away from the sources is reduced. These characteristics of brachytherapy provide advantages over external beam radiation therapy – the tumor can be treated with very high doses of localized radiation, whilst reducing the probability of unnecessary damage to surrounding healthy tissues.[106][107] A course of brachytherapy can often be completed in less time than other radiation therapy techniques. This can help reduce the chance of surviving cancer cells dividing and growing in the intervals between each radiation therapy dose.[107]

As one example of the localized nature of breast brachytherapy, the SAVI device delivers the radiation dose through multiple catheters, each of which can be individually controlled. This approach decreases the exposure of healthy tissue and resulting side effects, compared both to external beam radiation therapy and older methods of breast brachytherapy.[108]

Radionuclide therapy
Main article: Radionuclide therapy
Radionuclide therapy (also known as systemic radioisotope therapy, radiopharmaceutical therapy, or molecular radiotherapy), is a form of targeted therapy. Targeting can be due to the chemical properties of the isotope such as radioiodine which is specifically absorbed by the thyroid gland a thousandfold better than other bodily organs. Targeting can also be achieved by attaching the radioisotope to another molecule or antibody to guide it to the target tissue. The radioisotopes are delivered through infusion (into the bloodstream) or ingestion. Examples are the infusion of metaiodobenzylguanidine (MIBG) to treat neuroblastoma, of oral iodine-131 to treat thyroid cancer or thyrotoxicosis, and of hormone-bound lutetium-177 and yttrium-90 to treat neuroendocrine tumors (peptide receptor radionuclide therapy).

Another example is the injection of radioactive yttrium-90 or holmium-166 microspheres into the hepatic artery to radioembolize liver tumors or liver metastases. These microspheres are used for the treatment approach known as selective internal radiation therapy. The microspheres are approximately 30 µm in diameter (about one-third of a human hair) and are delivered directly into the artery supplying blood to the tumors. These treatments begin by guiding a catheter up through the femoral artery in the leg, navigating to the desired target site and administering treatment. The blood feeding the tumor will carry the microspheres directly to the tumor enabling a more selective approach than traditional systemic chemotherapy. There are currently three different kinds of microspheres: SIR-Spheres, TheraSphere and QuiremSpheres.

A major use of systemic radioisotope therapy is in the treatment of bone metastasis from cancer. The radioisotopes travel selectively to areas of damaged bone, and spare normal undamaged bone. Isotopes commonly used in the treatment of bone metastasis are radium-223,[109] strontium-89 and samarium (153Sm) lexidronam.[110]

In 2002, the United States Food and Drug Administration (FDA) approved ibritumomab tiuxetan (Zevalin), which is an anti-CD20 monoclonal antibody conjugated to yttrium-90.[111] In 2003, the FDA approved the tositumomab/iodine (131I) tositumomab regimen (Bexxar), which is a combination of an iodine-131 labelled and an unlabelled anti-CD20 monoclonal antibody.[112] These medications were the first agents of what is known as radioimmunotherapy, and they were approved for the treatment of refractory non-Hodgkin's lymphoma.

Intraoperative radiotherapy
Main article: Intraoperative radiation therapy
Intraoperative radiation therapy (IORT) is applying therapeutic levels of radiation to a target area, such as a cancer tumor, while the area is exposed during surgery.[113]

Rationale
The rationale for IORT is to deliver a high dose of radiation precisely to the targeted area with minimal exposure of surrounding tissues which are displaced or shielded during the IORT. Conventional radiation techniques such as external beam radiotherapy (EBRT) following surgical removal of the tumor have several drawbacks: The tumor bed where the highest dose should be applied is frequently missed due to the complex localization of the wound cavity even when modern radiotherapy planning is used. Additionally, the usual delay between the surgical removal of the tumor and EBRT may allow a repopulation of the tumor cells. These potentially harmful effects can be avoided by delivering the radiation more precisely to the targeted tissues leading to immediate sterilization of residual tumor cells. Another aspect is that wound fluid has a stimulating effect on tumor cells. IORT was found to inhibit the stimulating effects of wound fluid.[114]

History

X-ray treatment of tuberculosis in 1910. Before the 1920s, the hazards of radiation were not understood, and it was used to treat a wide range of diseases.
Main article: History of radiation therapy
Medicine has used radiation therapy as a treatment for cancer for more than 100 years, with its earliest roots traced from the discovery of X-rays in 1895 by Wilhelm Röntgen.[115] Emil Grubbe of Chicago was possibly the first American physician to use X-rays to treat cancer, beginning in 1896.[116]

The field of radiation therapy began to grow in the early 1900s largely due to the groundbreaking work of Nobel Prize–winning scientist Marie Curie (1867–1934), who discovered the radioactive elements polonium and radium in 1898. This began a new era in medical treatment and research.[115] Through the 1920s the hazards of radiation exposure were not understood, and little protection was used. Radium was believed to have wide curative powers and radiotherapy was applied to many diseases.

Prior to World War 2, the only practical sources of radiation for radiotherapy were radium, its "emanation", radon gas, and the X-ray tube. External beam radiotherapy (teletherapy) began at the turn of the century with relatively low voltage (<150 kV) X-ray machines. It was found that while superficial tumors could be treated with low voltage X-rays, more penetrating, higher energy beams were required to reach tumors inside the body, requiring higher voltages. Orthovoltage X-rays, which used tube voltages of 200-500 kV, began to be used during the 1920s. To reach the most deeply buried tumors without exposing intervening skin and tissue to dangerous radiation doses required rays with energies of 1 MV or above, called "megavolt" radiation. Producing megavolt X-rays required voltages on the X-ray tube of 3 to 5 million volts, which required huge expensive installations. Megavoltage X-ray units were first built in the late 1930s but because of cost were limited to a few institutions. One of the first, installed at St. Bartholomew's hospital, London in 1937 and used until 1960, used a 30 foot long X-ray tube and weighed 10 tons. Radium produced megavolt gamma rays, but was extremely rare and expensive due to its low occurrence in ores. In 1937 the entire world supply of radium for radiotherapy was 50 grams, valued at £800,000, or $50 million in 2005 dollars.

The invention of the nuclear reactor in the Manhattan Project during World War 2 made possible the production of artificial radioisotopes for radiotherapy. Cobalt therapy, teletherapy machines using megavolt gamma rays emitted by cobalt-60, a radioisotope produced by irradiating ordinary cobalt metal in a reactor, revolutionized the field between the 1950s and the early 1980s. Cobalt machines were relatively cheap, robust and simple to use, although due to its 5.27 year half-life the cobalt had to be replaced about every 5 years.

Medical linear particle accelerators, developed since the 1940s, began replacing X-ray and cobalt units in the 1980s and these older therapies are now declining. The first medical linear accelerator was used at the Hammersmith Hospital in London in 1953.[68] Linear accelerators can produce higher energies, have more collimated beams, and do not produce radioactive waste with its attendant disposal problems like radioisotope therapies.

With Godfrey Hounsfield's invention of computed tomography (CT) in 1971, three-dimensional planning became a possibility and created a shift from 2-D to 3-D radiation delivery. CT-based planning allows physicians to more accurately determine the dose distribution using axial tomographic images of the patient's anatomy. The advent of new imaging technologies, including magnetic resonance imaging (MRI) in the 1970s and positron emission tomography (PET) in the 1980s, has moved radiation therapy from 3-D conformal to intensity-modulated radiation therapy (IMRT) and to image-guided radiation therapy tomotherapy. These advances allowed radiation oncologists to better see and target tumors, which have resulted in better treatment outcomes, more organ preservation and fewer side effects.[117]

While access to radiotherapy is improving globally, more than half of patients in low and middle income countries still do not have available access to the therapy as of 2017.
Nuclear warfare, also known as atomic warfare, is a military conflict or prepared political strategy that deploys nuclear weaponry. Nuclear weapons are weapons of mass destruction; in contrast to conventional warfare, nuclear warfare can produce destruction in a much shorter time and can have a long-lasting radiological result. A major nuclear exchange would likely have long-term effects, primarily from the fallout released, and could also lead to secondary effects, such as "nuclear winter",[1][2][3][4][5][6] nuclear famine, and societal collapse.[7][8][9] A global thermonuclear war with Cold War-era stockpiles, or even with the current smaller stockpiles, may lead to various scenarios including the extinction of the human species.[10]

To date, the only use of nuclear weapons in armed conflict occurred in 1945 with the American atomic bombings of Hiroshima and Nagasaki. On August 6, 1945, a uranium gun-type device (code name "Little Boy") was detonated over the Japanese city of Hiroshima. Three days later, on August 9, a plutonium implosion-type device (code name "Fat Man") was detonated over the Japanese city of Nagasaki. Together, these two bombings resulted in the deaths of approximately 200,000 people and contributed to the surrender of Japan, which occurred before any further nuclear weapons could be produced.

After World War II, nuclear weapons were also developed by the Soviet Union (1949), the United Kingdom (1952), France (1960), and the People's Republic of China (1964), which contributed to the state of conflict and extreme tension that became known as the Cold War. In 1974, India, and in 1998, Pakistan, two countries that were openly hostile toward each other, developed nuclear weapons. Israel (1960s) and North Korea (2006) are also thought to have developed stocks of nuclear weapons, though it is not known how many. The Israeli government has never admitted nor denied having nuclear weapons, although it is known to have constructed the reactor and reprocessing plant necessary for building nuclear weapons.[11] South Africa also manufactured several complete nuclear weapons in the 1980s, but subsequently became the first country to voluntarily destroy their domestically made weapons stocks and abandon further production (1990s).[12][13] Nuclear weapons have been detonated on over 2,000 occasions for testing purposes and demonstrations.[14][15]

After the dissolution of the Soviet Union in 1991 and the resultant end of the Cold War, the threat of a major nuclear war between the two nuclear superpowers was generally thought to have declined.[16] Since then, concern over nuclear weapons has shifted to the prevention of localized nuclear conflicts resulting from nuclear proliferation, and the threat of nuclear terrorism. However, the threat of nuclear war is considered to have resurged after the Russian invasion of Ukraine, particularly with regard to Russian threats to use nuclear weapons during the invasion.[17][18]

Since 1947, the Doomsday Clock of the Bulletin of the Atomic Scientists has visualized how close the world is to a nuclear war. The Doomsday Clock reached high points in 1953, when the Clock was set to two minutes until midnight after the U.S. and the Soviet Union began testing hydrogen bombs, and in 2018, following the failure of world leaders to address tensions relating to nuclear weapons and climate change issues.[19] Since 2023, the Clock has been set at 90 seconds to midnight, the closest it has ever been.[20] The most recent advance of the Clock's time setting was largely attributed to the risk of nuclear escalation that arose from the Russian invasion of Ukraine.[21]

Types of nuclear warfare
The possibility of using nuclear weapons in war is usually divided into two subgroups, each with different effects and potentially fought with different types of nuclear armaments.

The first, a limited nuclear war[22] (sometimes attack or exchange), refers to the controlled use of nuclear weapons, whereby the implicit threat exists that a nation can still escalate their use of nuclear weapons. For example, using a small number of nuclear weapons against strictly military targets could be escalated through increasing the number of weapons used, or escalated through the selection of different targets. Limited attacks are thought to be a more credible response against attacks that do not justify all-out retaliation, such as an enemy's limited use of nuclear weapons.[23]

The second, a full-scale nuclear war, could consist of large numbers of nuclear weapons used in an attack aimed at an entire country, including military, economic, and civilian targets. Such an attack would almost certainly destroy the entire economic, social, and military infrastructure of the target nation, and would likely have a devastating effect on Earth's biosphere.[7][24]

Some Cold War strategists such as Henry Kissinger[25] argued that a limited nuclear war could be possible between two heavily armed superpowers (such as the United States and the Soviet Union). Some predict, however, that a limited war could potentially "escalate" into a full-scale nuclear war. Others[who?] have called limited nuclear war "global nuclear holocaust in slow motion", arguing that—once such a war took place—others would be sure to follow over a period of decades, effectively rendering the planet uninhabitable in the same way that a "full-scale nuclear war" between superpowers would, only taking a much longer (and arguably more agonizing) path to the same result.

Even the most optimistic predictions[by whom?] of the effects of a major nuclear exchange foresee the death of many millions of victims within a very short period of time. Such predictions usually include the breakdown of institutions, government, professional and commercial, vital to the continuation of civilization. The resulting loss of vital affordances (food, water and electricity production and distribution, medical and information services, etc.) would account for millions more deaths. More pessimistic predictions argue that a full-scale nuclear war could potentially bring about the extinction of the human race, or at least its near extinction, with only a relatively small number of survivors (mainly in remote areas) and a reduced quality of life and life expectancy for centuries afterward. However, such predictions, assuming total war with nuclear arsenals at Cold War highs, have not been without criticism.[4] Such a horrific catastrophe as global nuclear warfare would almost certainly cause permanent damage to most complex life on the planet, its ecosystems, and the global climate.[5]

A study presented at the annual meeting of the American Geophysical Union in December 2006 asserted that even a small-scale regional nuclear war could produce as many direct fatalities as all of World War II and disrupt the global climate for a decade or more. In a regional nuclear conflict scenario in which two opposing nations in the subtropics each used 50 Hiroshima-sized nuclear weapons (c. 15 kiloton each) on major population centers, the researchers predicted fatalities ranging from 2.6 million to 16.7 million per country. The authors of the study estimated that as much as five million tons of soot could be released, producing a cooling of several degrees over large areas of North America and Eurasia (including most of the grain-growing regions). The cooling would last for years and could be "catastrophic", according to the researchers.[26]

Either a limited or full-scale nuclear exchange could occur during an accidental nuclear war, in which the use of nuclear weapons is triggered unintentionally. Postulated triggers for this scenario have included malfunctioning early warning devices and/or targeting computers, deliberate malfeasance by rogue military commanders, consequences of an accidental straying of warplanes into enemy airspace, reactions to unannounced missile tests during tense diplomatic periods, reactions to military exercises, mistranslated or miscommunicated messages, and others.

A number of these scenarios actually occurred during the Cold War, though none resulted in the use of nuclear weapons.[27] Many such scenarios have been depicted in popular culture, such as in the 1959 film On the Beach, the 1962 novel Fail-Safe (released as a film in 1964); and the film Dr. Strangelove or: How I Learned to Stop Worrying and Love the Bomb, also released in 1964; the film WarGames, released in 1983.

History
Main articles: History of nuclear weapons and Timeline of nuclear weapons development
1940s
Atomic bombings of Hiroshima and Nagasaki
Main article: Atomic bombings of Hiroshima and Nagasaki

Mushroom cloud from the atomic explosion over Nagasaki rising 18,000 m (59,000 ft) into the air on the morning of August 9, 1945.
During the final stages of World War II in 1945, the United States conducted atomic raids on the Japanese cities of Hiroshima and Nagasaki, the first on August 6, 1945, and the second on August 9, 1945. These two events were the only times nuclear weapons have been used in combat.[28]

For six months before the atomic bombings, the U.S. 20th Air Force under General Curtis LeMay executed low-level incendiary raids against Japanese cities. The most destructive air raid to occur during the process was not the nuclear attacks, but the Operation Meetinghouse raid on Tokyo. On the night of March 9–10, 1945, Operation Meetinghouse commenced and 334 Boeing B-29 Superfortress bombers took off to raid, with 279 of them dropping 1,665 tons of incendiaries and explosives on Tokyo. The bombing was meant to burn wooden buildings and indeed the bombing caused fire that created a 50 m/s wind, which is comparable to tornadoes. Each bomber carried 6 tons of bombs. A total of 381,300 bombs, which amount to 1,783 tons of bombs, were used in the bombing. Within a few hours of the raid, it had killed an estimated 100,000 people and destroyed 41 km2 (16 sq mi) of the city and 267,000 buildings in a single night — the deadliest bombing raid in military aviation history other than the atomic raids on Hiroshima and Nagasaki.[29][30][31][32] By early August 1945, an estimated 450,000 people had died as the U.S. had intensely firebombed a total of 67 Japanese cities.

In late June 1945, as the U.S. wrapped up the two-and-a-half-month Battle of Okinawa (which cost the lives of 260,000 people, including 150,000 civilians),[33][34] it was faced with the prospect of invading the Japanese home islands in an operation codenamed Operation Downfall. Based on the U.S. casualties from the preceding island-hopping campaigns, American commanders estimated that between 50,000 and 500,000 U.S. troops would die and at least 600,000–1,000,000 others would be injured while invading the Japanese home islands. The U.S. manufacture of 500,000 Purple Hearts from the anticipated high level of casualties during the U.S. invasion of Japan gave a demonstration of how deadly and costly it would be. President Harry S. Truman realized he could not afford such a horrendous casualty rate, especially since over 400,000 American combatants had already died fighting in both the European and the Pacific theaters of the war.[35]

On July 26, 1945, the United States, the United Kingdom, and the Republic of China issued a Potsdam Declaration that called for the unconditional surrender of Japan. It stated that if Japan did not surrender, it would face "prompt and utter destruction".[36][37] The Japanese government ignored this ultimatum, sending a message that they were not going to surrender. In response to the rejection, President Truman authorized the dropping of the atomic bombs. At the time of its use, there were only two atomic bombs available, and despite the fact that more were in production back in mainland U.S., the third bomb wouldn't be available for combat until September.[38][39]


A photograph of Sumiteru Taniguchi's back injuries taken in January 1946 by a U.S. Marine photographer
On August 6, 1945, the uranium-type nuclear weapon codenamed "Little Boy" was detonated over the Japanese city of Hiroshima with an energy of about 15 kilotons of TNT (63,000 gigajoules), destroying nearly 50,000 buildings (including the headquarters of the 2nd General Army and Fifth Division) and killing approximately 70,000 people, including 20,000 Japanese combatants and 20,000 Korean slave laborers.[40][41] Three days later, on August 9, a plutonium-type nuclear weapon codenamed "Fat Man" was used against the Japanese city of Nagasaki, with the explosion equivalent to about 20 kilotons of TNT (84,000 gigajoules), destroying 60% of the city and killing approximately 35,000 people, including 23,200–28,200 Japanese munitions workers, 2,000 Korean slave laborers, and 150 Japanese combatants.[42] The industrial damage in Nagasaki was high, partly owing to the inadvertent targeting of the industrial zone, leaving 68–80 percent of the non-dock industrial production destroyed.[43]

Six days after the detonation over Nagasaki, Japan announced its surrender to the Allied Powers on August 15, 1945, signing the Instrument of Surrender on September 2, 1945, officially ending the Pacific War and, therefore, World War II, as Germany had already signed its Instrument of Surrender on May 8, 1945, ending the war in Europe. The two atomic bombings led, in part, to post-war Japan's adopting of the Three Non-Nuclear Principles, which forbade the nation from developing nuclear armaments.[44]

Immediately after the Japan bombings
After the successful Trinity nuclear test July 16, 1945, which was the very first nuclear detonation, the Manhattan project lead manager J. Robert Oppenheimer recalled:

We knew the world would not be the same. A few people laughed, a few people cried, and most people were silent. I remembered the line from the Hindu scripture the Bhagavad Gita. Vishnu is trying to persuade the prince that he should do his duty and to impress him takes on his multiarmed form and says, "Now, I am become Death, the destroyer of worlds." I suppose we all thought that one way or another.

— J. Robert Oppenheimer, The Decision To Drop The Bomb[45]

J. Robert Oppenheimer.
Immediately after the atomic bombings of Japan, the status of atomic weapons in international and military relations was unclear. Presumably, the United States hoped atomic weapons could offset the Soviet Union's larger conventional ground forces in Eastern Europe, and possibly be used to pressure Soviet leader Joseph Stalin into making concessions. Under Stalin, the Soviet Union pursued its own atomic capabilities through a combination of scientific research and espionage directed against the American program. The Soviets believed that the Americans, with their limited nuclear arsenal, were unlikely to engage in any new world wars, while the Americans were not confident they could prevent a Soviet takeover of Europe, despite their atomic advantage.

Within the United States, the authority to produce and develop nuclear weapons was removed from military control and put instead under the civilian control of the United States Atomic Energy Commission. This decision reflected an understanding that nuclear weapons had unique risks and benefits that were separate from other military technology known at the time.


Convair B-36 bomber.
For several years after World War II, the United States developed and maintained a strategic force based on the Convair B-36 bomber that would be able to attack any potential enemy from bomber bases in the United States. It deployed atomic bombs around the world for potential use in conflicts. Over a period of a few years, many in the American defense community became increasingly convinced of the invincibility of the United States to a nuclear attack. Indeed, it became generally believed that the threat of nuclear war would deter any strike against the United States.

Many proposals were suggested to put all American nuclear weapons under international control (by the newly formed United Nations, for example) as an effort to deter both their usage and an arms race. However, no terms could be arrived at that would be agreed upon by both the United States and the Soviet Union.[citation needed]


American and Soviet/Russian nuclear stockpiles.
On August 29, 1949, the Soviet Union tested its first nuclear weapon at Semipalatinsk in Kazakhstan (see also Soviet atomic bomb project). Scientists in the United States from the Manhattan Project had warned that, in time, the Soviet Union would certainly develop nuclear capabilities of its own. Nevertheless, the effect upon military thinking and planning in the United States was dramatic, primarily because American military strategists had not anticipated the Soviets would "catch up" so soon. However, at this time, they had not discovered that the Soviets had conducted significant nuclear espionage of the project from spies at Los Alamos National Laboratory, the most significant of which was done by the theoretical physicist Klaus Fuchs.[citation needed] The first Soviet bomb was more or less a deliberate copy of the Fat Man plutonium device. In the same year the first US-Soviet nuclear war plan was penned in the US with Operation Dropshot.

With the monopoly over nuclear technology broken, worldwide nuclear proliferation accelerated. The United Kingdom tested its first independent atomic bomb in 1952, followed by France developing its first atomic bomb in 1960 and then China developing its first atomic bomb in 1964. While much smaller than the arsenals of the United States and the Soviet Union, Western Europe's nuclear reserves were nevertheless a significant factor in strategic planning during the Cold War. A top-secret White paper, compiled by the Royal Air Force and produced for the British Government in 1959, estimated that British V bombers carrying nuclear weapons were capable of destroying key cities and military targets in the Soviet Union, with an estimated 16 million deaths in the Soviet Union (half of whom were estimated to be killed on impact and the rest fatally injured) before bomber aircraft from the U.S. Strategic Air Command reached their targets.

1950s
Although the Soviet Union had nuclear weapon capabilities at the beginning of the Cold War, the United States still had an advantage in terms of bombers and weapons. In any exchange of hostilities, the United States would have been capable of bombing the Soviet Union, whereas the Soviet Union would have more difficulty carrying out the reverse mission.

The widespread introduction of jet-powered interceptor aircraft upset this imbalance somewhat by reducing the effectiveness of the American bomber fleet. In 1949 Curtis LeMay was placed in command of the Strategic Air Command and instituted a program to update the bomber fleet to one that was all-jet. During the early 1950s the B-47 Stratojet and B-52 Stratofortress were introduced, providing the ability to bomb the Soviet Union more easily. Before the development of a capable strategic missile force in the Soviet Union, much of the war-fighting doctrine held by western nations revolved around using a large number of smaller nuclear weapons in a tactical role. It is debatable whether such use could be considered "limited" however because it was believed that the United States would use its own strategic weapons (mainly bombers at the time) should the Soviet Union deploy any kind of nuclear weapon against civilian targets. Douglas MacArthur, an American general, was fired by President Harry Truman, partially because he persistently requested permission to use his own discretion in deciding whether to utilize atomic weapons on the People's Republic of China in 1951 during the Korean War.[46] Mao Zedong, China's communist leader, gave the impression that he would welcome a nuclear war with the capitalists because it would annihilate what he viewed as their "imperialist" system.[47][48]

Let us imagine how many people would die if war breaks out. There are 2.7 billion people in the world, and a third could be lost. If it is a little higher it could be half ... I say that if the worst came to the worst and one-half dies, there will still be one-half left, but imperialism would be razed to the ground and the whole world would become socialist. After a few years there would be 2.7 billion people again.

— Mao Zedong, 1957[49]

The U.S. and USSR conducted hundreds of nuclear tests, including the Desert Rock exercises at the Nevada Test Site, USA, pictured above during the Korean War to familiarize their soldiers with conducting operations and counter-measures around nuclear detonations, as the Korean War threatened to expand.
The concept of a "Fortress North America" emerged during the Second World War and persisted into the Cold War to refer to the option of defending Canada and the United States against their enemies if the rest of the world were lost to them. This option was rejected with the formation of NATO and the decision to permanently station troops in Europe.

In the summer of 1951, Project Vista started, in which project analysts such as Robert F. Christy looked at how to defend Western Europe from a Soviet invasion. The emerging development of tactical nuclear weapons was looked upon as a means to give Western forces a qualitative advantage over the Soviet numerical supremacy in conventional weapons.[50]

Several scares about the increasing ability of the Soviet Union's strategic bomber forces surfaced during the 1950s. The defensive response by the United States was to deploy a fairly strong "layered defense" consisting of interceptor aircraft and anti-aircraft missiles, like the Nike, and guns, like the M51 Skysweeper, near larger cities. However, this was a small response compared to the construction of a huge fleet of nuclear bombers. The principal nuclear strategy was to massively penetrate the Soviet Union. Because such a large area could not be defended against this overwhelming attack in any credible way, the Soviet Union would lose any exchange.

This logic became ingrained in American nuclear doctrine and persisted for much of the duration of the Cold War. As long as the strategic American nuclear forces could overwhelm their Soviet counterparts, a Soviet pre-emptive strike could be averted. Moreover, the Soviet Union could not afford to build any reasonable counterforce, as the economic output of the United States was far larger than that of the Soviets, and they would be unable to achieve "nuclear parity".

Soviet nuclear doctrine, however, did not match American nuclear doctrine.[51][52] Soviet military planners assumed they could win a nuclear war.[51][53][54] Therefore, they expected a large-scale nuclear exchange, followed by a "conventional war" which itself would involve heavy use of tactical nuclear weapons. American doctrine rather assumed that Soviet doctrine was similar, with the mutual in mutually assured destruction necessarily requiring that the other side see things in much the same way, rather than believing—as the Soviets did—that they could fight a large-scale, "combined nuclear and conventional" war.

In accordance with their doctrine, the Soviet Union conducted large-scale military exercises to explore the possibility of defensive and offensive warfare during a nuclear war. The exercise, under the code name of "Snowball", involved the detonation of a nuclear bomb about twice as powerful as that which fell on Nagasaki and an army of approximately 45,000 soldiers on maneuvers through the hypocenter immediately after the blast.[55] The exercise was conducted on September 14, 1954, under command of Marshal Georgy Zhukov to the north of Totskoye village in Orenburg Oblast, Russia.

A revolution in nuclear strategic thought occurred with the introduction of the intercontinental ballistic missile (ICBM), which the Soviet Union first successfully tested in August 1957. In order to deliver a warhead to a target, a missile was much faster and more cost-effective than a bomber, and enjoyed a higher survivability due to the enormous difficulty of interception of the ICBMs (due to their high altitude and extreme speed). The Soviet Union could now afford to achieve nuclear parity with the United States in raw numbers, although for a time, they appeared to have chosen not to.

Photos of Soviet missile sites set off a wave of panic in the U.S. military, something the launch of Sputnik would do for the American public a few months later. Politicians, notably then-U.S. Senator John F. Kennedy suggested that a "missile gap" existed between the Soviet Union and the United States. The US military gave missile development programs the highest national priority, and several spy aircraft and reconnaissance satellites were designed and deployed to observe Soviet progress.

Early ICBMs and bombers were relatively inaccurate, which led to the concept of countervalue strikes — attacks directly on the enemy population, which would theoretically lead to a collapse of the enemy's will to fight. During the Cold War, the Soviet Union invested in extensive protected civilian infrastructure, such as large "nuclear-proof" bunkers and non-perishable food stores. By comparison, smaller scale civil defense programs were instituted in the United States starting in the 1950s, where schools and other public buildings had basements stocked with non-perishable food supplies, canned water, first aid, and dosimeter and Geiger counter radiation-measuring devices. Many of the locations were given "fallout shelter" designation signs. CONELRAD radio information systems were adopted, whereby the commercial radio sector (later supplemented by the National Emergency Alarm Repeaters) would broadcast on two AM radio frequencies in the event of a Civil Defense (CD) emergency. These two frequencies, 640 and 1240 kHz, were marked with small CD triangles on the tuning dial of radios of the period, as can still be seen on 1950s-vintage radios on online auction sites and museums. A few backyard fallout shelters were built by private individuals.

Henry Kissinger's view on tactical nuclear war in his controversial 1957 book Nuclear Weapons and Foreign Policy was that any nuclear weapon exploded in air burst mode that was below 500 kilotons in yield and thus averting serious fallout, may be more decisive and less costly in human lives than a protracted conventional war.

A list of targets made by the United States was released sometime during December 2015 by the U.S. National Archives and Records Administration. The language used to describe targets is "designated ground zeros". The list was released after a request was made during 2006 by William Burr who belongs to a research group at George Washington University, and belongs to a previously top-secret 800-page document. The list is entitled "Atomic Weapons Requirements Study for 1959" and was produced by U.S. Strategic Air Command during the year 1956.[56]

1960s

More than 100 US-built missiles having the capability to strike Moscow with nuclear warheads were deployed in Italy and Turkey in 1961

RF-101 Voodoo reconnaissance photograph of the MRBM launch site in San Cristóbal, Cuba (1962)
In 1960, the United States developed its first Single Integrated Operational Plan, a range of targeting options, and described launch procedures and target sets against which nuclear weapons would be launched, variants of which were in use from 1961 to 2003. That year also saw the start of the Missile Defense Alarm System, an American system of 12 early-warning satellites that provided limited notice of Soviet intercontinental ballistic missile launches between 1960 and 1966. The Ballistic Missile Early Warning System was completed in 1964.

The most powerful atomic bomb ever made, the Tsar Bomba, was tested by the Soviets on October 30, 1961. It was 50 megatons, or equal to 50 million tons of regular explosives.[57] A complex and worrisome situation developed in 1962, in what is called the Cuban Missile Crisis. The Soviet Union placed medium-range ballistic missiles 90 miles (140 km) from the United States, possibly as a direct response to American Jupiter missiles placed in Turkey. After intense negotiations, the Soviets ended up removing the missiles from Cuba and decided to institute a massive weapons-building program of their own. In exchange, the United States dismantled its launch sites in Turkey, although this was done secretly and not publicly revealed for over two decades. First Secretary Nikita Khrushchev did not even reveal this part of the agreement when he came under fire by political opponents for mishandling the crisis. Communication delays during the crisis led to the establishment of the Moscow–Washington hotline to allow reliable, direct communications between the two nuclear powers.

By the late 1960s, the number of ICBMs and warheads was so high on both sides that it was believed that both the United States and the Soviet Union were capable of completely destroying the infrastructure and a large proportion of the population of the other country. Thus, by some western game theorists, a balance of power system known as mutually assured destruction (or MAD) came into being. It was thought that no full-scale exchange between the powers would result in an outright winner, with at best one side emerging the pyrrhic victor. Thus both sides were deterred from risking the initiation of a direct confrontation, instead being forced to engage in lower-intensity proxy wars.

During this decade the People's Republic of China began to build subterranean infrastructure such as the Underground Project 131 following the Sino-Soviet split.

One drawback of the MAD doctrine was the possibility of a nuclear war occurring without either side intentionally striking first. Early Warning Systems (EWS) were notoriously error-prone. For example, on 78 occasions in 1979 alone, a "missile display conference" was called to evaluate detections that were "potentially threatening to the North American continent". Some of these were trivial errors and were spotted quickly, but several went to more serious levels. On September 26, 1983, Stanislav Petrov received convincing indications of an American first strike launch against the Soviet Union, but positively identified the warning as a false alarm. Though it is unclear what role Petrov's actions played in preventing a nuclear war during this incident, he has been honored by the United Nations for his actions.

Similar incidents happened many times in the United States, due to failed computer chips,[58] misidentifications of large flights of geese, test programs, and bureaucratic failures to notify early warning military personnel of legitimate launches of test or weather missiles. For many years, the U.S. Air Force's strategic bombers were kept airborne on a daily rotating basis "around the clock" (see Operation Chrome Dome), until the number and severity of accidents, the 1968 Thule Air Base B-52 crash in particular,[59] persuaded policymakers it was not worthwhile.

1970s
Israel responded to the Arab Yom Kippur War attack on 6 October 1973 by assembling 13 nuclear weapons in a tunnel under the Negev desert when Syrian tanks were sweeping in across the Golan Heights. On 8 October 1973, Israeli Prime Minister Golda Meir authorized Defense Minister Moshe Dayan to activate the 13 Israeli nuclear warheads and distribute them to Israeli air force units, with the intent that they be used if Israel began to be overrun.[60]

On 24 October 1973, as US President Richard Nixon was preoccupied with the Watergate scandal, Henry Kissinger ordered a DEFCON-3 alert[dubious – discuss] preparing American B-52 nuclear bombers for war. Intelligence reports indicated that the USSR was preparing to defend Egypt in its Yom Kippur War with Israel. It had become apparent that if Israel had dropped nuclear weapons on Egypt or Syria, as it prepared to do, then the USSR would have retaliated against Israel, with the US then committed to providing Israeli assistance, possibly escalating to a general nuclear war.[61]

By the late 1970s, people in both the United States and the Soviet Union, along with the rest of the world, had been living with the concept of mutual assured destruction (MAD) for about a decade, and it became deeply ingrained into the psyche and popular culture of those countries.[citation needed]

On May 18, 1974, India conducted its first nuclear test in the Pokhran test range. The name of the operation was Smiling Buddha, and India termed the test as a "peaceful nuclear explosion."

The Soviet Duga early warning over-the-horizon radar system was made operational in 1976. The extremely powerful radio transmissions needed for such a system led to much disruption of civilian shortwave broadcasts, earning it the nickname "Russian Woodpecker".

The idea that any nuclear conflict would eventually escalate was a challenge for military strategists. This challenge was particularly severe for the United States and its NATO allies. It was believed (until the 1970s) that a Soviet tank offensive into Western Europe would quickly overwhelm NATO conventional forces, leading to the necessity of the West escalating to the use of tactical nuclear weapons, one of which was the W-70.

This strategy had one major (and possibly critical) flaw, which was soon realized by military analysts but highly underplayed by the U.S. military: conventional NATO forces in the European theatre of war were far outnumbered by similar Soviet and Warsaw Pact forces, and it was assumed that in case of a major Soviet attack (commonly envisioned as the "Red tanks rolling towards the North Sea" scenario) that NATO—in the face of quick conventional defeat—would soon have no other choice but to resort to tactical nuclear strikes against these forces. Most analysts agreed that once the first nuclear exchange had occurred, escalation to global nuclear war would likely become inevitable. The Warsaw Pact's vision of an atomic war between NATO and Warsaw Pact forces was simulated in the top-secret exercise Seven Days to the River Rhine in 1979. The British government exercised their vision of a Soviet nuclear attack with Square Leg in early 1980.

Large hardened nuclear weapon storage areas were built across European countries in anticipation of local US and European forces falling back as the conventional NATO defense from the Soviet Union, named REFORGER, was believed to only be capable of stalling the Soviets for a short time.

1980s

Montage of the launch of a Trident C4 SLBM and the paths of its reentry vehicles.

FEMA-estimated primary counterforce targets for Soviet ICBMs in 1990. The resulting fall-out is indicated with the darkest considered as lethal to lesser fall-out yellow zones.[62][failed verification]
In the late 1970s and, particularly, during the early 1980s under U.S. President Ronald Reagan, the United States renewed its commitment to a more powerful military, which required a large increase in spending on U.S. military programs. These programs, which were originally part of the defense budget of U.S. President Jimmy Carter, included spending on conventional and nuclear weapons systems. Under Reagan, defensive systems like the Strategic Defense Initiative were emphasized as well.

Another major shift in nuclear doctrine was the development and the improvement of the submarine-launched, nuclear-armed, ballistic missile, or SLBM. It was hailed by many military theorists as a weapon that would make nuclear war less likely. SLBMs—which can move with "stealth" (greatly lessened detectability) virtually anywhere in the world—give a nation a "second strike" capability (i.e., after absorbing a "first strike"). Before the advent of the SLBM, thinkers feared that a nation might be tempted to initiate a first strike if it felt confident that such a strike would incapacitate the nuclear arsenal of its enemy, making retaliation impossible. With the advent of SLBMs, no nation could be certain that a first strike would incapacitate its enemy's entire nuclear arsenal. To the contrary, it would have to fear a near-certain retaliatory second strike from SLBMs. Thus, a first strike was a much less feasible (or desirable) option, and a deliberately initiated nuclear war was thought to be less likely to start.

However, it was soon realized that submarines could approach enemy coastlines undetected and decrease the warning time (the time between detection of the missile launch and the impact of the missile) from as much as half an hour to possibly under three minutes. This effect was especially significant to the United States, Britain and China, whose capitals of Washington D.C., London, and Beijing all lay within 100 miles (160 km) of their coasts. Moscow was much more secure from this type of threat, due to its considerable distance from the sea. This greatly increased the credibility of a "surprise first strike" by one faction and (theoretically) made it possible to knock out or disrupt the chain of command of a target nation before any counterstrike could be ordered (known as a "decapitation strike"). It strengthened the notion that a nuclear war could possibly be "won", resulting not only in greatly increased tensions and increasing calls for fail-deadly control systems, but also in a dramatic increase in military spending. The submarines and their missile systems were very expensive, and one fully equipped nuclear-powered and nuclear-armed missile submarine could cost more than the entire GNP of a developing country.[63] It was also calculated, however, that the greatest cost came in the development of both sea- and land-based anti-submarine defenses and in improving and strengthening the "chain of command", and as a result, military spending skyrocketed.

South Africa developed a nuclear weapon capability during the 1970s and early 1980s. It was operational for a brief period before being dismantled in the early 1990s.[64]

According to the 1980 United Nations report General and Complete Disarmament: Comprehensive Study on Nuclear Weapons: Report of the Secretary-General, it was estimated that there were a total of about 40,000 nuclear warheads in existence at that time, with a potential combined explosive yield of approximately 13,000 megatons. By comparison, the largest volcanic eruption in recorded history when the volcano Mount Tambora erupted in 1815—turning 1816 into the Year Without A Summer due to the levels of global dimming sulfate aerosols and ash expelled—it exploded with a force of roughly 33 billion tons of TNT or 33,000 megatons of TNT this is about 2.2 million Hiroshima Bombs,[65] and ejected 175 km3 (42 cu mi) of mostly rock/tephra,[66] that included 120 million tonnes of sulfur dioxide as an upper estimate.[67] A larger eruption, approximately 74,000 years ago, in Mount Toba produced 2,800 km3 (670 cu mi) of tephra, forming lake Toba,[68] and produced an estimated 6,000 million tonnes (6.6×109 short tons) of sulfur dioxide.[69][70] The explosive energy of the eruption may have been as high as equivalent to 20,000,000 megatons (Mt) of TNT,[71][better source needed] while the asteroid created Chicxulub impact, that is connected with the extinction of the dinosaurs corresponds to at least 70,000,000 Mt of energy, which is roughly 7000 times the maximum arsenal of the US and Soviet Union.[71]


Protest against the deployment of Pershing II missiles in Europe, Bonn, West Germany, 1981
However, comparisons with supervolcanoes are more misleading than helpful due to the different aerosols released, the likely air burst fuzing height of nuclear weapons and the globally scattered location of these potential nuclear detonations all being in contrast to the singular and subterranean nature of a supervolcanic eruption.[3] Moreover, assuming the entire world stockpile of weapons were grouped together, it would be difficult, due to the nuclear fratricide effect, to ensure the individual weapons would go off all at once. Nonetheless, many people believe that a full-scale nuclear war would result, through the nuclear winter effect, in the extinction of the human species, though not all analysts agree on the assumptions that underpin these nuclear winter models.[4]

On 26 September 1983, a Soviet early warning station under the command of Stanislav Petrov falsely detected 5 inbound intercontinental ballistic missiles from the US. Petrov correctly assessed the situation as a false alarm, and hence did not report his finding to his superiors. It is quite possible that his actions prevented "World War III", as the Soviet policy at that time was immediate nuclear response upon discovering inbound ballistic missiles.[72]

The world came unusually close to nuclear war in November 1983 when the Soviet Union thought that the NATO military exercise Able Archer 83 was a ruse or "cover-up" to begin a nuclear first strike. The Soviets responded by raising readiness and preparing their nuclear arsenal for immediate use. Soviet fears of an attack ceased once the exercise concluded without incident.

Post-Cold War
See also: Second Cold War
Although the dissolution of the Soviet Union ended the Cold War and greatly reduced tensions between the United States and the Russian Federation, the Soviet Union's formal successor state, both countries remained in a "nuclear stand-off" due to the continuing presence of a very large number of deliverable nuclear warheads on both sides. Additionally, the end of the Cold War led the United States to become increasingly concerned with the development of nuclear technology by other nations outside of the former Soviet Union. In 1995, a branch of the U.S. Strategic Command produced an outline of forward-thinking strategies in the document "Essentials of Post–Cold War Deterrence".

In 1995, a Black Brant sounding rocket launched from the Andøya Space Center caused a high alert in Russia, known as the Norwegian Rocket Incident. The Russians thought it might be a nuclear missile launched from an American submarine.[73][74]

In 1996, a Russian continuity of government facility, Kosvinsky Mountain, which is believed to be a counterpart to the US Cheyenne Mountain Complex, was completed.[75] It was designed to resist US earth-penetrating nuclear warheads,[75] and is believed to host the Russian Strategic Rocket Forces alternate command post, a post for the general staff built to compensate for the vulnerability of older Soviet era command posts in the Moscow region. In spite of this, the primary command posts for the Strategic Rocket Forces remains Kuntsevo in Moscow and the secondary is the Kosvinsky Mountain in the Ural Mountains.[citation needed] The timing of the Kosvinsky facilities completion date is regarded as one explanation for U.S. interest in a new nuclear "bunker buster" Earth-penetrating warhead and the declaration of the deployment of the B-61 mod 11 in 1997; Kosvinsky is protected by about 1000 feet of granite.[citation needed]


UN vote on adoption of the Treaty on the Prohibition of Nuclear Weapons on 7 July 2017
  Yes
  No
  Did not vote
As a consequence of the September 11 attacks, American forces immediately increased their readiness to the highest level in 28 years, closing the blast doors of the NORAD's Cheyenne Mountain Operations Center for the first time due to a non-exercise event. But unlike similar increases during the Cold War, Russia immediately decided to stand down a large military exercise in the Arctic region, in order to minimize the risk of incidents, rather than following suit.[76]

The former chair of the United Nations disarmament committee stated that there are more than 16,000 strategic and tactical nuclear weapons ready for deployment and another 14,000 in storage, with the U.S. having nearly 7,000 ready for use and 3,000 in storage, and Russia having about 8,500 ready for use and 11,000 in storage. In addition, China is thought to possess about 400 nuclear weapons, Britain about 200, France about 350, India about 80–100, and Pakistan 100–110. North Korea is confirmed as having nuclear weapons, though it is not known how many, with most estimates between 1 and 10. Israel is also widely believed to possess usable nuclear weapons. NATO has stationed about 480 American nuclear weapons in Belgium, the Netherlands, Italy, Germany, and Turkey, and several other nations are thought to be in pursuit of an arsenal of their own.[77]

Pakistan's nuclear policy was significantly affected by the 1965 war with India.[78] The 1971 war and India's nuclear program played a role in Pakistan's decision to go nuclear.[79] India and Pakistan both decided not to participate in the NPT.[80] Pakistan's nuclear policy became fixated on India because India refused to join the NPT and remained open to nuclear weapons.[81] Impetus by Indian actions spurred Pakistan's nuclear research.[82] After nuclear weapons construction was started by President Zulfikar Ali Bhutto's command, the chair of Pakistan Atomic Energy Commission Usmani quit in objection.[83] The 1999 war between Pakistan and India occurred after both acquired nuclear weapons.[84] It is believed by some that nuclear weapons are the reason a big war has not broken out in the subcontinent.[85] India and Pakistan still have a risk of nuclear conflict on the issue of war over Kashmir. Nuclear capability deliverable by sea were claimed by Pakistan in 2012.[86] The aim was to achieve a "minimum credible deterrence".[87] Pakistan's nuclear program culminated in the tests at Chagai.[88] One of the aims of Pakistan's programs is fending off potential annexation and maintaining independence.[89]

A key development in nuclear warfare throughout the 2000s and early 2010s is the proliferation of nuclear weapons to the developing world, with India and Pakistan both publicly testing several nuclear devices, and North Korea conducting an underground nuclear test on October 9, 2006. The U.S. Geological Survey measured a 4.2 magnitude earthquake in the area where the North Korean test is said to have occurred. A further test was announced by the North Korean government on May 25, 2009.[90] Iran, meanwhile, has embarked on a nuclear program which, while officially for civilian purposes, has come under close scrutiny by the United Nations and many individual states.

Recent studies undertaken by the CIA cite the enduring India-Pakistan conflict as the one "flash point" most likely to escalate into a nuclear war. During the Kargil War in 1999, Pakistan came close to using its nuclear weapons in case the conventional military situation underwent further deterioration.[91] Pakistan's foreign minister had even warned that it would "use any weapon in our arsenal", hinting at a nuclear strike against India.[92] The statement was condemned by the international community, with Pakistan denying it later on. This conflict remains the only war (of any sort) between two declared nuclear powers. The 2001-2002 India-Pakistan standoff again stoked fears of nuclear war between the two countries. Despite these very serious and relatively recent threats, relations between India and Pakistan have been improving somewhat over the last few years. However, with the November 26, 2008 Mumbai terror attacks, tensions again worsened.

External image
image icon A geopolitical example of nuclear strike plan of ROC Army in Kinmen history. Effective Radius: 10 km; Pop.: 1.06 million

Large stockpile with global range (dark blue), smaller stockpile with global range (medium blue), small stockpile with regional range (light blue).
Another potential geopolitical issue that is considered particularly worrisome by military analysts is a possible conflict between the United States and the People's Republic of China over Taiwan. Although economic forces are thought to have reduced the possibility of a military conflict, there remains concern about the increasing military buildup of China (China is rapidly increasing its naval capacity), and that any move toward Taiwan independence could potentially spin out of control.

Israel is thought to possess somewhere between one hundred and four hundred nuclear warheads. It has been asserted that the Dolphin-class submarines which Israel received from Germany have been adapted to carry nuclear-armed Popeye cruise missiles, so as to give Israel a second strike capability.[93] Israel has been involved in wars with its neighbors in the Middle East (and with other "non-state actors" in Lebanon and Palestine) on numerous prior occasions, and its small geographic size and population could mean that, in the event of future wars, the Israel Defense Forces might have very little time to react to an invasion or other major threat. Such a situation could escalate to nuclear warfare very quickly in some scenarios.

On March 7, 2013, North Korea threatened the United States with a pre-emptive nuclear strike.[94] On April 9, North Korea urged foreigners to leave South Korea, stating that both countries were on the verge of nuclear war.[95] On April 12, North Korea stated that a nuclear war was unavoidable. The country declared Japan as its first target.[96]

In 2014, when Russia-United States and Russia-NATO relations worsened over the Russo-Ukrainian War, the Russian state-owned television channel Russia 1 stated that "Russia is the only country in the world that is really capable of turning the USA into radioactive ash."[97] U.S. Secretary of Defense Ash Carter considered proposing deployment of ground-launched cruise missiles in Europe that could pre-emptively destroy Russian weapons.[98]

In August 2017, North Korea warned that it might launch mid-range ballistic missiles into waters within 18 to 24 miles (29 to 39 km) of Guam, following an exchange of threats between the governments of North Korea and the United States.[99][100] Escalating tensions between North Korea and the United States, including threats by both countries that they could use nuclear weapons against one another, prompted a heightened state of readiness in Hawaii. The perceived ballistic missile threat broadcast all over Hawaii on 13 January 2018 was a false missile alarm.[101][102]

In October 2018, the former Soviet leader Mikhail Gorbachev commented that U.S. withdrawal from the Intermediate-Range Nuclear Forces Treaty is "not the work of a great mind" and that "a new arms race has been announced".[103][104]

In early 2019, more than 90% of world's 13,865 nuclear weapons were owned by Russia and the United States.[105][106]

In 2019, Vladimir Putin warned that Russia would deploy nuclear missiles in Europe if the United States deployed intermediate-range nuclear missiles there. Journalist Dmitry Kiselyov listed the targets in the United States, which includes The Pentagon, Camp David, Fort Ritchie, McClellan Air Force Base, and Jim Creek Naval Radio Station. Kremlin spokesperson Dmitry Peskov denies the existence of the target list.[107][108]

On February 24, 2022, in a televised address preceding the start of Russia's full-scale invasion of Ukraine, Russian President Vladimir Putin stated that Russia "is today one of the most powerful nuclear powers in the world... No one should have any doubts that a direct attack on our country will lead to defeat and dire consequences for any potential aggressor." Later in the same speech, Putin stated: "Now a few important, very important words for those who may be tempted to intervene in ongoing events. Whoever tries to hinder us, and even more so to create threats for our country, for our people, should know that Russia's response will be immediate and will lead you to such consequences that you have never experienced in your history."[109][110] On February 27, 2022, Putin publicly put his nuclear forces on alert, stating that NATO powers had made "aggressive statements".[111] On April 14, The New York Times reported comments by CIA director William Burns, who said "potential desperation" could lead President Putin to order the use of tactical nuclear weapons.[112] On September 21, 2022, days before declaring the annexation of additional parts of Ukraine, Putin claimed in a national television address that high NATO officials had made statements about the possibility of "using nuclear weapons of mass destruction against Russia", and stated "if the territorial integrity of our country is threatened, we will certainly use all the means at our disposal to protect Russia and our people... It's not a bluff." NBC News characterized Putin's statements as a "thinly veiled" threat that Putin was willing to risk nuclear conflict if necessary to win the war with Ukraine.[113] Hans M. Kristensen, director of the Nuclear Information Project at the Federation of American Scientists, stated that "if you start detonating nuclear weapons in the [battlefield] you potentially get radioactive fallout that you can't control — it could rain over your own troops as well, so it might not be an advantage to do that in the field."[114]

According to a peer-reviewed study published in the journal Nature Food in August 2022,[115] a full-scale nuclear war between the U.S. and Russia would kill 360 million people directly, with a further 5 billion people dying from starvation. More than 2 billion people would die from a smaller-scale nuclear war between India and Pakistan.[116][117]

Sub-strategic use
See also: Nuclear bunker buster and Edward Teller § Decision to drop the bombs
The above examples envisage nuclear warfare at a strategic level, i.e., total war. However, nuclear powers have the ability to undertake more limited engagements.

"Sub-strategic use" includes the use of either "low-yield" tactical nuclear weapons, or of variable yield strategic nuclear weapons in a very limited role, as compared to battlefield exchanges of larger-yield strategic nuclear weapons. This was described by the UK Parliamentary Defence Select Committee as "the launch of one or a limited number of missiles against an adversary as a means of conveying a political message, warning or demonstration of resolve".[118] It is believed that all current nuclear weapons states possess tactical nuclear weapons, with the exception of the United Kingdom, which decommissioned its tactical warheads in 1998. However, the UK does possess scalable-yield strategic warheads, and this technology tends to blur the difference between "strategic", "sub-strategic", and "tactical" use or weapons. American, French and British nuclear submarines are believed to carry at least some missiles with dial-a-yield warheads for this purpose, potentially allowing a strike as low as one kiloton (or less) against a single target. Only the People's Republic of China and the Republic of India have declarative, unqualified, unconditional "no first use" nuclear weapons policies. India and Pakistan maintain only a credible minimum deterrence.

Commodore Tim Hare, former Director of Nuclear Policy at the British Ministry of Defence, has described "sub-strategic use" as offering the Government "an extra option in the escalatory process before it goes for an all-out strategic strike which would deliver unacceptable damage".[119] However, this sub-strategic capacity has been criticized as potentially increasing the "acceptability" of using nuclear weapons. Combined with the trend in the reduction in the worldwide nuclear arsenal as of 2007 is the warhead miniaturization and modernization of the remaining strategic weapons that is presently occurring in all the declared nuclear weapon states, into more "usable" configurations. The Stockholm International Peace Research Institute suggests that this is creating a culture where use of these weapons is more acceptable and therefore is increasing the risk of war, as these modern weapons do not possess the same psychological deterrent value as the large Cold-War era, multi-megaton warheads.[120]

In many ways, this present change in the balance of terror can be seen as the complete embracement of the switch from the 1950s Eisenhower doctrine of "massive retaliation"[121] to one of "flexible response", which has been growing in importance in the US nuclear war fighting plan/SIOP every decade since.

For example, the United States adopted a policy in 1996 of allowing the targeting of its nuclear weapons at non-state actors ("terrorists") armed with weapons of mass destruction.[122]

Another dimension to the tactical use of nuclear weapons is that of such weapons deployed at sea for use against surface and submarine vessels. Until 1992, vessels of the United States Navy (and their aircraft) deployed various such weapons as bombs, rockets (guided and unguided), torpedoes, and depth charges. Such tactical naval nuclear weapons were considered more acceptable to use early in a conflict because there would be few civilian casualties. It was feared by many planners that such use would probably quickly have escalated into a large-scale nuclear war.[123] This situation was particularly exacerbated by the fact that such weapons at sea were not constrained by the safeguards provided by the Permissive Action Link attached to U.S. Air Force and Army nuclear weapons. It is unknown if the navies of the other nuclear powers yet today deploy tactical nuclear weapons at sea.

The 2018 US Nuclear Posture Review emphasised the need for the US to have sub-strategic nuclear weapons as additional layers for its nuclear deterrence.[124]

Nuclear terrorism
Main article: Nuclear terrorism
Nuclear terrorism by non-state organizations or actors (even individuals) is a largely unknown and understudied factor in nuclear deterrence thinking, as states possessing nuclear weapons are susceptible to retaliation in kind, while sub- or trans-state actors may be less so. The collapse of the Soviet Union has given rise to the possibility that former Soviet nuclear weapons might become available on the black market (so-called 'loose nukes').

A number of other concerns have been expressed about the security of nuclear weapons in newer nuclear powers with relatively less stable governments, such as Pakistan, but in each case, the fears have been addressed to some extent by statements and evidence provided by those nations, as well as cooperative programs between nations. Worry remains, however, in many circles that a relative decrease in the security of nuclear weapons has emerged in recent years, and that terrorists or others may attempt to exert control over (or use) nuclear weapons, militarily applicable technology, or nuclear materials and fuel.

Another possible nuclear terrorism threat are devices designed to disperse radioactive materials over a large area using conventional explosives, called dirty bombs. The detonation of a "dirty bomb" would not cause a nuclear explosion, nor would it release enough radiation to kill or injure a large number of people. However, it could cause severe disruption and require potentially very costly decontamination procedures and increased spending on security measures.[125]

Radioactive materials can also be used for targeted assassinations. For example, the poisoning of Alexander Litvinenko was described by medical professionals, as "an ominous landmark: the beginning of an era of nuclear terrorism."[126][127][128][129]

Survival
See also: Nuclear famine, Nuclear War Survival Skills, and Civil defense
The predictions of the effects of a major countervalue nuclear exchange include millions of city dweller deaths within a short period of time. Some 1980s predictions had gone further and argued that a full-scale nuclear war could eventually bring about the extinction of the human race.[7] Such predictions, sometimes but not always based on total war with nuclear arsenals at Cold War highs, received contemporary criticism.[4] On the other hand, some 1980s governmental predictions, such as FEMA's CRP-2B and NATO's Carte Blanche, have received criticism from groups such as the Federation of American Scientists for being overly optimistic. CRP-2B, for instance, infamously predicted that 80% of Americans would survive a nuclear exchange with the Soviet Union, a figure that neglected nuclear war's impacts on healthcare infrastructure, the food supply, and the ecosystem and assumed that all major cities could be successfully evacuated within 3–5 days.[130] A number of Cold War publications advocated preparations that could purportedly enable a large proportion of civilians to survive even a total nuclear war. Among the most famous of these is Nuclear War Survival Skills.[131]

To avoid injury and death from a nuclear weapon's heat flash and blast effects, the two most far-ranging prompt effects of nuclear weapons, schoolchildren were taught to duck and cover by the early Cold War film of the same name. Such advice is once again being given in case of nuclear terrorist attacks.[132]

Prussian blue, or "Radiogardase", is stockpiled in the US, along with potassium iodide and DPTA, as pharmaceuticals useful in treating internal exposure to harmful radioisotopes in fallout.[133]

Publications on adapting to a changing diet and supplying nutritional food sources following a nuclear war, with particular focus on agricultural radioecology, include Nutrition in the postattack environment by the RAND corporation.[134]

The British government developed a public alert system for use during a nuclear attack with the expectation of a four-minute warning before detonation. The United States expected a warning time of anywhere from half an hour (for land-based missiles) to less than three minutes (for submarine-based weapons). Many countries maintain plans for continuity of government following a nuclear attack or similar disasters. These range from a designated survivor, intended to ensure the survival of some form of government leadership, to the Soviet Dead Hand system, which allows for retaliation even if all Soviet leadership were destroyed. Nuclear submarines are given letters of last resort: orders on what action to take in the event that an enemy nuclear strike has destroyed the government.

A number of other countries around the world have taken significant efforts to maximize their survival prospects in the event of large calamities, both natural and manmade. For example, metro stations in Pyongyang, North Korea, were constructed 110 metres (360 ft) below ground, and were designed to serve as nuclear shelters in the event of war, with each station entrance built with thick steel blast doors.[135][136] An example of privately funded fallout shelters is the Ark Two Shelter in Ontario, Canada, and autonomous shelters have been constructed with an emphasis on post-war networking and reconstruction.[137] In Switzerland, the majority of homes have an underground blast and fallout shelter. The country has an overcapacity of such shelters and can accommodate slightly more than the nation's population size.[138][139]

While the nuclear fallout shelters described above are the ideal long-term protection methods against dangerous radiation exposure in the event of a nuclear catastrophe, it is also necessary to have mobile protection equipment for medical and security personnel to safely assist in containment, evacuation, and many other necessary public safety objectives which ensue as a result of nuclear detonation. There are many basic shielding strategies used to protect against the deposition of radioactive material from external radiation environments. Respirators that protect against internal deposition are used to prevent the inhalation and ingestion of radioactive material and dermal protective equipment which is used to protect against the deposition of material on external structures like skin, hair, and clothing. While these protection strategies do slightly reduce the exposure, they provide almost no protection from externally penetrating gamma radiation, which is the cause of acute radiation syndrome and can be extremely lethal in high dosages. Naturally, shielding the entire body from high-energy gamma radiation is optimal, but the required mass to provide adequate attenuation makes functional movement nearly impossible.

Recent scientific studies have shown the feasibility of partial body shielding as a viable protection strategy against externally penetrating gamma radiation. The concept is based in providing sufficient attenuation to only the most radio-sensitive organs and tissues in efforts to defer the onset of acute radiation syndrome, the most immediate threat to humans from high doses of gamma radiation. Acute radiation syndrome is a result of irreversible bone marrow damage from high-energy radiation exposure. Due to the regenerative property of hematopoietic stem cells found in bone marrow, it is only necessary to protect enough bone marrow to repopulate the exposed areas of the body with the shielded supply. Because 50% of the body's supply of bone marrow is stored in the pelvic region which is also in close proximity to other radio-sensitive organs in the abdomen, the lower torso is a logical choice as the primary target for protection.[140]